## 11:码头工人网络

永远是网络！

每当出现基础设施问题，我们总是指责网络。部分原因是网络是一切的中心——**没有网络，就没有 app！**

在 Docker 的早期，网络很难——真的很难。这几天，几乎是*一种快感；-)*

 *在本章中，我们将了解 Docker 网络的基础知识。比如集装箱网络模型(CNM)和`libnetwork`。我们也会弄脏自己的手建立一些网络。

像往常一样，我们将把这一章分成三个部分:

*   TLDR
*   深潜
*   命令

### 码头工人网络-TLDR

Docker 在容器内部运行应用程序，应用程序需要通过许多不同的网络进行通信。这意味着 Docker 需要强大的网络能力。

幸运的是，Docker 有容器到容器网络的解决方案，以及连接到现有网络和虚拟局域网的解决方案。后者对于与外部系统(如虚拟机和物理服务器)上的功能和服务交互的容器化应用程序非常重要。

Docker 网络基于一个开源的可插拔架构，称为容器网络模型(CNM)。`libnetwork`是 Docker 在现实世界中实现的 CNM，它提供了 Docker 的所有核心网络功能。驱动程序插入`libnetwork`以提供特定的网络拓扑。

为了创建流畅的开箱即用体验，Docker 附带了一组本机驱动程序，可以处理最常见的网络需求。其中包括单主机桥接网络、多主机覆盖以及插入现有虚拟局域网的选项。生态系统合作伙伴可以通过提供自己的驱动程序来进一步扩展事物。

最后但同样重要的是，`libnetwork`提供了本地服务发现和基本容器负载平衡解决方案。

这就是大局。让我们进入细节。

### 码头工人网络-深潜

我们将按如下方式组织本章的这一部分:

*   理论
*   单主机桥接网络
*   多主机覆盖网络
*   连接到现有网络
*   服务发现
*   入口网络

#### 理论

在最高级别，Docker 网络由三个主要组件组成:

*   集装箱网络模型(CNM)
*   `libnetwork`
*   司机

CNM 是设计规范。它概述了 Docker 网络的基本构建块。

`libnetwork`是 CNM 的真实世界实现，由 Docker 使用。它是用 Go 编写的，实现了 CNM 概述的核心组件。

驱动程序通过实现特定的网络拓扑来扩展模型，例如 VXLAN 覆盖网络。

图 11.1 显示了它们如何在很高的层次上结合在一起。

<figure class="image" style="width: 396px">![Figure 11.1](Images/figure11-1.png)

<figcaption>Figure 11.1</figcaption>

</figure>

让我们仔细看看每一个。

##### 集装箱网络模型(CNM)

一切都从设计开始。

Docker 网络的设计指南是 CNM。它概述了 Docker 网络的基本构建块，您可以在这里阅读完整的规范:https://github . com/Docker/libnetwork/blob/master/docs/design . MD

我建议阅读整个规范，但在较高的层次上，它定义了三个主要的构建模块:

*   沙箱
*   端点
*   网络

一**沙盒是一个孤立的网络栈。它包括:以太网接口、端口、路由表和域名系统配置。**

 *****端点*** 为虚拟网络接口(如`veth`)。像普通的网络接口一样，它们负责建立连接。在 CNM 的情况下，将*沙箱*连接到*网络*是*端点*的工作。

***网络*****是一个交换机(802.1d 桥)的软件实现。因此，它们将需要通信的端点集合在一起并隔离。**

 **图 11.2 显示了三个组件及其连接方式。

<figure class="image" style="width: 396px">![Figure 11.2 The Container Network Model (CNM)](Images/figure11-2.png)

<figcaption>Figure 11.2 The Container Network Model (CNM)</figcaption>

</figure>

Docker 环境中调度的原子单元是容器，顾名思义，容器网络模型就是为容器提供网络。图 11.3 显示了 CNM 组件如何与容器相关联——沙箱被放置在容器内部以提供网络连接。

<figure class="image" style="width: 396px">![Figure 11.3](Images/figure11-3.png)

<figcaption>Figure 11.3</figcaption>

</figure>

容器 A 只有一个接口(端点)并连接到网络 A。容器 B 有两个接口(端点)并连接到网络 A **和**网络 B。这两个容器将能够通信，因为它们都连接到网络 A。但是，如果没有第 3 层路由器的帮助，容器 B 中的两个*端点*无法相互通信。

了解*端点*的行为类似于常规网络适配器也很重要，这意味着它们只能连接到单个网络。因此，如果一个容器需要连接到多个网络，它将需要多个端点。

图 11.4 再次扩展了该图，这次添加了一个 Docker 主机。虽然容器 A 和容器 B 运行在同一台主机上，但是它们的网络堆栈在操作系统级别通过沙箱完全隔离。

<figure class="image" style="width: 396px">![Figure 11.4](Images/figure11-4.png)

<figcaption>Figure 11.4</figcaption>

</figure>

##### 图书馆网络

CNM 是设计文档，`libnetwork`是规范实现。它是开源的，用 Go 编写，跨平台(Linux 和 Windows)，由 Docker 使用。

在 Docker 的早期，所有的网络代码都存在于守护进程中。这是一场噩梦——守护进程变得臃肿，它没有遵循构建模块化工具的 Unix 原则，这些工具可以自己工作，但也可以轻松地组合到其他项目中。结果，所有这些都被撕掉，重构到一个基于 CNM 原理的外部库`libnetwork`中。如今，所有的核心 Docker 网络代码都存在于`libnetwork`中。

如您所料，它实现了 CNM 定义的所有三个组件。它还实现了本地*服务发现*、*基于入口的容器负载平衡*，以及网络控制平面和管理平面功能。

##### 司机

如果`libnetwork`实现控制平面和管理平面功能，那么驱动程序实现数据平面。例如，连接和隔离都由驱动程序处理。网络的实际创建也是如此。关系如图 11.5 所示。

<figure class="image" style="width: 396px">![Figure 11.5](Images/figure11-5.png)

<figcaption>Figure 11.5</figcaption>

</figure>

Docker 附带几个内置驱动程序，称为本地驱动程序或*本地驱动程序*。在 Linux 上，它们包括:`bridge`、`overlay`和`macvlan`。在 Windows 上，它们包括:`nat`、`overlay`、`transparent`和`l2bridge`。我们将在本章后面看到如何使用其中的一些。

第三方也可以编写称为*远程驱动程序*或插件的 Docker 网络驱动程序。编织网是一个流行的例子，可以从 Docker Hub 下载。

每个驱动程序负责其所负责的网络上所有资源的实际创建和管理。例如，名为“prod-fe-cuda”的覆盖网络将由`overlay`驱动程序拥有和管理。这意味着将调用`overlay`驱动程序来创建、管理和删除该网络上的所有资源。

为了满足复杂的高流动性环境的需求，`libnetwork`允许多个网络驱动程序同时活动。这意味着您的 Docker 环境可以支持广泛的异构网络。

#### 单主机桥接网络

最简单的 Docker 网络是单主机桥接网络。

这个名字告诉我们两件事:

*   **单主机**告诉我们它只存在于单个 Docker 主机上，只能连接同一个主机上的容器。
*   **网桥**告诉我们这是一个 802.1d 网桥(第 2 层交换机)的实现。

Linux 上的 Docker 使用内置的`bridge`驱动程序创建单主机桥接网络，而 Windows 上的 Docker 使用内置的`nat`驱动程序创建它们。实际上，它们的工作原理是一样的。

图 11.6 显示了两台 Docker 主机，它们有相同的本地桥接网络，称为“mynet”。尽管网络是相同的，但它们是独立的隔离网络。这意味着图中的容器不能直接通信，因为它们在不同的网络上。

<figure class="image" style="width: 396px">![Figure 11.6](Images/figure11-6.png)

<figcaption>Figure 11.6</figcaption>

</figure>

每个 Docker 主机都有一个默认的单主机桥接网络。在 Linux 上，它被称为“桥”，在 Windows 上，它被称为“nat”(是的，它们与用于创建它们的驱动程序同名)。默认情况下，这是所有新容器将连接到的网络，除非您在命令行上用`--network`标志覆盖它。

下面的清单显示了新安装的 Linux 和 Windows Docker 主机上的`docker network ls`命令的输出。输出经过修整，只显示每台主机上的默认网络。请注意，网络的名称与用来创建它的驱动程序是如何相同的——这是一种巧合，而不是一种要求。

<figure class="code" dir="ltr">

```
//Linux
$ docker network ls
NETWORK ID        NAME        DRIVER        SCOPE
333e184cd343      bridge      bridge        local

//Windows
> docker network ls
NETWORK ID        NAME        DRIVER        SCOPE
095d4090fa32      nat         nat           local 
```

</figure>

 ``docker network inspect`命令是巨大信息的宝库。如果你对低级细节感兴趣，我强烈建议通读它的输出。

<figure class="code" dir="ltr">

```
docker network inspect bridge
[
    {
        "Name": "bridge",     << Will be nat on Windows
        "Id": "333e184...d9e55",
        "Created": "2018-01-15T20:43:02.566345779Z",
        "Scope": "local",
        "Driver": "bridge",   << Will be nat on Windows
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        <Snip>
    }
] 
```

</figure>

 `在 linux 主机上使用`bridge`驱动程序构建的 Docker 网络基于在 Linux 内核中已经存在了近 20 年的久经沙场的 *linux 桥*技术。这意味着它们性能很高，非常稳定。这也意味着您可以使用标准的 Linux 实用程序来检查它们。比如说。

<figure class="code" dir="ltr">

```
$ ip link show docker0
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc...
    link/ether 02:42:af:f9:eb:4f brd ff:ff:ff:ff:ff:ff 
```

</figure>

 `所有基于 Linux 的 Docker 主机上的默认“桥”网络映射到内核中名为“T3”Docker 0 的底层 *Linux 桥*。从`docker network inspect`的输出可以看出这一点。

<figure class="code" dir="ltr">

```
$ docker network inspect bridge | grep bridge.name
"com.docker.network.bridge.name": "docker0", 
```

</figure>

 `Docker 默认的“桥”网络和 Linux 内核中的“docker0”桥之间的关系如图 11.7 所示。

<figure class="image" style="width: 396px">![Figure 11.7](Images/figure11-7.png)

<figcaption>Figure 11.7</figcaption>

</figure>

图 11.8 通过在顶部添加插入“桥”网络的容器扩展了该图。“桥”网络映射到主机内核中的“docker 0”Linux 桥，该桥可以通过端口映射映射回主机上的以太网接口。

<figure class="image" style="width: 396px">![Figure 11.8](Images/figure11-8.png)

<figcaption>Figure 11.8</figcaption>

</figure>

让我们使用`docker network create`命令创建一个名为“localnet”的新的单主机桥接网络。

<figure class="code" dir="ltr">

```
//Linux
$ docker network create -d bridge localnet

//Windows
> docker network create -d nat localnet 
```

</figure>

 `新网络被创建，并将出现在任何未来`docker network ls`命令的输出中。如果你使用的是 Linux，你还会在内核中创建一个新的 *Linux 桥*。

让我们使用 Linux `brctl`工具来看看当前系统上的 Linux 桥。你可能需要使用`apt-get install bridge-utils`或者你的 Linux 发行版的等效软件来手动安装`brctl`二进制文件。

<figure class="code" dir="ltr">

```
$ brctl show
bridge name       bridge id             STP enabled    interfaces
docker0           8000.0242aff9eb4f     no
br-20c2e8ae4bbb   8000.02429636237c     no 
```

</figure>

 `输出显示了两个桥。第一行是我们已经知道的“docker0”桥。这与 Docker 中的默认“桥”网络有关。第二桥(br-20c2e8ae4bbb)涉及新的`localnet` Docker 桥网络。它们都没有启用生成树，也没有连接任何设备(`interfaces`列)。

此时，主机上的网桥配置如图 11.9 所示。

<figure class="image" style="width: 396px">![Figure 11.9](Images/figure11-9.png)

<figcaption>Figure 11.9</figcaption>

</figure>

让我们创建一个新的容器，并将其连接到新的`localnet`桥网络。如果你在 Windows 上跟随，你应该用“`mcr.microsoft.com/powershell:nanoserver pwsh.exe -Command Start-Sleep 86400`”代替“`alpine sleep 1d`”。

<figure class="code" dir="ltr">

```
$ docker container run -d --name c1 \
  --network localnet \
  alpine sleep 1d 
```

</figure>

 `这个容器现在将在`localnet`网络上。你可以用`docker network inspect`确认这一点。

<figure class="code" dir="ltr">

```
$ docker network inspect localnet --format '{{json .Containers}}'
{
  "4edcbd...842c3aa": {
    "Name": "c1",
    "EndpointID": "43a13b...3219b8c13",
    "MacAddress": "02:42:ac:14:00:02",
    "IPv4Address": "172.20.0.2/16",
    "IPv6Address": ""
    }
}, 
```

</figure>

 `输出显示新的“c1”容器在`localnet`桥/nat 网络上。

如果你再次运行 Linux `brctl show`命令，你会看到 c1 的接口连接到`br-20c2e8ae4bbb`桥。

<figure class="code" dir="ltr">

```
$ brctl show
bridge name       bridge id           STP enabled     interfaces
br-20c2e8ae4bbb   8000.02429636237c   no              vethe792ac0
docker0           8000.0242aff9eb4f   no 
```

</figure>

 `这如图 11.10 所示。

<figure class="image" style="width: 396px">![Figure 11.10](Images/figure11-10.png)

<figcaption>Figure 11.10</figcaption>

</figure>

如果我们向同一个网络添加另一个新容器，它应该能够通过名称 ping 通“c1”容器。这是因为所有新容器都自动向嵌入式 Docker DNS 服务注册，使它们能够解析同一网络上所有其他容器的名称。

> **小心:**Linux 上默认的`bridge`网络不支持通过 Docker DNS 服务进行名称解析。所有其他*用户定义的*桥接网络都可以。由于容器位于用户定义的`localnet`网络上，下面的演示将会起作用。

我们来测试一下。

1.  Create a new interactive container called “c2” and put it on the same `localnet` network as “c1”.

    <figure class="code" dir="ltr">

    ```
     //Linux
     $ docker container run -it --name c2 \
       --network localnet \
       alpine sh

     //Windows
     > docker container run -it --name c2 `
       --network localnet `
       mcr.microsoft.com/powershell:nanoserver 
    ```

    </figure>

     `您的终端将切换到“c2”容器。` 
`*   From within the “c2” container, ping the “c1” container by name.

    <figure class="code" dir="ltr">

    ```
     > ping c1
     Pinging c1 [172.26.137.130] with 32 bytes of data:
     Reply from 172.26.137.130: bytes=32 time=1ms TTL=128
     Reply from 172.26.137.130: bytes=32 time=1ms TTL=128
     Control-C 
    ```

    </figure>

     `有效！这是因为 c2 容器正在运行本地 DNS 解析器，该解析器将请求转发到内部 Docker DNS 服务器。该域名系统服务器维护所有以`--name`或`--net-alias`标志开始的容器的映射。`` 

 ``请尝试在您仍登录到容器时运行一些与网络相关的命令。这是了解更多 Docker 容器网络工作原理的好方法。下面的代码片段显示了从先前创建的“c2”窗口容器中运行的`ipconfig`命令。您可以从容器中取出`Ctrl+P+Q`并运行另一个`docker network inspect localnet`命令来匹配 IP 地址。

<figure class="code" dir="ltr">

```
PS C:\> ipconfig
Windows IP Configuration
Ethernet adapter Ethernet:
   Connection-specific DNS Suffix  . :
   Link-local IPv6 Address . . . . . : fe80::14d1:10c8:f3dc:2eb3%4
   IPv4 Address. . . . . . . . . . . : 172.26.135.0
   Subnet Mask . . . . . . . . . . . : 255.255.240.0
   Default Gateway . . . . . . . . . : 172.26.128.1 
```

</figure>

 `到目前为止，我们已经说过桥接网络上的容器只能与同一网络上的其他容器通信。但是，您可以使用*端口映射*来解决这个问题。

端口映射允许您将容器映射到 Docker 主机上的端口。任何到达配置端口上的 Docker 主机的流量都将被定向到容器。高级流程如图 11.11 所示

<figure class="image" style="width: 396px">![Figure 11.11](Images/figure11-11.png)

<figcaption>Figure 11.11</figcaption>

</figure>

在图中，运行在容器中的应用程序正在端口`80`上运行。这被映射到主机的`10.0.0.15`接口上的端口`5000`。最终结果是在`10.0.0.15:5000`上到达主机的所有流量被重定向到端口 80 上的容器。

让我们来看一个例子，将运行 web 服务器的容器上的端口 80 映射到 Docker 主机上的端口 5000。该示例将在 Linux 上使用 NGINX。如果你在 Windows 上继续，你需要用一个基于 Windows 的网络服务器映像代替`nginx`，比如`mcr.microsoft.com/windows/servercore/iis:nanoserver`。

1.  运行一个新的 web 服务器容器，并将容器上的端口 80 映射到 Docker 主机上的端口 5000。

    <figure class="code" dir="ltr">

    ```
     $ docker container run -d --name web \
       --network localnet \
       --publish 5000:80 \
       nginx 
    ```

    </figure>

`*   Verify the port mapping.

    <figure class="code" dir="ltr">

    ```
     $ docker port web
     80/tcp -> 0.0.0.0:5000 
    ```

    </figure>

     `这表明容器中的端口 80 被映射到 Docker 主机上所有接口上的端口 5000。` `*   Test the configuration by pointing a web browser to port 5000 on the Docker host. To complete this step, you’ll need to know the IP or DNS name of your Docker host. If you’re using Docker Desktop on Mac or Windows, you’ll be able to use `localhost:5000` or `127.0.0.1:5000`.

    <figure class="image" style="width: 396px">![Figure 11.12](Images/figure11-12.png)

    <figcaption>Figure 11.12</figcaption>

    </figure>

    现在，任何外部系统都可以通过到 Docker 主机上的 TCP 端口 5000 的端口映射来访问运行在`localnet`桥网络上的 NGINX 容器。`` 

 ``像这样映射端口是可行的，但是它很笨重，无法扩展。例如，只有一个容器可以绑定到主机上的任何端口。这意味着该主机上没有其他容器能够绑定到端口`5000`。这是单主机桥接网络只对本地开发和非常小的应用有用的原因之一。

#### 多主机覆盖网络

我们有一整章专门讨论多主机覆盖网络。所以我们将保持这一部分简短。

覆盖网络是多主机的。它们允许单个网络跨越多个主机，以便不同主机上的容器可以直接通信。它们非常适合容器到容器的通信，包括只包含容器的应用程序，并且可以很好地扩展。

Docker 为覆盖网络提供了本地驱动程序。这使得创建它们就像在`docker network create`命令中添加`--d overlay`标志一样简单。

#### 连接到现有网络

将容器化应用程序连接到外部系统和物理网络的能力至关重要。一个常见的例子是部分容器化的应用程序——容器化的部分需要一种方法来与仍在现有物理网络和虚拟局域网上运行的非容器化部分进行通信。

内置的`MACVLAN`驱动程序(【Windows 上的 T1】)就是基于这一点而创建的。它通过给每个集装箱分配自己的媒体访问控制地址和 IP 地址，使集装箱成为现有物理网络上的一流公民。我们在图 11.13 中展示了这一点。

<figure class="image" style="width: 396px">![Figure 11.13](Images/figure11-13.png)

<figcaption>Figure 11.13</figcaption>

</figure>

从积极的方面来看，MACVLAN 的性能很好，因为它不需要端口映射或额外的网桥——您可以将容器接口连接到主机接口(或子接口)。然而，从负面来看，它要求主机网卡处于**混杂模式**，这在企业网络和公共云平台上并不总是被允许的。因此，MACVLAN 非常适合您的企业数据中心网络(假设您的网络团队可以适应混杂模式)，但它可能无法在公共云中工作。

让我们借助一些图片和一个假设的例子来深入挖掘一下。

假设我们有一个包含两个虚拟局域网的现有物理网络:

*   VLAN 100: 10.0.0.0/24
*   VLAN 200: 192.168.3.0/24

<figure class="image" style="width: 396px">![Figure 11.14](Images/figure11-14.png)

<figcaption>Figure 11.14</figcaption>

</figure>

接下来，我们添加一个 Docker 主机并将其连接到网络。

<figure class="image" style="width: 396px">![Figure 11.15](Images/figure11-15.png)

<figcaption>Figure 11.15</figcaption>

</figure>

然后，我们要求在该主机上运行的容器要接入 VLAN 100。为此，我们使用`macvlan`驱动程序创建了一个新的 Docker 网络。然而，`macvlan`驱动程序需要我们告诉它一些关于我们将要与之关联的网络的事情。比如:

*   子网信息
*   门
*   它可以分配给容器的 IP 范围
*   主机上要使用哪个接口或子接口

以下命令将创建一个名为“macvlan100”的新 MACVLAN 网络，该网络将把集装箱连接到 vlan100。

<figure class="code" dir="ltr">

```
$ docker network create -d macvlan \
  --subnet=10.0.0.0/24 \
  --ip-range=10.0.0.0/25 \
  --gateway=10.0.0.1 \
  -o parent=eth0.100 \
  macvlan100 
```

</figure>

 `这将创建“macvlan100”网络和 eth0.100 子接口。配置现在如下所示。

<figure class="image" style="width: 396px">![Figure 11.16](Images/figure11-16.png)

<figcaption>Figure 11.16</figcaption>

</figure>

MACVLAN 使用标准的 Linux 子接口，你必须用它们将要连接的 VLAN 的 ID 来标记它们。在这个例子中，我们连接到 VLAN 100，所以我们用`.100` ( `etho.100`)标记子接口。

我们还使用`--ip-range`标志来告诉 MACVLAN 网络它可以分配给容器的 IP 地址子集。这个地址范围必须保留给 Docker，不要被其他节点或 DHCP 服务器使用，因为没有管理平面功能来检查重叠的 IP 范围。

`macvlan100`网络已经为容器做好了准备，所以让我们用下面的命令部署一个。

<figure class="code" dir="ltr">

```
$ docker container run -d --name mactainer1 \
  --network macvlan100 \
  alpine sleep 1d 
```

</figure>

 `现在配置如图 11.17 所示。但是请记住，底层网络(VLAN 100)看不到任何 MACVLAN 的魔力，它只看到带有 MAC 和 IP 地址的容器。考虑到这一点，“mactainer1”容器将能够 ping 通 VLAN 100 上的任何其他系统并与之通信。挺贴心的！

> **注意:**如果不能让这个工作，可能是因为主机网卡没有处于混杂模式。请记住，公共云平台通常不允许混杂模式。

<figure class="image" style="width: 396px">![Figure 11.17](Images/figure11-17.png)

<figcaption>Figure 11.17</figcaption>

</figure>

在这一点上，我们已经有了一个 MACVLAN 网络，并使用它来连接一个新的集装箱到一个现有的 VLAN。然而，这并不止于此。Docker MACVLAN 驱动程序建立在经过测试的同名 Linux 内核驱动程序之上。因此，它支持 VLAN 中继。这意味着我们可以创建多个 MACVLAN 网络，并将同一个 Docker 主机上的容器连接到它们，如图 11.18 所示。

<figure class="image" style="width: 396px">![Figure 11.18](Images/figure11-18.png)

<figcaption>Figure 11.18</figcaption>

</figure>

这几乎涵盖了 MACVLAN。Windows 为`transparent`驱动程序提供了类似的解决方案。

##### 用于故障排除的容器和服务日志

在继续服务发现之前，先快速了解连接问题的故障排除。

如果您认为您遇到了容器之间的连接问题，那么值得检查 Docker 守护程序日志以及容器日志。

在 Windows 系统上，守护进程日志存储在`~AppData\Local\Docker`下，可以在 Windows 事件查看器中查看。在 Linux 上，这取决于你使用的是什么系统。如果你正在运行一个`systemd`，日志将进入`journald`，你可以使用`journalctl -u docker.service`命令查看它们。如果您没有运行`systemd`，您应该查看以下位置:

*   运行`upstart` : `/var/log/upstart/docker.log`的 Ubuntu 系统
*   基于 RHEL 的系统:`/var/log/messages`
*   德比安:`/var/log/daemon.log`

您还可以告诉 Docker 您希望守护程序日志记录有多详细。为此，请编辑守护程序配置文件(`daemon.json`)，以便将“`debug`”设置为“`true`”并将“`log-level`”设置为以下选项之一:

*   `debug`最啰嗦的选项
*   `info`默认值和第二详细选项
*   `warn`第三个最冗长的选项
*   `error`第四个最冗长的选项
*   `fatal`最少详细选项

以下来自`daemon.json`的片段启用调试并将级别设置为`debug`。它将在所有 Docker 平台上工作。

<figure class="code" dir="ltr">

```
{
  <Snip>
  "debug":true,
  "log-level":"debug",
  <Snip>
} 
```

</figure>

 `对文件进行更改后，请务必重新启动 Docker。

那是守护进程日志。集装箱日志呢？

使用`docker container logs`命令可以查看独立容器中的日志，使用`docker service logs`命令可以查看 Swarm 服务日志。然而，Docker 支持许多日志驱动程序，它们并不都与`docker logs`命令一起工作。

除了守护程序日志的驱动程序和配置，每个 Docker 主机都有一个默认的日志驱动程序和容器配置。一些驱动因素包括:

*   `json-file`(默认)
*   `journald`(仅适用于运行`systemd`的 Linux 主机)
*   `syslog`
*   `splunk`
*   `gelf`

`json-file`和`journald`可能是最容易配置的，它们都使用`docker logs`和`docker service logs`命令。命令的格式是`docker logs <container-name>`和`docker service logs <service-name>`。

如果您使用其他日志驱动程序，您可以使用第三方平台的本地工具查看日志。

以下来自`daemon.json`的片段显示了配置为使用`syslog`的 Docker 主机。

<figure class="code" dir="ltr">

```
{
  "log-driver": "syslog"
} 
```

</figure>

 `您可以配置一个单独的容器或服务，从带有`--log-driver`和`--log-opts`标志的特定日志驱动程序开始。这些将覆盖`daemon.json`中设置的任何内容。

容器日志的工作前提是您的应用程序在容器内作为 PID 1 运行，并将日志发送到`STDOUT`，将错误发送到`STDERR`。然后，日志驱动程序将这些“日志”转发到通过日志驱动程序配置的位置。

如果您的应用程序登录到一个文件，可以使用符号链接将日志文件写入重定向到 STDOUT 和 STDERR。

以下是对一个名为“vantage-db”的容器运行`docker logs`命令的示例，该容器被配置为使用`json-file`日志驱动程序。

<figure class="code" dir="ltr">

```
$ docker logs vantage-db
1:C 2 Feb 09:53:22.903 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 2 Feb 09:53:22.904 # Redis version=4.0.6, bits=64, commit=00000000, modified=0, pid=1
1:C 2 Feb 09:53:22.904 # Warning: no config file specified, using the default config.
1:M 2 Feb 09:53:22.906 * Running mode=standalone, port=6379.
1:M 2 Feb 09:53:22.906 # WARNING: The TCP backlog setting of 511 cannot be enforced because...
1:M 2 Feb 09:53:22.906 # Server initialized
1:M 2 Feb 09:53:22.906 # WARNING overcommit_memory is set to 0! 
```

</figure>

 `很有可能在守护程序日志或容器日志中发现网络连接错误。

#### 服务发现

除了核心组网，`libnetwork`还提供一些重要的网络服务。

*服务发现*允许所有容器和 Swarm 服务通过名称相互定位。唯一的要求是它们在同一个网络上。

在引擎盖下，这利用了 Docker 的嵌入式 DNS 服务器和每个容器中的 DNS 解析器。图 11.19 显示了容器“c1”通过名称 pinging 容器“c2”。同样的原则也适用于群服务。

<figure class="image" style="width: 396px">![Figure 11.19](Images/figure11-19.png)

<figcaption>Figure 11.19</figcaption>

</figure>

让我们一步步来。

*   **步骤 1:**`ping c2`命令调用本地域名解析器将名称“c2”解析为一个 IP 地址。所有 Docker 容器都有一个本地 DNS 解析器。
*   **步骤 2:** 如果本地解析器的本地缓存中没有“c2”的 IP 地址，它会向 Docker DNS 服务器发起递归查询。本地解析器被预先配置为知道如何到达 Docker DNS 服务器。
*   **步骤 3:**Docker DNS 服务器保存所有使用`--name`或`--net-alias`标志创建的容器的名称到 IP 的映射。这意味着它知道容器“c2”的 IP 地址。
*   **步骤 4:**DNS 服务器将“c2”的 IP 地址返回给“c1”中的本地解析器。这样做是因为这两个容器在同一个网络上——如果它们在不同的网络上，这将不起作用。
*   **步骤 5:**`ping`命令将 ICMP 回应请求数据包发送到“c2”的 IP 地址。

以`--name`标志开始的每个 Swarm 服务和独立容器将向 Docker DNS 服务注册其名称和 IP。这意味着所有容器和服务副本都可以使用 Docker DNS 服务来查找彼此。

然而，服务发现是*网络范围的*。这意味着名称解析仅适用于同一网络上的容器和服务。如果两个容器位于不同的网络上，它们将无法相互解析。

关于服务发现和名称解析的最后一点…

可以使用定制的域名系统选项来配置 Swarm 服务和独立容器。例如，`--dns`标志允许您指定一个自定义域名系统服务器列表，以备嵌入式 Docker 域名系统服务器无法解析查询时使用。这在 Docker 之外查询服务名称时很常见。您也可以使用`--dns-search`标志为针对非限定名称的查询(即当查询不是完全限定域名时)添加自定义搜索域。

在 Linux 上，这些都是通过在每个容器内的`/etc/resolv.conf`文件中添加条目来工作的。

以下示例将启动一个新的独立容器，并添加臭名昭著的`8.8.8.8`谷歌域名系统服务器，以及`nigelpoulton.com`作为搜索域，以追加到不合格的查询中。

<figure class="code" dir="ltr">

```
$ docker container run -it --name c1 \
  --dns=8.8.8.8 \
  --dns-search=nigelpoulton.com \
  alpine sh 
```

</figure>

 `#### 入口负载平衡

Swarm 支持两种发布模式，使服务可以在集群之外访问:

*   入口模式(默认)
*   主机方式

通过*入口模式*发布的服务可以从群集中的任何节点访问，甚至是运行服务副本的节点**而不是**。通过*主机模式*发布的服务只能通过点击运行服务副本的节点来访问。图 11.20 显示了两种模式的区别。

<figure class="image" style="width: 396px">![Figure 11.20](Images/figure11-20.png)

<figcaption>Figure 11.20</figcaption>

</figure>

默认为入口模式。这意味着任何时候您发布带有`-p`或`--publish`的服务，它将默认为*入口模式*。要在*主机模式*下发布服务，需要使用`--publish`标志的长格式**和**添加`mode=host`。我们来看一个使用*主机模式*的例子。

<figure class="code" dir="ltr">

```
$ docker service create -d --name svc1 \
  --publish published=5000,target=80,mode=host \
  nginx 
```

</figure>

 `关于命令的几点注意事项？`docker service create`允许您使用*长格式语法*或*短格式语法*发布服务。简短的形式是这样的:`-p 5000:80`我们已经看过几次了。但是，不能使用短格式在*主机模式*下发布服务。

长型看起来是这样的:`--publish published=5000,target=80,mode=host`。这是一个逗号分隔的列表，每个逗号后面没有空格。这些选项的工作原理如下:

*   `published=5000`通过端口 5000 对外提供服务
*   `target=80`确保对`published`端口的外部请求被映射回服务副本上的端口 80
*   `mode=host`确保外部请求只有通过运行服务副本的节点进入时才会到达服务。

入口模式是您通常使用的模式。

在幕后，*入口模式*使用称为**服务网格**或**群模式服务网格**的第 4 层路由网格。图 11.21 显示了以入口模式公开的服务的外部请求的基本流量。

<figure class="image" style="width: 396px">![Figure 11.21](Images/figure11-21.png)

<figcaption>Figure 11.21</figcaption>

</figure>

让我们快速浏览一下图表。

1.  顶部的命令部署了一个名为“svc1”的新 Swarm 服务。它将服务附加到`overnet`网络，并在端口 5000 上发布。
2.  像这样发布 Swarm 服务(`--publish published=5000,target=80`)将在入口网络的端口 5000 上发布。由于 swarm 中的所有节点都连接到入口网络，这意味着端口在 Swarm 范围内发布*。*
**   在集群上实现逻辑，确保通过端口 5000 上的**任何节点**到达入口网络的任何流量将被路由到端口 80 上的“svc1”服务。*   此时，部署了“svc1”服务的单个副本，并且集群具有一个映射规则，该规则表示“*”所有到达端口 5000 上的入口网络的流量都需要路由到运行“svc1”服务副本的节点*。*   红线表示流量到达端口 5000 上的`node1`，并通过入口网络路由到节点 2 上运行的服务副本。*

 *重要的是要知道传入的流量可能会到达端口 5000 上的四个 Swarm 节点中的任何一个，并且我们会得到相同的结果。这是因为服务是通过入口网络在群体范围内发布的。

同样重要的是要知道，如果有多个副本在运行，如图 11.22 所示，所有副本之间的流量将是平衡的。

<figure class="image" style="width: 396px">![Figure 11.22](Images/figure11-22.png)

<figcaption>Figure 11.22</figcaption>

</figure>

### 码头工人网络-命令

Docker 网络有自己的`docker network`子命令。主要命令包括:

*   `docker network ls`:列出本地 Docker 主机上的所有网络。
*   `docker network create`:创建新的 Docker 网络。默认情况下，它使用 Windows 上的`nat`驱动程序和 Linux 上的`bridge`驱动程序来创建它们。您可以使用`-d`标志指定驱动程序(网络类型)。`docker network create -d overlay overnet`将使用本地 Docker `overlay`驱动程序创建一个名为 overnet 的新覆盖网络。
*   `docker network inspect`:提供 Docker 网络的详细配置信息。
*   `docker network prune`:删除 Docker 主机上所有未使用的网络。
*   `docker network rm`:删除 Docker 主机上的特定网络。

### 章节总结

容器网络模型(CNM)是 Docker 网络的主设计文档，定义了用于构建 Docker 网络的三个主要构造— *沙箱*、*端点*和*网络*。

`libnetwork`是用 Go 写的开源库，实现了 CNM。它由 Docker 使用，是所有核心 Docker 网络代码的所在地。它还提供了 Docker 的网络控制平面和管理平面。

驱动程序通过添加代码来实现特定的网络类型，如桥接网络和覆盖网络，从而扩展 Docker 网络堆栈(`libnetwork`)。Docker 附带了几个内置驱动程序，但您也可以使用第三方驱动程序。

单主机桥接网络是 Docker 网络最基本的类型，适合本地开发和非常小的应用。它们不能扩展，如果您想在网络之外发布服务，它们需要端口映射。Linux 上的 Docker 使用内置的`bridge`驱动实现桥接网络，而 Windows 上的 Docker 使用内置的`nat`驱动实现桥接网络。

覆盖网络风靡一时，是优秀的仅容器多主机网络。我们将在下一章深入讨论它们。

`macvlan`驱动程序(Windows 上的`transparent`)允许您将容器连接到现有的物理网络和虚拟局域网。他们通过给集装箱一级公民自己的 MAC 和 IP 地址来制造集装箱。不幸的是，它们需要主机网卡上的混杂模式，这意味着它们无法在公共云中工作。

Docker 还使用`libnetwork`来实现基本的服务发现，以及用于入口流量的基于容器的负载平衡的服务网格。*`````````````````````*****