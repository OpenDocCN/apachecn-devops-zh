# 第十章。蜂群和云

在本书中，我们与 Docker Swarm 在一组不同的底层技术上进行了合作，但迄今为止，我们没有深入探讨这一含义:我们在 AWS、DigitalOcean 和本地工作站上运行 Swarm。出于测试和准备的目的，我们运行 Swarm 的平台可能是次要的(*让我们用 Docker Machine 启动一些 AWS 实例，并以这种方式工作*，但是对于生产来说，理解利弊、推理、评估和跟踪趋势是强制性的。

在本章中，我们将回顾几种公共云和私有云选项和技术及其可能的交集。我们最终将在[第 11 章](11.html "Chapter 11. What is next?")*中对待 **CaaS** ( **容器即服务**)和 **IaaC** ( **基础设施即代码**)的全新流行语，接下来是什么？*

我们将主要关注:

*   AWS 和 Azure 的 Docker
*   数据中心坞站
*   在 OpenStack 上群集

# AWS 和 Azure 的 Docker

只要有了 Docker For Mac 和 Windows，Docker 团队就开始着手为运营商开发*新一代*工具集:Docker for AWS 和 Docker for Windows。这些旨在为部署 Docker 基础架构提供一种自动化的体验，尤其是面向群的基础架构。

目标是为人们提供一种标准的做事方式，将底层基础设施与 Docker 工具集成在一起，并让人们在他们喜欢的平台上不费力地运行最新的软件版本。最终目标真的是让开发者把东西从搭载 Docker for Mac/Windows 的笔记本电脑搬到云端，搭载 Docker for AWS/Azure。

## AWS 的Docker

在 Docker 生态系统中，用户体验一如既往地出色。这些要求是:

*   自动气象站识别码
*   一个 SSH 密钥导入到您的 AWS 密钥环中
*   就绪安全组

基本上，AWS 的 Docker 是一个云表单的可点击模板。CloudForms 是 AWS 的编排系统，它允许创建复杂系统的模板，例如，您可以指定一个由三个 web 服务器、一个数据库和一个负载平衡器组成的 web 基础架构。

AWS 的 Docker 当然不是 web 或其他通用基础设施，而是具有创建 Docker Swarm(模式)基础设施的能力:它创建您指定数量的主机和工作机，将负载平衡器放在前面，并相应地配置所有网络。

这是欢迎屏幕:

![Docker for AWS](img/image_10_001.jpg)

然后，您可以指定一些基本和高级选项:

![Docker for AWS](img/image_10_002.jpg)

如您所见，您可以选择经理和员工的数量，以及要启动的实例的风格。到目前为止，多达 1000 名工人得到了支持。之后，您只需在下一步中单击创建堆栈，并等待几分钟，让 CloudForms 启动基础架构。

模板的作用是:

1.  在您的 AWS 帐户、网络和子网中创建一个新的虚拟私有云。
2.  创建两个自动缩放组，一个用于经理，一个用于工人。
3.  启动经理，确保他们健康成长，达到 Raft 法定人数。
4.  开始一个接一个地将工人登记到群中。
5.  创建**弹性负载平衡器** ( **ELBs** )来路由流量
6.  结束

一旦云信息完成，它将提示绿色确认。

![Docker for AWS](img/image_10_003.jpg)

现在，我们已经准备好进入新的 Docker Swarm 基础架构。只需拿起管理器的一个公共 IP，并使用第一步中指定的 SSH 密钥连接到它:

```
 ssh docker@ec2-52-91-75-252.compute-1.amazonaws.com

```

![Docker for AWS](img/image_10_004.jpg)

## 蓝色Docker

由于与微软达成协议，Azure 的自动 Swarm 部署也可以作为一次点击体验(或几乎)使用。

在 Azure 上部署 Swarm 的先决条件是:

*   拥有有效的 Azure 帐户
*   将此帐户标识与 Azure 的 Docker 关联
*   活动目录主体应用标识

要生成最后一个，您可以方便地使用 docker 映像，并通过以下方式启动它:

```
docker run -it docker4x/create-sp-azure docker-swarm

```

在此过程中，在某些时候，您将被要求通过浏览器登录到指定的网址。最后，您可以在 Azure 向导表单中输入一个配对标识/密码。

![Docker for Azure](img/image_10_005.jpg)

一切正常后，点击**确定****创建**即可。

![Docker for Azure](img/image_10_006.jpg)

只要有适当的内部网络、负载平衡器和路由器，就会创建一组经典虚拟机来运行指定数量的管理器(这里是 1)和工作器(这里是 4)。就像在 AWS 的 Docker 中一样，您可以通过 ssh 到一个管理器的公共 IP 来开始使用您部署的 Swarm:

```
ssh docker@52.169.125.191

```

![Docker for Azure](img/image_10_007.jpg)

现在的 Azure 模板有一个限制，它只支持一个管理器。然而，增加新经理的可能性应该很快就会出现。

# 数据中心坞站

Docker 数据中心，前身是 Tutum，由 Docker 收购，是 Docker 使用通用控制面板 UCP 的一键部署解决方案，是 Docker 的商业和企业产品。

码头数据中心包括:

*   **通用控制平面** ( **UCP** )，界面，参考[https://docs.docker.com/ucp/overview](https://docs.docker.com/ucp/overview)
*   **Docker 可信注册(DTR)**私有注册，参考[https://docs.docker.com/docker-trusted-registry](https://docs.docker.com/docker-trusted-registry)

在 Dockercon 16 上，该团队发布了对在 AWS 和 Azure 上运行的 Docker 数据中心的支持(目前处于测试阶段)。要试用 Docker 数据中心，您需要将许可证与您的公司/项目 AWS 或 Azure ID 相关联。

对于 AWS 的数据中心，就像对于 AWS 的 Docker 一样，有一个云信息模板可以立即启动 Docker 数据中心。要求是:

*   至少配置一个路由 53，即 AWS DNS 服务，参见
*   a 数据中心许可证坞站

您需要做的是按照许可证上的链接进入“创建堆栈”页面。从这里，您只需输入**主机**标识和 Docker 数据中心许可证，并开始创建堆栈。在内部，Docker Datacenter 将一些虚拟机放在专用网络(节点)上，而另一些虚拟机则由弹性负载平衡器(控制器为 ELBs)进行负载平衡，并在其上安装商业支持的引擎版本。Docker 数据中心虚拟机的当前版本在内部运行 Swarm 独立和发现机制，以相互连接。我们可以预期数据中心的稳定版本将很快发布。

Docker 数据中心和 and Docker 的主要区别在于，第一个数据中心旨在为所有企业做好准备。虽然后者是部署特定 Swarm 集群的最快方法，但前者更像是一个完整的解决方案，具有精美的用户界面、公证人和来自生态系统的可选服务。

# 聚集在开放堆栈上

说到私有云，最流行的 IaaS 开源解决方案是 OpenStack。OpenStack 是一个伟大的程序生态系统(以前称为项目)，目标是提供所谓的云操作系统。OpenStack 的核心程序有:

*   **Keystone** :身份和授权系统
*   **Nova** :虚拟机抽象层。Nova 可以插入虚拟化模块，如 Libvirt、VMware
*   **中子**:网络模块，处理租户网络、实例端口、路由和流量
*   **煤渣**:负责处理卷的存储模块
*   **一瞥**:映像存储

一切都被其他演员粘在一起:

*   数据库系统，如 MySQL，保存配置
*   一个 AMQP 经纪人，如兔子，排队和交付业务
*   一个代理系统，比如 HAproxy，用来代理 HTTP API 请求

在 OpenStack 中的典型虚拟机创建中，会发生以下情况:

1.  来自用户界面(地平线)或命令行界面的用户决定生成虚拟机。
2.  她/他点击按钮或键入命令，如`nova boot ...`
3.  Keystone 通过签入用户的数据库或 LDAP(取决于 OpenStack 的配置方式)来检查该用户在其租户中的授权和身份验证，并生成将在整个会话中使用的令牌:`Here is your token: gAAAAABX78ldEiY2` *。*
4.  如果身份验证成功，并且用户被授权生成虚拟机，Nova 将通过使用授权令牌来调用:“我们正在启动一个虚拟机，您能在哪里找到合适的物理主机吗？”
5.  如果存在这样的主机，Nova 会从“一瞥”中获取用户选择的映像:“一瞥，请递给我一个 Ubuntu Xenial 可引导 qcow2 文件”
6.  在物理启动虚拟机的计算主机上，一个`nova-compute`进程与配置的插件对话，例如，对 Libvirt 说:“我们正在这个主机上启动一个虚拟机”
7.  中子为虚拟机分配私有(和公共，如果需要)网络端口:“请在这些子网池中的指定网络上创建这些端口”
8.  如果用户愿意，煤渣会在调度程序设计的主机上分配卷。那就是。让我们创建额外的卷，并将它们连接到虚拟机。
9.  如果使用 KVM，将生成一个包含上述所有信息的合适的 XML，Libvirt 将在计算主机上启动虚拟机
10.  当虚拟机启动时，一些变量通过 cloud-init 注入，例如，SSH 密钥允许无密码 SSH 登录

这正是 Docker Machine 的 OpenStack 驱动程序的行为方式(This 上的步骤 8 除外):当您使用`-d openstack`用 Machine 创建 Docker Host 时，您必须指定一个现有的扫视映像、一个现有的私有(并且可选地是公共)网络，并且(可选地，否则是自动生成的)指定一个 SSH 映像，存储在 Nova 数据库中。当然，您必须将授权变量传递给您的 OpenStack 环境中的 Machine，或者将它们作为导出的 shell 变量。

在 OpenStack 上创建 Docker 主机的机器命令将如下所示:

```
docker-machine create \
--driver openstack \
--openstack-image-id 98011e9a-fc46-45b6-ab2c-cf6c43263a22 \
--openstack-flavor-id 3 \
--openstack-floatingip-pool public \
--openstack-net-id 44ead515-da4b-443b-85cc-a5d13e06ddc85 \
--openstack-sec-groups machine \
--openstack-ssh-user ubuntu \
ubuntu1

```

## 新的 OpenStack

因此，OpenStack 上 Docker Swarm 的经典方法是在专用网络上开始创建实例，比如说来自 Ubuntu 16.04 映像的 10 个虚拟机:

*   从 web 用户界面，指定 10 作为实例数
*   或者从命令行界面，使用`nova boot ... --max-count 10 machine-`
*   或者通过使用Docker机器

最后一种是更有前途的方式，因为 Machine 会自动安装 Docker，而无需以后在新创建的实例上进行黑客攻击或使用其他工具(例如带有通用驱动程序的 Machine、Belt、Ansible、Salt 或其他脚本)。但是在编写时(Machine 0.8.2)，Machine 不支持大容量主机创建，所以您将不得不使用一些基本的外壳逻辑来循环一个`docker-machine`命令:

```
#!/bin/bash
for i in `seq 0 9`; do
 docker-machine create -d openstack ... openstack-machine-$i
done

```

这根本不是一个好的 UX，也因为当我们谈到几十台主机时，机器规模仍然非常糟糕。

### (不推荐使用的)新Docker驱动程序

曾经，Nova 有一个驱动程序，将 Docker 容器作为 Nova 的最终目的地(例如，这些驱动程序允许从 Nova 创建和管理 Docker 容器，而不是创建 KVM 或 VmWare 虚拟机)。如果对旧的*Swarm 使用这样的工具是有意义的(因为一切都是作为容器编排的)，那么 Swarm Mode 对此不感兴趣，它需要 Docker Hosts 而不是裸容器。*

 *## 现实——友好的方式打开堆栈

幸运的是，OpenStack 是一个非常有活力的项目，现在已经到了发布**O**(**octa**)的时候，它被很多可选模块丰富了。从 Docker Swarm 的角度来看，最有趣的是:

*   **热度:**这是指挥系统，可以通过模板创建 VMs 配置。
*   **Murano:** 这是应用目录，可以从开源社区维护的目录中运行应用，包括 Docker 和 Kubernetes 容器。
*   **Magnum:** 这是 Rackspace 的容器即服务解决方案。
*   **Kuryr:** 这是网络抽象器。使用 Kuryr，您可以链接中子租户网络和用 Docker Libnetwork 创建的 Docker 网络(如 Swarm 网络)，并将 OpenStack 实例与 Docker 容器连接起来，就像它们连接到同一个网络一样。

## 打开堆栈加热

OpenStack Heat 有点像 Docker Compose，允许您通过模板启动系统，但它要强大得多:您不仅可以从一个映像引导一组实例，比如 Ubuntu 16.04，还可以编排它们，这意味着创建网络，将虚拟机接口连接到网络，放置负载平衡器并在实例上执行后续任务，例如安装 Docker。大致来说，Heat 相当于亚马逊为 OpenStack 开发的 CloudFormation。

在热火，一切都是从 YAML 模板开始的，得益于它，你可以在启动基础设施之前对其进行建模，就像你使用 Compose 一样。例如，您可以创建如下模板文件:

```
...
resources:
  dockerhosts_group:
    type: OS::Heat::ResourceGroup
    properties:
      count: 10
      resource_def:
        type: OS::Nova::Server
        properties:
          # create a unique name for each server
          # using its index in the group
          name: docker_host_%index%
          image: Ubuntu 16.04
          flavor: m.large
...

```

然后，您可以从中启动一个堆栈(`heat stack-create -f configuration.hot dockerhosts`)。Heat 将调用 Nova、中子、煤渣和所有必要的 OpenStack 服务来编排资源并使它们可用。

这里我们不打算展示如何通过 Heat 启动 Docker Swarm 基础设施，而是在这里我们将看到 Magnum，它在下面使用 Heat 来操作 OpenStack 对象。

## OpenStack Magnum

Magnum 于 2015 年末发布，由 OpenStack 容器团队开发，旨在使 Docker Swarm 和 **Kubernetes** 等**容器编排引擎** ( **COEs** )成为 OpenStack 中的一流资源。OpenStack 领域曾经有并将有许多项目专注于提供容器支持，但 Magnum 走得更远，因为它旨在支持*容器编排*，而不是裸容器管理。

![OpenStack Magnum](img/image_10_008.jpg)

到目前为止，重点特别放在了 Kubernetes 上，但我们这里说的是 **Magnum** ，因为它是最有希望的开源技术，提供了一种在私有云上运行 CaaS 编排的便捷方式。在撰写本文时，马格南还不支持最新的蜂群模式:这个特性必须得到解决。作者打开了一个启动板蓝图，最终可能会在书出版后开始工作:[https://蓝图.启动板. net/magnum/+spec/swarm-mode-support](https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support)。

### 架构和核心概念

Magnum 有两个主要组件，运行在控制器节点上:

```
magnum-api
magnum-conductor

```

第一个进程`magnum-api`是典型的 OpenStack API 提供程序，由 magnum Python 客户端或其他进程调用进行操作，例如创建集群。后者`magnum-conductor`由`magnum-api`(或多或少与`nova-conductor`功能相同)通过一个 AMQP 服务器调用，如 Rabbit，其目标是与 Kubernetes 或 Docker APIs 接口。实际上，这两个二进制文件共同提供了一种编排抽象。

![Architecture and core concepts](img/image_10_009.jpg)

在 OpenStack 集群计算节点上，除了`nova-compute`进程之外，没有什么特别需要运行的:Magnum conductor 直接利用 Heat 创建堆栈，从而在 Nova 中创建网络并实例化虚拟机。

随着项目的进行，马格南术语也在不断发展。但这些是主要的概念:

*   **容器**是 Docker 容器。
*   一个**集群**(以前是一个湾)是一个节点对象的集合，在那里工作被调度，例如，Swarm 节点。
*   一个**集群模板**(以前的 BayModel)是存储集群类型信息的模板。例如，一个集群模板定义*一个有 3 个经理和 5 个工人的集群*。
*   **Pods** 是在同一台物理机或虚拟机上运行的容器的集合。

至于高级选项，如存储、新的 Coe 支持和扩展，Magnum 是一个非常活跃的项目，我们建议您在[http://docs.openstack.org/developer/magnum/](http://docs.openstack.org/developer/magnum/)上关注它的发展。

### 在 Mirantis OpenStack 上安装 HA Magnum

安装 Magnum 并不是一件小事，尤其是如果你想保证 OpenStack HA 部署中的一些典型故障转移。网上有很多关于如何在 DevStack(开发人员的 1 节点分段设置)中配置 Magnum 的教程，但是没有一个说明如何在有多个控制器的真实生产系统上工作。这里我们展示了如何在真实的设置中安装 Magnum..

通常，生产 OpenStack 安装会计算专用于不同目标的节点数量。在最小高可用性部署中，通常有:

*   三个或更多(出于法定人数原因为奇数)**控制器节点**，负责托管 OpenStack 程序的 API 和配置服务，如 Rabbit、MySQL 和 HAproxy
*   任意数量的**计算节点**，工作负载物理运行于此(虚拟机托管于此)

可选地，可以有专用的存储、监控、数据库、网络和其他节点。

在这里的设置中，基于安装了 Heat 的运行 Newton 的 **Mirantis OpenStack** ，我们有三个控制器和三个计算加存储节点。HA 配置了起搏器，使 MySQL、Rabbitmq 和 HAproxy 等资源保持高可用性。API 由 HAproxy 代理。这是显示起搏器中配置的资源的屏幕截图。它们都已启动并正常工作:

![Install HA Magnum on Mirantis OpenStack](img/image_10_010.jpg)

集群中所有节点都运行 Ubuntu 16.04 (Xenial)，对于这个版本稳定的 Magnum 2.0 包是存在的，从上游消耗下来，用`apt-get install`安装就足够了。

然而，在安装 Magnum 之前，有必要准备环境。首先，需要一个数据库。只需键入以下命令，即可从任何控制器进入 MySQL 控制台:

```
node-1# mysql

```

在 MySQL 中，创建 magnum 数据库和用户，并授予正确的权限:

```
CREATE DATABASE magnum;
GRANT ALL PRIVILEGES ON magnum.* TO 'magnum'@'controller' \
  IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON magnum.* TO 'magnum'@'%' \
  IDENTIFIED BY 'password';

```

现在，有必要在 Keystone 中创建服务凭证，首先定义一个必须添加到服务组的 magnum OpenStack 用户。服务组是一个特殊的组，其中包括跨集群运行的 OpenStack 服务，如 Nova、中子等。

```
openstack user create --domain default --password-prompt magnum
openstack role add --project services --user magnum admin

```

之后，必须创建新的服务:

```
openstack service create --name magnum \   --description "OpenStack 
    Container Infrastructure" \   container-infra

```

OpenStack 程序通过它们的 API 被调用和对话。通过端点访问应用编程接口，端点是一对网址和端口，在高可用性设置中由高可用性代理。在我们的设置中，HAproxy 在`10.21.22.2`接收 HTTP 请求。并在控制器 IPs 之间平衡它们，即`10.21.22.4, 5`和`6`。

![Install HA Magnum on Mirantis OpenStack](img/image_10_011.jpg)

我们必须为 Magnum 创建这样的端点，默认情况下，Magnum 在端口 9511 上监听每个区域(公共、内部和管理):

```
openstack endpoint create --region RegionOne \
  container-infra public http://10.21.22.2:9511/v1
openstack endpoint create --region RegionOne \
  container-infra internal http://10.21.22.2:9511/v1
openstack endpoint create --region RegionOne \
  container-infra admin http://10.21.22.2:9511/v1

```

此外，Magnum 需要额外的配置来在域内部组织其工作负载，因此必须添加一个专用域和一个域用户:

```
openstack domain create --description "Magnum" magnum
openstack user create --domain magnum --password-prompt 
    magnum_domain_admin
openstack role add --domain magnum --user magnum_domain_admin admin

```

现在一切就绪，终于可以运行`apt-get`了。在所有三个控制器上，运行以下命令，并在 ncurses 界面中，始终回答“否”，以不更改环境或保持默认配置:

```
apt-get install magnum-api magnum-conductor

```

### 配置高可用性马格南安装

马格南的配置非常简单。要使它处于启动和运行状态，需要做的是:

1.  通过`magnum.conf`文件进行配置。
2.  重启马格南二进制文件。
3.  打开端口`tcp/9511`。
4.  配置 HAproxy 以接受和平衡 magnum APIs。
5.  重新载入 HAproxy。

必须在每个控制器上完成的关键配置如下。首先，在每个控制器上，主机参数应该是管理网络上接口的 IP:

```
[api]
host = 10.21.22.6

```

如果没有安装**巴比肯**(专门管理密码等机密的 OpenStack 项目)，证书必须由`**x509keypair**`插件处理:

```
[certificates]
cert_manager_type = x509keypair

```

然后，需要一个数据库连接字符串。在这个高可用性设置中，MySQL 在 VIP `10.21.22.2`上回答:

```
[database]
connection=mysql://magnum:password@10.21.22.2/magnum

```

Keystone 身份验证的配置如下(选项不言自明):

```
[keystone_authtoken]
auth_uri=http://10.21.22.2:5000/
memcached_servers=10.21.22.4:11211,
    10.21.22.5:11211,10.21.22.6:11211
auth_type=password
username=magnum
project_name=services
auth_url=http://10.21.22.2:35357/
password=password
user_domain_id = default
project_domain_id = default
auth_host = 127.0.0.1
auth_protocol = http
admin_user = admin
admin_password =
admin_tenant_name = admin

```

Oslo(消息代理)必须配置为消息传递:

```
[oslo_messaging_notifications]
driver = messaging

```

Rabbitmq 配置是这样的，它指定了 Rabbit 集群主机(因为 Rabbit 运行在控制器上，所有控制器管理网络的入侵防御系统):

```
[oslo_messaging_rabbit]
rabbit_hosts=10.21.22.6:5673, 10.21.22.4:5673, 10.21.22.5:5673
rabbit_ha_queues=True
heartbeat_timeout_threshold=60
heartbeat_rate=2
rabbit_userid=magnum
rabbit_password=A3elbTUIqOcqRihB6XE3MWzN

```

最后，受托人的附加配置如下:

```
[trust]
trustee_domain_name = magnum
trustee_domain_admin_name = magnum_domain_admin
trustee_domain_admin_password = magnum

```

重新配置后，马格南服务必须重新启动:

```
service magnum-api restart
service magnum-conductor restart

```

Magnum 默认使用端口`tcp/9511`，因此到该端口的流量必须在 iptables 中接受:修改 iptables 以添加此规则:

```
-A INPUT -s 10.21.22.0/24 -p tcp -m multiport --dports 9511 -m 
    comment --comment "117 magnum-api from 10.21.22.0/24" -j ACCEPT

```

就在其他 OpenStack 服务之后，就在`116 openvswitch db`之后。

现在，是时候配置 HAproxy 接受 magnum 了。在所有控制器上的`/etc/haproxy/conf.d`中添加一个`180-magnum.cfg`文件，内容如下:

```
listen magnum-api
 bind 10.21.22.2:9511
 http-request  set-header X-Forwarded-Proto https if { ssl_fc }
 option  httpchk
 option  httplog
 option  httpclose
 option  http-buffer-request
 timeout  server 600s
 timeout  http-request 10s
 server node-1 10.21.22.6:9511  check inter 10s fastinter 2s 
      downinter 3s rise 3 fall 3
 server node-2 10.21.22.5:9511  check inter 10s fastinter 2s 
      downinter 3s rise 3 fall 3
 server node-3 10.21.22.4:9511  check inter 10s fastinter 2s 
      downinter 3s rise 3 fall 3

```

这将 magnum-api 配置为监听 VIP `10.21.22.2:9511`，支持三个控制器。

就在这之后，必须从起搏器重新启动 HAproxy。从任何控制器运行:

```
pcs resource disable p_haproxy

```

等到所有控制器上都没有运行 HAproxy 进程(可以用`ps aux`检查)，但这应该非常快，不到 1 秒，然后:

```
pcs resource enable p_haproxy

```

之后，Magnum 将提供以下服务:

```
source openrc
magnum service-list

```

![Configure an HA Magnum installation](img/image_10_012.jpg)

### 在马格南上创建一个蜂群集群

创建群集群，当 COE 将被添加到 Magnum 时，将需要以下步骤:

1.  创建群体模式模板。
2.  从模板启动集群。

我们不会过多地探究尚不存在的东西，但命令会是这样的:

```
magnum cluster-template-create \
--name swarm-mode-cluster-template \
--image-id ubuntu_xenial \
--keypair-id fuel \
--fixed-network private \
--external-network-id public \
--dns-nameserver 8.8.8.8 \
--flavor-id m1.medium \
--docker-volume-size 5 \
--coe swarm-mode

```

这里定义了一个基于 Ubuntu Xenial 的集群模板，具有`m1.medium`种风格:虚拟机将被注入燃料密钥空气，将有一个额外的外部公共知识产权。基于这样一个模板创建集群的 UX 应该是:

```
magnum cluster-create --name swarm-mode-cluster \
      --cluster-template swarm-mode-cluster-template \
      --manager-count 3 \
      --node-count 8

```

这里，一个 Swarm 集群被实例化为三个管理人员和五个工作人员。

Magnum 是一个伟大的项目，在 OpenStack 上运行容器编排的最高抽象级别。它运行在 Rackspace 云上，并通过 Carina 提供给公众使用，参见[http://blog . Rackspace . com/Carina by Rackspace-simplified-containers-with-easy-on-native-container-environment](http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment)。

# 总结

在本章中，我们探索了可以运行 Docker Swarm 集群的替代平台。我们使用了最新的 Docker 工具 AWS 的 Docker 和 Azure 的 Docker——我们用它们来演示如何以新的方式安装 Swarm。在介绍了 Docker Datacenter 之后，我们转向了私有云部分。我们在 OpenStack 上工作，展示了如何在上面运行 Docker 主机，如何安装 OpenStack Magnum，以及如何在上面创建 Swarm 对象。我们差不多完成了旅行。

下一章也是最后一章将概述 Docker 编排的未来。*