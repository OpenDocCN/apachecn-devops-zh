# 管弦乐队

在前一章中，我们介绍了 Docker Compose，这是一个允许我们在单个 Docker 主机上使用声明方式定义的多服务应用的工具。

本章介绍管弦乐队的概念。它告诉我们为什么需要管弦乐队，以及他们在概念上是如何工作的。本章还将概述最受欢迎的管弦乐队，并列举一些他们的优缺点。

在本章中，我们将涵盖以下主题:

*   什么是管弦乐队，为什么我们需要他们？
*   管弦乐队的任务
*   流行管弦乐队概述

完成本章后，您将能够:

*   说出指挥者负责的三到四项任务
*   列出两到三个最受欢迎的管弦乐队
*   用你自己的话和适当的类比，向感兴趣的外行解释为什么我们需要容器编排器

# 什么是管弦乐队，为什么我们需要他们？

在[第 6 章](06.html) *【分布式应用架构】*中，我们学习了哪些模式和最佳实践常用于成功构建、发布和运行高度分布式的应用。现在，如果我们的高度分布式应用是容器化的，那么我们将面临与非容器化分布式应用完全相同的问题或挑战。其中一些挑战在[第 6 章](06.html)、*分布式应用架构*中讨论，服务发现、负载平衡、扩展等等。

与 Docker 对容器所做的类似——通过引入容器来标准化软件的包装和运输——我们希望有一些工具或基础设施软件来处理所有或大部分提到的挑战。这个软件变成了我们所说的编排器，或者我们也称之为编排引擎。

如果我刚才说的对你来说还没有多大意义，那我们换个角度来看。以演奏乐器的艺术家为例。他们可以独自为观众演奏美妙的音乐，而不仅仅是艺术家和他们的乐器。但是现在请一群音乐家。把他们都放在一个房间里，给他们一首交响乐的音符，让他们演奏，然后离开房间。没有任何导演，这群非常有才华的音乐人就无法和谐地演奏这首曲子；这听起来或多或少像是不和谐的声音。只有当管弦乐队有一个指挥来指挥一群音乐家时，我们才能欣赏到管弦乐队的音乐:

![](img/2e78b35e-a8de-4a49-8a6f-c212a561baa0.jpg)

A container orchestrator is like the conductor of an orchestra

我们现在有了容器而不是音乐家，也没有了不同的乐器，我们有了对容器主机运行有不同要求的容器。音乐不是以不同的节奏播放，我们有容器，它们以特定的方式相互通信，并且必须放大和缩小。在这一点上，容器管弦乐队的指挥和管弦乐队的指挥有着非常相似的角色。它确保集群中的容器和其他资源和谐地一起工作。

我希望你现在能更清楚地看到什么是容器编排器，为什么我们需要一个。假设您确认了这个问题，我们现在可以问自己指挥者将如何实现预期的结果，即确保集群中的所有容器彼此和谐地演奏。答案是，管弦乐队的指挥必须执行非常具体的任务，类似于管弦乐队的指挥也有一套他们执行的任务，以便驯服并同时提升管弦乐队。

# 管弦乐队的任务

*那么，* *我们期望一个值得花钱的指挥者为我们执行的任务是什么？*我们来详细看看。下面的列表显示了在撰写本文时，企业用户通常期望从他们的编排者那里得到的最重要的任务。

# 调和期望的状态

当使用一个编排器时，可以用声明的方式告诉它如何运行一个给定的应用或应用服务。我们在[第八章](08.html) *、Docker Compose* 中学习了*陈述句*与*祈使句*的含义。描述我们想要运行的应用服务的声明性方式的一部分是元素，例如使用哪个容器映像、运行这个服务的多少个实例、打开哪些端口等等。我们的应用服务属性的声明就是我们所说的*期望状态*。

因此，当我们现在告诉 orchestrator 第一次根据声明创建这样一个新的应用服务时，orchestrator 会确保在集群中调度尽可能多的容器。如果容器映像在应该运行容器的集群的目标节点上还不可用，那么调度程序会确保首先从映像注册表中下载它们。接下来，容器以所有设置开始，例如要连接的网络或要公开的端口。指挥者尽最大努力在集群中实现与我们声明中完全一致的效果。

一旦我们的服务按照请求启动并运行，也就是说，它在期望的状态下运行，那么编排者继续监视它。每当编排者发现服务的实际状态与其期望状态之间存在差异时，它会再次尽力协调期望状态。

应用服务的实际状态和期望状态之间会有什么样的差异？好吧，假设服务的一个副本，也就是其中一个容器，由于一个错误而崩溃，那么编排者将发现实际状态与期望状态在副本数量上不同:缺少一个副本。编排者将立即安排一个新实例到另一个集群节点，替换崩溃的实例。另一个差异可能是，如果服务被缩减，运行的应用服务实例太多。在这种情况下，编排者只需随机删除所需数量的实例，以实现实际实例数和所需实例数之间的均等。另一个差异可能是当编排者发现应用服务的一个实例运行错误(可能是旧的)版本的底层容器映像时。现在，你应该明白了，对吧？

因此，我们不是主动监视集群中运行的应用服务并纠正与期望状态的任何偏差，而是将这项繁琐的任务委托给编排者。如果我们使用声明性的而不是命令性的方式来描述应用服务的期望状态，这将非常有效。

# 复制和全球服务

有两种完全不同类型的服务，我们可能希望在由编排者管理的集群中运行。它们是**复制的**和**全球**服务。复制服务是需要在特定数量的实例中运行的服务，例如 10 个实例。反过来，全局服务是一种需要在集群的每个工作节点上运行一个实例的服务。我在这里使用了术语*工人节点*。在由指挥者管理的集群中，我们通常有两种类型的节点，**管理人员**和**工作人员**。管理器节点通常只由编排器用来管理集群，不运行任何其他工作负载。工作节点反过来运行实际的应用。

因此，orchestrator 确保对于一个全局服务，它的一个实例在每个工作节点上运行，不管有多少个工作节点。我们不需要关心实例的数量，只需要在每个节点上保证运行服务的单个实例。

我们可以再次完全依靠指挥者来完成这一壮举。在复制的服务中，我们将始终保证找到确切的所需实例数，而对于全局服务，我们可以确信，在每个工作节点上，将始终恰好运行一个服务实例。指挥者将一直尽最大努力来保证这种期望的状态。

In Kubernetes, a global service is also called a daemon set.

# 服务发现

当我们以声明的方式描述应用服务时，我们不应该告诉编排者服务的不同实例必须在哪些集群节点上运行。我们让编排者来决定哪些节点最适合这个任务。

当然，在技术上可以指示指挥者使用非常确定的布局规则，但这是一种反模式，根本不推荐。

因此，如果我们现在假设编排引擎对于将应用服务的单个实例放在哪里有完整和自由的意愿，而且实例可能崩溃并被编排者重新安排到不同的节点，那么我们将意识到跟踪单个实例在任何给定时间的运行位置是徒劳的。更好的是，我们甚至不应该试图了解这一点，因为它并不重要。

好吧，你可能会说，但是如果我有两个服务，A 和 B，服务 A 依赖于服务 B 呢；*服务 A 的任何给定实例不应该知道在哪里可以找到服务 B 的实例吗？*

在这一点上，我必须大声而明确地说——不，不应该。这种知识在高度分布式和可伸缩的应用中是不可取的。相反，我们应该依靠编排者来为我们提供到达我们所依赖的其他服务实例所需的信息。这有点像电话技术的旧时代，那时我们不能直接给朋友打电话，而必须给电话公司的中心局打电话，然后由接线员把我们送到正确的目的地。在我们的例子中，指挥者扮演操作员的角色，将来自服务 A 实例的请求路由到服务 b 的可用实例。整个过程称为**服务发现**。

# 按指定路线发送

到目前为止，我们已经了解到，在分布式应用中，我们有许多交互服务。当服务 A 与服务 B 交互时，是通过数据包的交换来实现的。这些数据包需要以某种方式从服务 A 传输到服务 b。将数据包从源传输到目的地的过程也称为**路由**。作为应用的作者或操作者，我们确实期望编排者接管路由任务。正如我们将在后面的章节中看到的，路由可以发生在不同的层次上。就像现实生活中一样。假设你在一家大公司的办公楼里工作。现在，您有一份文档需要转发给公司的另一名员工。内部邮局将从发件箱中取出文件，并将其送到位于同一栋大楼的邮局。如果目标人在同一栋楼里工作，那么文件就可以直接转发给那个人。另一方面，如果此人在同一街区的另一栋大楼工作，该文件将被转发到目标大楼的邮局，然后从那里通过内部邮政服务分发给收件人。第三，如果文件的目标是在公司位于不同城市甚至国家的另一个分支机构工作的员工，那么文件将被转发到外部邮政服务，如联合包裹，该服务将把文件运送到目标位置，再一次，内部邮政服务将从该位置接管并将其交付给收件人。

当在容器中运行的应用服务之间路由数据包时，也会发生类似的事情。源容器和目标容器可以位于同一个集群节点上，这对应于两个员工在同一栋大楼中工作的情况。目标容器可以在不同的集群节点上运行，这对应于两个员工在同一街区的不同建筑中工作的情况。最后，第三种情况是数据包来自集群外部，必须路由到在集群内部运行的目标容器。

所有这些情况以及更多的情况都必须由指挥者来处理。

# 负载平衡

在高度可用的分布式应用中，所有组件都必须是冗余的。这意味着每个应用服务都必须在多个实例中运行，这样，如果一个实例出现故障，服务作为一个整体仍然可以运行。

为了确保一个服务的所有实例都在工作，而不仅仅是闲坐着，必须确保服务请求被平均分配给所有实例。这种在服务实例之间分配工作负载的过程称为**负载平衡**。对于如何分配工作负载，存在各种算法。通常情况下，负载平衡器使用所谓的循环算法工作，该算法确保使用循环算法将工作负载平均分配给实例。

我们再次期望编排者处理从一个服务到另一个服务或者从外部资源到内部服务的负载平衡请求。

# 缩放比例

当在一个由 orchestrator 管理的集群中运行我们的容器化、分布式应用时，我们也希望有一种简单的方法来处理预期或意外增加的工作负载。为了处理增加的工作负载，我们通常只安排遇到这种增加负载的服务的额外实例。然后，负载平衡器将被自动配置为将工作负载分布在更多可用的目标实例上。

但是在现实场景中，工作负载会随着时间的推移而变化。如果我们看看像亚马逊这样的购物网站，它可能在晚上高峰时段负载很高，那时每个人都在家在线购物；在黑色星期五等特殊日子，它可能会承受极端负荷；并且它在清晨可能经历很少的交通流量。因此，服务不仅需要能够向上扩展，还需要在工作负载下降时能够向下扩展。

我们还期望编排者在扩展或缩减时以有意义的方式分发服务实例。将服务的所有实例安排在同一个集群节点上是不明智的，因为如果该节点宕机，整个服务都会宕机。负责放置容器的编排器的调度器还需要考虑不要将所有实例放置在同一计算机机架中，因为如果机架的电源出现故障，整个服务也会受到影响。此外，关键服务的服务实例甚至应该分布在数据中心，以避免中断。所有这些决定以及更多的决定都是指挥者的责任。

# 自愈

如今，管弦乐队非常老练，可以为我们维持一个健康的系统做很多事情。编排器监控集群中运行的所有容器，并自动用新实例替换崩溃或无响应的容器。协调器监控集群节点的健康状况，如果节点变得不健康或出现故障，则将其从调度器循环中移除。位于这些节点上的工作负载会自动重新调度到不同的可用节点。

指挥者监控当前状态并自动修复损坏或协调期望状态的所有这些活动导致所谓的**自愈**系统。在大多数情况下，我们不必积极参与并修复损坏。指挥者会自动为我们做这件事。

但是有些情况，没有我们的帮助，指挥者是无法处理的。想象一下，我们有一个服务实例在一个容器中运行。容器已启动并运行，从外部看，看起来非常健康。但是里面的应用处于不健康的状态。应用没有崩溃，只是不再能像设计的那样工作了。没有我们的暗示，指挥者怎么可能知道这件事？不行！处于不健康或无效的状态对每个应用服务来说意味着完全不同的东西。换句话说，健康状况取决于服务。只有服务的作者或其运营商知道健康在服务的上下文中意味着什么。

现在，编排器定义了接缝或探针，应用服务可以通过这些接缝或探针在什么状态下与编排器进行通信。存在两种基本类型的探针:

*   服务可以告诉指挥者它是否健康
*   该服务可以告诉指挥者它已经准备好或者暂时不可用

服务如何确定上述答案完全取决于服务。编排者只定义它将如何询问，例如，通过 HTTP GET 请求，或者它期望什么类型的回答，例如，OK 或 NOT OK。

如果我们的服务实现逻辑来回答前面的健康或可用性问题，那么我们就有了一个真正的自我修复系统，因为编排者可以杀死不健康的服务实例，并用新的健康服务实例替换它们，并且它可以将暂时不可用的服务实例从负载平衡器的循环中移除。

# 零停机部署

如今，证明一个需要更新的任务关键型应用完全宕机越来越难。这不仅意味着错失机会，还可能导致公司声誉受损。使用该应用的客户只是不准备再接受这样的不便，并且会很快离开。此外，我们的发布周期越来越短。在过去，我们每年会发布一到两个新版本，现在，许多公司每周更新应用多次，甚至每天更新多次。

这个问题的解决方案是提出一个零宕机应用更新策略。编排者需要能够批量更新单个应用服务。这也叫**滚动更新**。在任何给定时间，给定服务的实例总数中只有一个或几个被删除，并被新版本的服务替换。只有当新实例可以运行，并且不会产生任何意外错误或显示任何不良行为时，才会更新下一批实例。重复这一过程，直到所有实例都被新版本替换。如果由于某种原因，更新失败了，我们希望编排器自动将更新的实例回滚到它们以前的版本。

其他可能的零宕机部署是所谓的金丝雀版本和蓝绿色部署。在这两种情况下，服务的新版本将与当前的活动版本并行安装。但最初，新版本只能在内部访问。然后，运营部门可以对新版本进行冒烟测试，当新版本运行良好时，在蓝绿色部署的情况下，路由器会从当前的蓝色版本切换到新的绿色版本。一段时间以来，新的绿色版本的服务受到密切监控，如果一切正常，旧的蓝色版本可以退役。另一方面，如果新的绿色版本不像预期的那样工作，那么只需要将路由器设置回旧的蓝色版本，以实现完全回滚。

在金丝雀版本的情况下，路由器的配置方式是，它通过新版本的服务输送一小部分流量，比如 1%，而 99%的流量仍然通过旧版本路由。新版本的行为受到密切监控，并与旧版本的行为进行比较。如果一切看起来都很好，那么通过新服务传输的流量百分比就会略有增加。重复这个过程，直到 100%的流量通过新服务路由。如果新服务已经运行了一段时间，并且一切看起来都很好，那么旧服务可以退出使用。

大多数编排者至少支持开箱即用的滚动更新类型的零宕机部署。蓝绿色和淡黄色版本通常很容易实现。

# 亲和力和位置感知

有时，某些应用服务需要在其运行的节点上提供专用硬件。例如，输入/输出绑定服务要求集群节点连接高性能**固态硬盘** ( **固态硬盘**)，或者某些服务要求**加速处理单元** ( **APU** )。编排器允许我们定义每个应用服务的节点关联性。然后，编排者将确保其调度器只调度满足所需标准的集群节点上的容器。

应避免定义与特定节点的亲缘关系；这将引入单点故障，从而损害高可用性。始终将一组多个群集节点定义为应用服务的目标。

一些编排引擎也支持所谓的**位置感知**或**地理感知**。这意味着您可以请求编排者在一组不同的位置上平均分布服务的实例。例如，可以用可能的值`west`、`center`和`east`来定义标签`datacenter`，并将该标签应用于具有对应于相应节点所在地理区域的值的所有集群节点。然后，指示指挥者使用这个标签来对某个应用服务进行地理感知。在这种情况下，如果一个人请求九个服务副本，编排者将确保三个实例被部署到三个数据中心(西区、中区和东区)的每个节点上。

地理认知甚至可以分级定义；例如，可以将数据中心作为顶级鉴别器，然后是可用性区域，然后是服务器机架。

地理感知或位置感知用于降低因电源故障或数据中心停机而导致的停机概率。如果应用实例分布在服务器机架、可用性区域，甚至数据中心，那么所有的事情都不可能一下子发生。一个区域将始终可用。

# 安全

如今，信息技术中的安全性是一个非常热门的话题。网络战正处于历史最高水平。大多数高知名度的公司都是黑客攻击的受害者，造成了非常昂贵的后果。每个**首席信息官** ( **CIO** )或**首席技术官** ( **CTO** )最可怕的噩梦之一就是早上醒来，在新闻中听到他们的公司成为黑客攻击的受害者，敏感信息被窃取或泄露。

为了应对这些安全威胁，我们需要建立一个安全的软件供应链，并深入实现安全防御。让我们看看企业级指挥者可以完成的一些任务。

# 安全通信和加密节点身份

首先，我们希望确保由编排者管理的集群是安全的。只有受信任的节点才能加入群集。加入集群的每个节点都获得一个加密节点标识，节点之间的所有通信都必须加密。为此，节点可以使用**互传输层安全** ( **MTLS** )。要相互验证集群的节点，需要使用证书。这些证书会定期或应请求自动轮换，以在证书泄露时保护系统。

集群中发生的通信可以分为三种类型。一个是关于通信飞机。有**管理**、**控制**、**数据**飞机:

*   集群管理器或主机使用管理平面来例如调度服务实例、执行运行状况检查，或者创建和修改集群中的任何其他资源，例如数据卷、机密或网络。
*   控制平面用于在集群的所有节点之间交换重要的状态信息。例如，这种信息用于更新集群上用于路由目的的本地 IP 表。
*   数据平面是应用服务相互通信和交换数据的地方。

通常，指挥人员主要关心管理和控制平面的安全。确保数据平面的安全是留给用户的，然而指挥者可以促进这项任务。

# 安全网络和网络策略

运行应用服务时，并非每个服务都需要与群集中的其他服务进行通信。因此，我们希望能够相互沙箱化服务，并且只在绝对需要相互通信的同一个网络沙箱中运行这些服务。所有其他服务和来自群集外部的所有网络流量都不应有可能访问沙盒服务。

这种基于网络的沙箱至少有两种方式。我们可以使用**软件定义的网络** ( **SDN** )来对应用服务进行分组，或者我们可以拥有一个平面网络，并使用网络策略来控制哪些人可以访问特定的服务或服务组，哪些人不可以。

# 基于角色的访问控制(RBAC)

除了安全之外，为了使 it 企业做好准备，协调者必须完成的最重要的任务之一是提供对集群及其资源的基于角色的访问。RBAC 定义了系统的主体、用户或用户组如何组织成团队等等，来访问和操作系统。它确保未经授权的人员不会对系统造成任何伤害，也不会看到系统中任何他们不应该知道或看到的可用资源。

A typical enterprise might have user groups such as Development, QA, and Prod, and each of those groups can have one to many users associated with it. John Doe, the developer, is a member of the Development group and, as such, can access resources dedicated to the development team, but he cannot access, for example, the resources of the Prod team, of which Ann Harbor is a member. She, in turn, cannot interfere with the Development team's resources.

实现 RBAC 的一种方式是通过**授权**的定义。授权是主体、角色和资源集合之间的关联。这里，角色由一组对资源的访问权限组成。这些权限可以是创建、停止、删除、列出或查看容器；部署新的应用服务；列出群集节点或查看群集节点的详细信息；还有更多。

资源集合是群集的一组逻辑相关的资源，如应用服务、机密、数据卷或容器。

# 机密

在日常生活中，我们有很多机密。机密是不打算公开的信息，比如你用来访问网上银行账户的用户名和密码组合，或者你手机或健身房储物柜的密码。

写软件的时候，我们也经常需要用到机密。例如，我们需要一些证书来用我们想要访问的一些外部服务来验证我们的应用服务，或者我们需要一个令牌来在访问一些其他 API 时验证和授权我们的服务。过去，为了方便起见，开发人员只是将这些值硬编码或者以明文形式放在一些外部配置文件中。在那里，这种非常敏感的信息已经被广大的观众接触到，而实际上他们根本没有机会看到这些机密。

幸运的是，如今，管弦乐队提供了所谓的机密，以高度安全的方式处理如此敏感的信息。机密可以由授权或信任的人员创建。这些机密的值然后被加密并存储在高度可用的集群状态数据库中。这些机密，因为被加密了，现在在休息时是安全的。一旦授权的应用服务请求了一个机密，该机密只被转发给实际运行该特定服务实例的集群节点，并且该机密值从不存储在该节点上，而是装入基于`tmpfs`内存的卷中的容器中。只有在相应的容器中，机密值才以明文形式提供。

我们已经提到机密是安全的。一旦服务请求它们，集群管理器或主机就解密该机密，并通过线路将其发送到目标节点。*那么，在运输过程中保持安全的机密呢？*好的，我们之前了解到集群节点使用 MTLS 进行通信，因此虽然以明文形式传输，但该机密仍然是安全的，因为数据包将由 MTLS 加密。因此，机密在休息和运输中是安全的。只有被授权使用机密的服务才能访问这些机密值。

# 内容信任

为了增加安全性，我们希望确保只有受信任的映像才能在我们的生产集群中运行。一些编排者允许我们配置一个集群，这样它就只能运行签名映像。内容信任和映像签名都是为了确保映像的作者是我们期望的作者，即我们信任的开发人员，或者更好的是，我们信任的 CI 服务器。此外，有了内容信任，我们希望保证我们获得的映像是新鲜的，而不是旧的，可能是脆弱的映像。最后，我们希望确保映像在传输过程中不会被恶意黑客破坏。后者常被称为**中间人** ( **MITM** )攻击。

通过在源位置对映像进行签名并在目标位置验证签名，我们可以保证我们想要运行的映像不会受到损害。

# 反向正常运行时间

在安全性方面，我想讨论的最后一点是反向正常运行时间。我们这样说是什么意思？假设您已经配置并保护了一个生产集群。在这个集群上，您正在运行公司的一些关键任务应用。现在，一名黑客设法在您的一个软件栈中发现了一个安全漏洞，并获得了对您的一个集群节点的根访问权限。光是这一点就已经够糟糕的了，但更糟糕的是，这个黑客现在可以掩盖他们在这个节点上的存在，毕竟他们是机器的根，然后用它作为基地来攻击集群的其他节点。

Root access in Linux or any Unix-type operating system means that one can do anything on this system. It is the highest level of access that someone can have. In Windows, the equivalent role is that of an Administrator.

但是*如果我们利用容器短暂和集群节点快速配置的事实，如果完全自动化，通常在几分钟内完成，会怎么样？*我们只需在某个正常运行时间(比如 1 天)后杀死每个集群节点。指挥者被指示排出该节点，然后将其从集群中排除。一旦节点脱离群集，它就会被拆除，并由新调配的节点替换。

这样，黑客就失去了他们的基础，问题也就消除了。虽然这个概念还没有广泛应用，但对我来说，这似乎是朝着提高安全性迈出的一大步，就我与从事该领域工作的工程师讨论过的情况而言，实现起来并不困难。

# 反省

到目前为止，我们已经讨论了许多由指挥者负责的任务，并且它可以以完全自主的方式执行。但是也需要人工操作员能够看到和分析集群上当前运行的内容，以及各个应用的状态或运行状况。对于这一切，我们需要自省的可能性。指挥者需要以一种易于消费和理解的方式展现关键信息。

编排者应该从所有集群节点收集系统度量，并使操作员可以访问它。指标包括 CPU、内存和磁盘使用情况、网络带宽消耗等。这些信息应该很容易在每个节点的基础上获得，也可以以聚合的形式获得。

我们还希望编排者允许我们访问由服务实例或容器生成的日志。更重要的是，如果我们有正确的授权，指挥者应该为我们提供对每个容器的访问。有了`exec`对容器的访问，人们就可以调试行为不当的容器。

在高度分布式的应用中，对应用的每个请求都要经过许多服务，直到被完全处理，跟踪请求是一项非常重要的任务。理想情况下，指挥者支持我们实现跟踪策略，或者给我们一些好的指导方针。

最后，当操作人员使用所有收集的指标、日志和跟踪信息的图形表示时，他们可以最好地监控系统。这里，我们谈论的是仪表板。每一个优秀的指挥者都应该至少提供一些基本的仪表板，用图形表示最关键的系统参数。

但是人类操作员并不关心内省。我们还需要能够将外部系统与编排者连接起来，以使用这些信息。需要有一个可用的应用编程接口，外部系统可以通过该接口访问集群状态、指标和日志等数据，并使用这些信息做出自动决策，例如创建寻呼机或电话警报、发送电子邮件，或者在系统超过某些阈值时触发警报。

# 流行管弦乐队概述

在撰写本文时，有许多编排引擎在使用。但是有几个明显的赢家。排名第一的位置显然由至高无上的Kubernetes斯占据。排在第二位的是 Docker 自己的 SwarmKit，其次是 Apache Mesos、AWS **弹性容器服务** ( **ECS** )或微软 **Azure 容器服务** ( **ACS** )等其他公司。

# 忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈忽必烈

Kubernetes 最初由谷歌设计，后来捐赠给**云原生计算基金会** ( **CNCF** )。Kubernetes 是以谷歌专有的博格系统为模型的，该系统多年来一直在超大规模运行容器。Kubernetes 是谷歌试图回到画板上，完全重新开始，设计一个系统，将博格学到的所有经验都纳入其中。

与博格相反，博格是专有技术，库本内斯很早就开源了。这是谷歌非常明智的选择，因为它吸引了大量来自公司外部的贡献者，而且仅在几年时间里，围绕Kubernetes进化出了一个更庞大的生态系统。可以理直气壮地说，Kubernetes 是容器编排领域的社区宠儿。没有任何其他的编导能够制造如此多的宣传，吸引如此多的有才华的人愿意以一种有意义的方式作为贡献者或早期采用者为项目的成功做出贡献。

在这方面，容器编排空间中的 Kubernetes 在我看来非常像 Linux 在服务器操作系统空间中的样子。Linux 已经成为服务器操作系统事实上的标准。所有相关公司，如微软、IBM、亚马逊、红帽，甚至 Docker，都已经接受了 Kubernetes。

有一件事是不可否认的:Kubernetes 从一开始就是为了巨大的可扩展性而设计的。毕竟，它的设计是考虑到谷歌博格的。

有人可能会对 Kubernetes 提出反对，因为它的设置和管理很复杂，至少在撰写本文时是这样。对于新来者来说，有一个很大的障碍需要克服。第一步是陡峭的。但是一旦一个人和这个管弦乐队合作了一段时间，这一切都是有意义的。整体设计经过深思熟虑，执行得非常好。

在 Kubernetes 的最新版本 1.10 中，其**通用可用性** ( **GA** )是在 2018 年 3 月发布的，与 Docker Swarm 等其他管弦乐队相比，最初的大多数缺点都已消除。例如，安全性和保密性现在不仅是事后的想法，而且是系统不可分割的一部分。

新功能以惊人的速度实现。新版本大约每 3 个月发布一次，更准确地说，大约每 100 天发布一次。大多数新功能都是由需求驱动的，也就是说，使用 Kubernetes 来编排其任务关键型应用的公司可以表达他们的需求。这使得 Kubernetes 企业做好了准备。如果认为这个协调者只针对初创企业，而不是规避风险的企业，那就错了。情况恰恰相反。我的这种说法是基于什么？好吧，我的说法是有道理的，因为微软、Docker 和红帽等公司的客户大多是大企业，它们已经完全接受了 Kubernetes，并为其提供企业级支持，如果它被使用并集成到它们的企业产品中。

Kubernetes 同时支持 Linux 和 Windows 容器。

# Docker群

众所周知，Docker 普及并商品化了软件容器。Docker 没有发明容器，但是标准化了它们，并使它们广泛可用，尤其是通过提供免费的映像注册 Docker Hub。最初，Docker 主要关注开发人员和开发生命周期。但是很快开始使用和喜爱容器的公司也希望使用容器，不仅仅是在新应用的开发或测试过程中，还希望在生产中运行这些应用。

最初，Docker 在这个领域没有什么可提供的，所以其他公司跳进了这个真空，向用户提供帮助。但没过多久，Docker 意识到对一个简单而强大的管弦乐队有着巨大的需求。Docker 的第一次尝试是一款名为经典 Swarm 的产品。这是一个独立的产品，使用户能够创建一个 Docker 主机集群，该集群可用于以高可用性和自我修复的方式运行和扩展他们的容器化应用。

然而，建立一个经典的 Docker Swarm 是困难的。涉及到许多复杂的手工步骤。客户喜欢这款产品，但却难以应对其复杂性。所以 Docker 决定可以做得更好。它又回到了画板上，想出了 SwarmKit。SwarmKit 是在西雅图的 DockerCon 2016 上推出的，是 Docker 引擎最新版本的一个组成部分。是的，你说得对，SwarmKit 过去是，现在仍然是 Docker 引擎不可或缺的一部分。因此，如果你安装了一个 Docker 主机，你会自动拥有一个可用的 SwarmKit。

SwarmKit 的设计考虑了简单性和安全性。口头禅过去是，现在仍然是，建立一个群体几乎是微不足道的，群体必须是高度安全的开箱即用。Docker Swarm 基于最小特权的假设运行。

安装一个完整的、高可用性的 Docker Swarm 实际上很简单，首先在集群中的第一个节点上安装一个`docker swarm init`，它成为所谓的领导者，然后在所有其他节点上安装一个`docker swarm join <join-token>`。`join-token`由领导在初始化时生成。在最多有 10 个节点的 Swarm 上，整个过程不到 5 分钟。如果它是自动化的，它需要更少的时间。

正如我已经提到的，当 Docker 设计和开发 SwarmKit 时，安全性是必备的。容器通过依赖 Linux 内核命名空间和 cgroups 以及 Linux 系统调用白名单(seccomp)和对 Linux 功能的支持以及 **Linux 安全模块** ( **LSM** )来提供安全性。现在，除此之外，SwarmKit 还增加了 MTLS 和在静止和运输过程中加密的机密。此外，swarm 定义了所谓的**容器网络模型** ( **CNM** )，它允许 sdn 为在 Swarm 上运行的应用服务提供沙箱。

Docker SwarmKit 同时支持 Linux 和 Windows 容器。

# 阿帕奇 Mesos 和马拉松

Apache **Mesos** 是一个开源项目，最初是为了让一个服务器或节点集群从外部看起来像一个大服务器而设计的。Mesos 是使计算机集群管理变得简单的软件。Mesos 的用户不应该关心单个服务器，而应该假设他们有一个巨大的资源池可以支配，这相当于集群中所有节点的所有资源的总和。

用 IT 术语来说，Mesos 已经相当古老了，至少与其他管弦乐队相比是这样。它于 2009 年首次公开亮相。但是在那个时候，当然，它不是为了运行容器而设计的，因为 Docker 甚至还不存在。与 Docker 对容器所做的类似，Mesos 使用 Linux cgroups 来隔离资源，如 CPU、内存或单个应用或服务的磁盘 I/O。

Mesos 实际上是建立在它之上的其他有趣服务的底层基础设施。具体从容器的角度来看，**马拉松**很重要。马拉松是一个运行在 Mesos 之上的容器指挥器，能够扩展到数千个节点。

马拉松支持多个容器运行时，例如 Docker 或它自己的 Mesos 容器。它不仅支持无状态应用服务，还支持有状态应用服务，例如 PostgreSQL 或 MongoDB 等数据库。与 Kubernetes 和 Docker SwarmKit 类似，它支持本章前面描述的许多功能，例如高可用性、运行状况检查、服务发现、负载平衡和位置感知，仅举几个最重要的例子。

虽然 Mesos 和在一定程度上马拉松是相当成熟的项目，但它们的覆盖面相对有限。它似乎在大数据领域最受欢迎，也就是说，运行数据处理服务，如 Spark 或 Hadoop。

# 亚马逊 ECS

如果您正在寻找一个简单的管弦乐队，并且已经大量购买了 AWS 生态系统，那么亚马逊的 ECS 可能是您的正确选择。重要的是要指出 ECS 的一个非常重要的限制:如果你购买了这个容器编排器，那么你就把自己锁在了 AWS 中。您将无法轻松地将运行在 ECS 上的应用移植到另一个平台或云。

亚马逊将其 ECS 服务宣传为一种高度可扩展、快速的容器管理服务，使其易于在集群上运行、停止和管理 Docker 容器。除了运行容器之外，ECS 还允许从容器内部运行的应用服务直接访问许多其他 AWS 服务。这种与许多流行的 AWS 服务紧密而无缝的集成使 ECS 对那些正在寻找一种简单的方法使其容器化应用在健壮且高度可扩展的环境中启动和运行的用户来说极具吸引力。亚马逊还提供自己的私人映像注册。

有了 AWS ECS，您可以使用 Fargate 让它完全管理底层基础架构，这样您就可以专注于部署容器化的应用，而不必关心如何创建和管理节点集群。ECS 同时支持 Linux 和 Windows 容器。

总之，ECS 使用简单，可扩展性高，并且与其他流行的 AWS 服务很好地集成在一起，但是它不如 Kubernetes 或 Docker SwarmKit 那样强大，并且它只在亚马逊 AWS 上可用。

# 微软 ACS

类似于我们所说的 ECS，我们也可以对微软的 ACS 提出同样的要求。这是一个简单的容器编排服务，如果您已经在 Azure 生态系统中进行了大量投资，那么它就有意义了。我应该说和我为亚马逊 ECS 指出的一样:如果你买入 ACS，那么你就把自己锁定在微软的产品上。将您的容器化应用从 ACS 移动到任何其他平台或云都不容易。

ACS 是微软的容器服务，支持多个管弦乐队，如 Kubernetes、Docker Swarm 和 Mesos DC/OS。随着 Kubernetes 变得越来越受欢迎，微软的重心显然已经转移到了那个管弦乐队。微软甚至对其服务进行了重新命名，称之为“天青 Kubernetes 服务” ( **AKS** )将重点放在了 Kubernetes 上。

AKS 为您管理 Azure 中托管的 Kubernetes 或 Docker Swarm 或 DC/OS 环境，因此您可以专注于想要部署的应用，而不必关心基础架构的配置。用微软自己的话说，它声称:

AKS makes it quick and easy to deploy and manage containerized applications without container orchestration expertise. It also eliminates the burden of ongoing operations and maintenance by provisioning, upgrading, and scaling resources on demand, without taking your applications offline.

# 摘要

本章演示了为什么首先需要管弦乐队，以及他们在概念上是如何工作的。它指出了在写作时哪些管弦乐队是最突出的，并讨论了各种管弦乐队之间的主要共性和差异。

下一章将介绍 Docker 的原生管弦乐手，名为 SwarmKit。它将详细介绍 SwarmKit 用来在内部集群或云中部署和运行分布式、弹性、健壮和高可用性应用的所有概念和对象。

# 问题

回答以下问题来评估您的学习进度:

1.  为什么我们需要一个管弦乐队？列举两到三个理由。
2.  说出指挥者的三到四个典型职责。
3.  说出至少两个容器指挥者，以及他们背后的主要赞助者。

# 进一步阅读

以下链接提供了对编排相关主题的一些更深入的了解:

*   *库本内斯-https://kubernetes.io/*生产级流程编排
*   *https://docs.docker.com/engine/swarm/*Docker群模式概述
*   *中间层-http://bit.ly/2GMpko3*的容器编排服务
*   *容器和编排在* [和](https://bit.ly/2npjrEl)http://bit.ly/2DFoQgx 解释