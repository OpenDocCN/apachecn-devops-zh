# 三、收集和查询指标并发送警报

Insufficient facts always invite danger.

- *史巴克*

到目前为止，我们探索了如何利用 Kubernetes 的一些核心特性。我们使用了水平自动缩放器和集群自动缩放器。前者依赖于度量服务器，而后者不是基于度量，而是基于调度程序无法将 Pods 放置在现有集群容量内。尽管度量服务器确实提供了一些基本的度量，但我们迫切需要更多。

我们必须能够监控我们的集群，而度量服务器是不够的。它包含有限数量的度量，它使它们保持很短的时间，并且除了最简单的查询之外，它不允许我们执行任何事情。我不能说，如果我们只依赖于 Metrics Server，我们就是盲目的，而是严重受损。在不增加我们收集的指标数量及其保留率的情况下，我们只能对 Kubernetes 集群中的情况略知一二。

能够获取和存储指标本身并不是目标。我们还需要能够查询它们来寻找问题的原因。为此，我们需要度量来“丰富”信息，并且我们需要一种强大的查询语言。

最后，如果不能在第一时间得到有问题的通知，那么能够找到问题的原因是没有多大价值的。这意味着我们需要一个系统，允许我们定义警报，当达到特定阈值时，将向我们发送通知，或者在适当的时候，将通知发送到系统的其他部分，这些部分可以自动执行补救问题的步骤。

如果我们做到了这一点，我们将更接近拥有一个不仅能自我康复(Kubernetes 已经做到了)而且能对变化的条件做出反应的自适应系统。我们可能会走得更远，试图预测“坏事”将在未来发生，并在它们出现之前积极主动地解决它们。

总而言之，我们需要一个工具，或者一套工具，让我们能够获取和存储“丰富”的指标，让我们能够查询它们，并在问题发生时通知我们，或者更好的是，在问题即将发生时通知我们。

在本章中，我们可能无法构建自适应系统，但我们可以尝试创建一个基础。但是，首先，我们需要一个集群，允许我们“玩”一些新的工具和概念。

# 创建集群

我们将继续使用来自`vfarcic/k8s-specs`([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))存储库的定义。为了安全起见，我们先拉最新版本。

All the commands from this chapter are available in the `03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9)) Gist.

```
 1  cd k8s-specs
 2
 3  git pull
```

在本章中，我们将需要一些以前不是需求的东西，即使您可能已经使用过它们。

我们将开始使用用户界面，因此我们将需要 NGINX 入口控制器来路由来自集群外部的流量。我们还需要环境变量`LB_IP`和我们可以通过其访问工作节点的 IP。我们将使用它来配置一些入口资源。

用于测试本章示例的 Gists 如下。请照原样使用它们，或者作为灵感来创建您自己的集群，或者确认您已经拥有的集群是否符合要求。由于新的要求(入口和`LB_IP`)，所有集群设置指南都是新的。

A note to Docker for Desktop users
You'll notice `LB_IP=[...]` command at the end of the Gist. You'll have to replace `[...]` with the IP of your cluster. Probably the easiest way to find it is through the `ifconfig` command. Just remember that it cannot be `localhost`, but the IP of your laptop (for example, `192.168.0.152)`. A note to minikube and Docker for Desktop users
We have to increase memory to 3 GB. Please have that in mind in case you were planning only to skim through the Gist that matches your Kubernetes flavor.

要点如下。

*   `gke-monitor.sh` : **具有 3 个 n1-standard-1 工作节点的 GKE** 、 **nginx Ingress** 、 **tiller** 和存储在环境变量**LB _ IP**(https://gist . github . com/vfarcic/10e 14 bfbec 466347 D70 d 11 a 78 Fe 7 ee C4)中的集群 IP。
*   `eks-monitor.sh` : **具有 3 个 T2 .小型工作节点的 EKS** 、 **nginx Ingress** 、 **tiller** 、 **Metrics Server** 以及存储在环境变量 **LB_IP** 中的集群 IP(https://gist . github . com/vfarcic/211 F8 DBE 204131 f 8109 f 417605 dbdd D5)。
*   `aks-monitor.sh` : **带有 3 个 Standard_B2s 工作节点的 AKS** 、 **nginx Ingress** 和 **tiller** ，以及存储在环境变量 **LB_IP** 中的集群 IP。
*   `docker-monitor.sh` : **Docker for Desktop** 带有 **2 个 CPU**、 **3 GB RAM** 、**nginx intrusion**、 **tiller** 、 **Metrics Server** ，以及存储在环境变量**LB _ IP**([https://gist . github . com/vfarcic/4d 9ab 04058 cf 00 B9 DD 0 faac 11 BDA 8](https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13)
*   `minikube-monitor.sh` : **带 **2 个 CPU**、 **3 GB RAM** 、**入口**、**存储提供程序**、**默认存储类**和**指标-服务器**插件已启用、**分蘖**和存储在环境变量 **LB_IP** 中的集群 IP([https://gist。](https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248)**

现在我们有了一个集群，我们需要选择工具来完成我们的目标。

# 选择用于存储和查询指标及警报的工具

**水平 Pods 自动缩放器** ( **HPA** )和**集群自动缩放器** ( **CA** )提供了缩放我们的 Pods 和集群的基本但非常初级的机制。

虽然它们确实可以很好地扩展，但它们不能解决我们在出现问题时需要得到提醒的问题，也不能提供找到问题原因所需的足够信息。我们需要用额外的工具来扩展我们的设置，这些工具将允许我们存储和查询指标，并在出现问题时接收通知。

如果我们专注于可以自己安装和管理的工具，那么使用什么就没有什么疑问了。如果我们看一下*云原生计算基金会(CNCF)* 项目([https://www.cncf.io/projects/](https://www.cncf.io/projects/))的名单，到目前为止(2018 年 10 月)只有两个毕业。那是*Kubernetes*和*普罗米修斯*([https://prometheus.io/](https://prometheus.io/))。假设我们正在寻找一个工具，允许我们存储和查询指标，普罗米修斯满足了这一需求，选择是直接的。这并不是说没有其他类似的工具值得考虑。有，但都是基于服务的。我们可能会在以后探索它们，但目前，我们专注于我们可以在集群中运行的那些。因此，我们将把普罗米修斯加入到这个组合中，并尝试回答一个简单的问题。什么是普罗米修斯？

Prometheus is a database (of sorts) designed to fetch (pull) and store highly dimensional time series data.

时间序列由一个度量名称和一组键值对来标识。数据存储在内存和磁盘上。前者允许快速检索信息，而后者是为了容错而存在的。

普罗米修斯的查询语言使我们能够轻松找到既可用于图表，更重要的是，可用于警报的数据。它并不试图提供“伟大”的可视化体验。为此，它与*格拉夫纳*([https://grafana.com/](https://grafana.com/))集成在一起。

与大多数其他类似工具不同，我们不会将数据推送到普罗米修斯。或者，更准确地说，这不是获取指标的常见方式。相反，普罗米修斯是一个基于拉的系统，定期从出口商那里获取指标。我们可以使用许多第三方出口商。但是，在我们的例子中，最重要的出口商被烤成了 Kubernetes。普罗米修斯可以从导出器中提取数据，导出器从库贝应用编程接口中转换信息。通过它，我们可以得到(几乎)我们可能需要的一切。或者，至少这是大部分信息的来源。

最后，如果我们在出现问题时没有得到通知，那么在普罗米修斯中存储度量标准将没有多大用处。即使我们真的将普罗米修斯与格拉夫纳集成在一起，那也只会为我们提供仪表板。我认为你有比盯着彩色图表更好的事情要做。因此，我们需要一种方法将普罗米修斯的警报发送到，比如说，Slack。幸运的是，*警报管理器*([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))允许我们这样做。它是由同一个社区维护的独立应用。

我们将通过实践练习来了解所有这些部分是如何结合在一起的。因此，让我们开始安装普罗米修斯、警报器管理器和一些其他应用。

# 普罗米修斯和警报器管理员简介

我们将继续使用 Helm 作为安装机制的趋势。普罗米修斯的掌舵图被保留为官方图表之一。你可以在项目的*自述文件*中找到更多信息。如果您关注*配置部分*([https://github . com/helm/charts/tree/master/stable/Prometheus # Configuration](https://github.com/helm/charts/tree/master/stable/prometheus#configuration))中的变量，您会注意到我们可以调整的东西相当多。我们不会讨论所有的变量。你可以查看官方文件。相反，我们将从一个基本的设置开始，并随着需求的增加进行扩展。

让我们看一下我们将作为开始使用的变量。

```
 1  cat mon/prom-values-bare.yml
```

输出如下。

```
server:
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/ssl-redirect: "false"
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
  resources:
    limits:
      cpu: 100m
      memory: 1000Mi
    requests:
      cpu: 10m
      memory: 500Mi
alertmanager:
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/ssl-redirect: "false"
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 5m
      memory: 10Mi
kubeStateMetrics:
  resources:
    limits:
      cpu: 10m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 25Mi
nodeExporter:
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 5m
      memory: 10Mi
pushgateway:
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
        requests:
      cpu: 5m
      memory: 10Mi
```

我们现在所做的就是为我们将要安装的所有五个应用定义`resources`，以及启用带有一些注释的入口，这些注释将确保我们不会被重定向到 HTTPS 版本，因为我们没有我们的专用域的证书。我们将深入研究稍后安装的应用。现在，我们将定义普罗米修斯和警报管理器用户界面的地址。

```
 1  PROM_ADDR=mon.$LB_IP.nip.io
 2
 3  AM_ADDR=alertmanager.$LB_IP.nip.io
```

让我们安装图表。

```
 1  helm install stable/prometheus \
 2      --name prometheus \
 3      --namespace metrics \
 4      --version 7.1.3 \
 5      --set server.ingress.hosts={$PROM_ADDR} \
 6      --set alertmanager.ingress.hosts={$AM_ADDR} \
 7      -f mon/prom-values-bare.yml
```

我们刚刚执行的命令应该是不言自明的，所以我们将跳转到输出的相关部分。

```
...
RESOURCES:
==> v1beta1/DaemonSet
NAME                     DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE
prometheus-node-exporter 3       3       0     3          0         <none>        3s 
==> v1beta1/Deployment
NAME                          DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
prometheus-alertmanager       1       1       1          0         3s
prometheus-kube-state-metrics 1       1       1          0         3s
prometheus-pushgateway        1       1       1          0         3s
prometheus-server             1       1       1          0         3s
...
```

我们可以看到图表安装了一个 DeamonSet 和四个 Deployments。

DeamonSet 是节点导出器，它将在集群的每个节点上运行一个 Pod。它提供了普罗米修斯将提取的特定于节点的指标。第二个导出器(Kube 状态度量)作为单个副本部署运行。它从库贝应用编程接口获取数据，并将其转换为普罗米修斯友好的格式。这两者将提供我们所需的大部分指标。稍后，我们可能会选择与其他出口商一起扩大业务。目前，这两个指标以及直接从 Kube API 获取的指标应该提供比我们在单个章节中所能吸收的更多的指标。

进一步，我们得到了服务器，这是普罗米修斯本身。警报管理员会将警报转发到他们的目的地。最后，还有一个 Pushgateway，我们可能会在下面的章节中探讨。

在等待所有这些应用投入运行的同时，我们可能会探索它们之间的流程。

普罗米修斯服务器从出口商那里获取数据。在我们的例子中，这些是节点导出器和 Kube 状态度量。这些出口商的工作是从数据源获取数据，并将其转换为对普罗米修斯友好的格式。节点导出器从安装在节点上的`/proc`和`/sys`卷获取数据，而库贝状态度量从库贝应用编程接口获取数据。度量标准存储在普罗米修斯内部。

除了能够查询这些数据，我们还可以定义警报。当警报达到阈值时，它会被转发到充当十字路口的警报管理器。

根据其内部规则，它可以将这些警报进一步转发到不同的目的地，如 Slack、电子邮件和 HipChat(仅举几例)。

![](img/701f20e3-39b9-495f-b689-ccf64772ece1.png)

Figure 3-1: The flow of data to and from Prometheus (arrows indicate the direction)

到目前为止，普罗米修斯服务器可能已经推出。我们会确认以防万一。

```
 1  kubectl -n metrics \
 2      rollout status \
 3      deploy prometheus-server
```

让我们看看通过`prometheus-server`部署创建的 Pod 内部有什么。

```
 1  kubectl -n metrics \
 2      describe deployment \
 3      prometheus-server
```

输出限于相关部分，如下所示。

```
  Containers:
   prometheus-server-configmap-reload:
    Image: jimmidyson/configmap-reload:v0.2.2
    ...
   prometheus-server:
    Image: prom/prometheus:v2.4.2
    ...
```

除了基于`prom/prometheus`映像的容器，我们还从`jimmidyson/configmap-reload`中创建了另一个容器。后者的工作是每当我们更改存储在配置映射中的配置时重新加载普罗米修斯。

接下来，我们可能想看一下`prometheus-server`配置图，因为它存储了普罗米修斯需要的所有配置。

```
 1  kubectl -n metrics \
 2      describe cm prometheus-server
```

输出限于相关部分，如下所示。

```
...
Data
====
alerts:
----
{} 
prometheus.yml:
----
global:
  evaluation_interval: 1m
  scrape_interval: 1m
  scrape_timeout: 10s 
rule_files:
- /etc/config/rules
- /etc/config/alerts
scrape_configs:
- job_name: prometheus
  static_configs:
  - targets:
    - localhost:9090
- bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  job_name: kubernetes-apiservers
  kubernetes_sd_configs:
  - role: endpoints
  relabel_configs:
  - action: keep
    regex: default;kubernetes;https
    source_labels:
    - __meta_kubernetes_namespace
    - __meta_kubernetes_service_name
    - __meta_kubernetes_endpoint_port_name
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
...
```

我们可以看到`alerts`仍然是空的。我们很快就会改变。

再往下是`prometheus.yml`配置，`scrape_configs`占据了大部分空间。我们可以花一整章的时间来解释当前的配置以及修改的方法。我们不会那样做，因为你面前的配置近乎疯狂。这是一个很好的例子，说明了事物是如何变得比它应该变得更复杂的。在大多数情况下，你应该保持现状。如果你想摆弄它，请查阅官方文件。

接下来，我们将快速浏览一下普罗米修斯的屏幕。

A note to Windows users
Git Bash might not be able to use the `open` command. If that's the case, replace `open` with `echo`. As a result, you'll get the full address that should be opened directly in your browser of choice.

```
 1  open "http://$PROM_ADDR/config"
```

配置屏幕反映了我们已经在`prometheus-server`配置图中看到的相同信息，因此我们将继续。

接下来，让我们看看目标。

```
 1  open "http://$PROM_ADDR/targets"
```

该屏幕包含七个目标，每个目标提供不同的指标。普罗米修斯定期从这些目标中提取数据。

All the outputs and screenshots in this chapter are taken from AKS. You might see some differences depending on your Kubernetes flavor. You might notice that this chapter contains much more screenshots than any other. Even though it might look like there are too many, I wanted to make sure that you can compare your results with mine, since there will be inevitable differences that might sometimes look confusing if you do not have a reference (my screenshots).

![](img/f207a763-f021-4f45-966b-948bae855230.png)

Figure 3-2: Prometheus' targets screen A note to AKS users
The *kubernetes-apiservers* target might be red indicating that Prometheus cannot connect to it. That's OK since we won't use its metrics. A note to minikube users
The *kubernetes-service-endpoints* target might have a few sources in red. There's no reason for alarm. Those are not reachable, but that won't affect our exercises.

我们无法从屏幕上找到每个目标提供了什么。我们将尝试用普罗米修斯拉他们的同样方式询问出口商。

要做到这一点，我们需要找到我们可以联系出口商的服务。

```
 1  kubectl -n metrics get svc
```

AKS 的输出如下。

```
NAME                          TYPE      CLUSTER-IP    EXTERNAL-IP PORT(S)  AGE
prometheus-alertmanager       ClusterIP 10.23.245.165 <none>      80/TCP   41d
prometheus-kube-state-metrics ClusterIP None          <none>      80/TCP   41d
prometheus-node-exporter      ClusterIP None          <none>      9100/TCP 41d
prometheus-pushgateway        ClusterIP 10.23.244.47  <none>      9091/TCP 41d
prometheus-server             ClusterIP 10.23.241.182 <none>      80/TCP   41d
```

我们对`prometheus-kube-state-metrics`和`prometheus-node-exporter`感兴趣，因为它们提供了从我们将在本章中使用的出口商处获取数据的途径。

接下来，我们将创建一个临时 Pod，通过它我们可以访问这些服务背后的出口商提供的数据。

```
 1  kubectl -n metrics run -it test \
 2      --image=appropriate/curl \
 3      --restart=Never \
 4      --rm \
 5      -- prometheus-node-exporter:9100/metrics
```

我们基于`appropriate/curl`创建了一个新的 Pod。该映像的唯一目的是提供`curl`。我们指定`prometheus-node-exporter:9100/metrics`作为命令，相当于用那个地址运行`curl`。结果，输出了许多指标。它们都是相同的`key/value`格式，可选标签用花括号(`{`和`}`)括起来。在每个指标的顶部，都有一个`HELP`条目，解释其功能以及`TYPE`(例如`gauge`)。其中一个指标如下。

```
 1  # HELP node_memory_MemTotal_bytes Memory information field
    MemTotal_bytes.
 2  # TYPE node_memory_MemTotal_bytes gauge
 3  node_memory_MemTotal_bytes 3.878477824e+09
```

我们可以看到它提供`Memory information field MemTotal_bytes`，类型为`gauge`。`TYPE`下方是实际公制，带键(`node_memory_MemTotal_bytes`)和值`3.878477824e+09`。

大多数节点导出器指标都没有标签。所以，我们必须在`prometheus-kube-state-metrics`出口商中寻找一个例子。

```
 1  kubectl -n metrics run -it test \
 2      --image=appropriate/curl \
 3      --restart=Never \
 4      --rm \
 5      -- prometheus-kube-state-metrics:8080/metrics
```

如您所见，Kube 状态度量遵循与节点导出器相同的模式。主要的区别是，他们中的大多数确实有标签。一个例子如下。

```
 1  kube_deployment_created{deployment="prometheus-
    server",namespace="metrics"} 1.535566512e+09
```

该指标表示在`metrics`命名空间内创建部署`prometheus-server`的时间。

我将让您更详细地探索这些指标。我们很快会用到很多。

现在，请记住，通过结合来自节点导出器、Kube 状态度量和来自 Kubernetes 本身的度量，我们可以满足大多数需求。或者，更准确地说，它们提供了大多数基本和常见用例所需的数据。

接下来，我们将查看警报屏幕。

```
 1  open "http://$PROM_ADDR/alerts"
```

屏幕是空的。不要绝望。我们会多次回到那个屏幕。随着我们的进展，警报会越来越多。现在，请记住，您可以在那里找到您的警报。

最后，我们将打开图形屏幕。

```
 1  open "http://$PROM_ADDR/graph"
```

这是您花时间调试通过警报发现的问题的地方。

作为我们的第一个任务，我们将尝试检索关于我们的节点的信息。我们将使用`kube_node_info`所以让我们看看它的描述(帮助)和类型。

```
 1  kubectl -n metrics run -it test \
 2      --image=appropriate/curl \
 3      --restart=Never \
 4      --rm \
 5      -- prometheus-kube-state-metrics:8080/metrics \
 6      | grep "kube_node_info"
```

仅限于`HELP`和`TYPE`条目的输出如下。

```
 1  # HELP kube_node_info Information about a cluster node.
 2  # TYPE kube_node_info gauge
 3  ...
```

You are likely to see variations between your results and mine. That's normal since our clusters probably have different amounts of resources, my bandwidth might be different, and so on. In some cases, my alerts will fire, and yours won't, or the other way around. I'll do my best to explain my experience and provide screenshots that accompany them. You'll have to compare that with what you see on your screen.

现在，让我们尝试在普罗米修斯中使用这个度量。

请在表达式字段中键入以下查询。

```
 1  kube_node_info
```

单击执行按钮检索`kube_node_info`度量的值。

Unlike previous chapters, the Gist from this one (`03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9))) contains not only the commands but also Prometheus expressions. They are all commented (with `#`). If you're planning to copy and paste the expressions from the Gist, please exclude the comments. Each expression has `# Prometheus expression` comment on top to help you identify it. As an example, the one you just executed is written in the Gist as follows. `# Prometheus expression` `# kube_node_info`

如果你查看`kube_node_info`的`HELP`条目，你会发现它提供了`information about a cluster node`，并且是一个`gauge`。A **规**([https://prometheus.io/docs/concepts/metric_types/#gauge](https://prometheus.io/docs/concepts/metric_types/#gauge))是一个表示单个数值的公制，可以任意上下。

这对于有关节点的信息很有意义，因为它们的数量会随着时间的推移而增加或减少。

A Prometheus gauge is a metric that represents a single numerical value that can arbitrarily go up and down.

如果我们关注输出，您会注意到集群中的条目和工作节点一样多。值(`1`)在这个上下文中没有用。另一方面，标签可以提供一些有用的信息。比如我这个例子，操作系统(`os_image`)就是`Ubuntu 16.04.5 LTS`。通过这个例子，我们可以看到，我们不仅可以使用这些指标来计算值(例如，可用内存)，还可以了解我们系统的细节。

![](img/f2eea74f-64b6-4499-887d-fa8f42a957ef.png)

Figure 3-3: Prometheus' console output of the kube_node_info metric

让我们看看是否可以通过将该指标与普罗米修斯的一个函数相结合来获得更有意义的查询。我们将`count`集群中工作节点的数量。`count`是普罗米修斯的*聚合运算符*之一。

请执行下面的表达式。

```
 1  count(kube_node_info)
```

输出应该显示集群中工作节点的总数。就我而言(AKS)有`3`。乍一看，这可能没什么帮助。您可能认为，如果没有普罗米修斯，您应该知道集群中有多少节点。但这可能不是真的。其中一个节点可能出现故障，并且没有恢复。如果您在不扩展组的情况下运行集群，这一点尤其正确。或者集群自动缩放器增加或减少节点数量。一切都会随着时间的推移而改变，要么是因为失败，要么是因为人类的行为，要么是因为一个自我适应的系统。不管波动的原因是什么，当某件事达到阈值时，我们可能希望得到通知。我们将使用节点作为第一个例子。

我们的任务是定义一个警报，如果集群中的节点多于三个或少于一个，该警报将通知我们。我们会想象这些是我们的极限，我们想知道是由于故障还是集群自动缩放达到了下限还是上限。

我们将看看普罗米修斯图表价值的新定义。既然定义很大，而且会随着时间增长，从现在开始，我们只看区别。

```
 1  diff mon/prom-values-bare.yml \
 2      mon/prom-values-nodes.yml
```

输出如下。

```
> serverFiles:
>   alerts:
>     groups:
>     - name: nodes
>       rules:
>       - alert: TooManyNodes
>         expr: count(kube_node_info) > 3
>         for: 15m
>         labels:
>           severity: notify
>         annotations:
>           summary: Cluster increased
>           description: The number of the nodes in the cluster increased
>       - alert: TooFewNodes
>         expr: count(kube_node_info) < 1
>         for: 15m
>         labels:
>           severity: notify
>         annotations:
>           summary: Cluster decreased
>           description: The number of the nodes in the cluster decreased
```

我们增加了一个新条目`serverFiles.alerts`。如果您查看普罗米修斯的头盔文档，您会发现它允许我们定义警报(因此得名)。在其中，我们使用“标准”普罗米修斯语法来定义警报。

Please consult *Alerting Rules documentation* ([https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)) for more info about the syntax.

我们只定义了一组叫做`nodes`的规则。里面是两个`rules`。第一个(`TooManyNodes`)如果超过`3`节点`for`超过`15`分钟会通知我们。另一个(`TooFewNodes`)则会反其道而行之。它会告诉我们`15`分钟内是否没有节点(`<1`)。两个`rules`都有`labels`和`annotations`，目前仅用于信息目的。稍后，我们将看到它们的真正用法。

让我们升级普罗米修斯的图表，看看新警报的效果。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-nodes.yml
```

这将需要一些时间，直到新的配置被“发现”，普罗米修斯被重新加载。过一会儿，我们可以打开普罗米修斯警报屏幕，检查我们是否得到了第一个条目。

From now on, I won't comment (much) on the need to wait for a while until next config is propagated. If what you see on the screen does not coincide with what you're expecting, please wait for a while and refresh it.

```
 1  open "http://$PROM_ADDR/alerts"
```

您应该会看到两个警报。

两个警报都是绿色的，因为没有一个评估为`true`。根据您选择的 Kuberentes 风格，您要么只有一个节点(例如，Docker 用于 Desktop 和 minikube)，要么有三个节点(例如，GKE、EKS、AKS)。由于我们的警报正在检查我们的节点是少于一个，还是多于三个，因此无论您使用哪种 Kubernetes 风味，这两个条件都不满足。

If your cluster was not created through one of the Gists provided at the beginning of this chapter, then you might have more than three nodes in your cluster, and the alert will fire. If that's the case, I suggest you modify the `mon/prom-values-nodes.yml` file to adjust the threshold of the alert.

![](img/c8da3d50-5320-4ddc-b3ed-5175750546d8.png)

Figure 3-4: Prometheus' alerts screen

看到不活动的警报很无聊，所以我想给你看一个会触发(变成红色)的警报。为此，我们可以向集群中添加更多节点(除非您使用像 Docker for Desktop 和 minikube 这样的单节点集群)。但是，修改其中一个警报的表达式会更容易，所以这就是我们接下来要做的。

```
 1  diff mon/prom-values-nodes.yml \
 2      mon/prom-values-nodes-0.yml
```

输出如下。

```
57,58c57,58
< expr: count(kube_node_info) > 3
< for: 15m
---
> expr: count(kube_node_info) > 0
> for: 1m
66c66
< for: 15m
---
> for: 1m
```

新的定义改变了`TooManyNodes`警报的条件，即如果超过零个节点，则触发。我们还更改了`for`声明，这样我们就不需要在警报响起前等待`15`分钟。

让我们再次升级图表。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-nodes-0.yml
```

...我们将返回警报屏幕。

```
 1  open "http://$PROM_ADDR/alerts"
```

几分钟后(不要忘记刷新屏幕)，警报将切换到挂起状态，颜色将变为黄色。这意味着警报的条件得到满足(我们确实有零个以上的节点)，但是`for`时间段尚未到期。

等待一分钟(`for`周期的持续时间)并刷新屏幕。警报状态切换为开火，颜色变为红色。普罗米修斯发出了我们的第一次警报。

![](img/cb890924-63a6-49f3-953b-783f7495ba3b.png)

Figure 3-5: Prometheus' alerts screen with one of the alerts firing

警报发送到哪里了？普罗米修斯头盔显示器部署了警报器管理器，并预先配置了普罗米修斯向那里发送警报。我们来看看它的 UI。

```
 1  open "http://$AM_ADDR"
```

我们可以看到一个警报到达了警报管理器。如果我们点击`TooManyNodes`警报旁边的+ info 按钮，我们将看到注释(摘要和描述)以及标签(严重性)。

![](img/2de2ea86-984f-4529-ad8a-f54a3680104a.png)

Figure 3-6: Alertmanager UI with one of the alerts expanded

我们可能不会坐在警报管理器前等待问题出现。如果这是我们的目标，我们不妨等待普罗米修斯的警报。

显示警报确实不是我们拥有警报管理器的原因。它应该接收警报并进一步发送。它没有做任何这种事情，只是因为我们还没有定义它应该用来转发警报的规则。那是我们的下一个任务。

我们将看看普罗米修斯图表值的另一个更新。

```
 1  diff mon/prom-values-nodes-0.yml \
 2      mon/prom-values-nodes-am.yml
```

输出如下。

```
71a72,93
> alertmanagerFiles:
>   alertmanager.yml:
>     global: {}
>     route:
>       group_wait: 10s
>       group_interval: 5m
>       receiver: slack
>       repeat_interval: 3h
>       routes:
>       - receiver: slack
>         repeat_interval: 5d
>         match:
>           severity: notify
>           frequency: low
>     receivers:
>     - name: slack
>       slack_configs:
>       - api_url: "https://hooks.slack.com/services/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"
>         send_resolved: true
>         title: "{{ .CommonAnnotations.summary }}"
>         text: "{{ .CommonAnnotations.description }}"
>         title_link: http://my-prometheus.com/alerts
```

当我们应用这个定义时，我们将把`alertmanager.yml`文件添加到 Alertmanager 中。如果包含应用于调度警报的规则。`route`部分包含将应用于所有与`routes`不匹配的警报的一般规则。`group_wait`值使警报管理器等待`10`秒，以防来自同一组的其他警报到达。这样，我们将避免收到多个相同类型的警报。

当一个组的第一个警报被调度时，它将在发送来自同一组的下一批新警报之前使用`group_interval`字段的值(`5m`)。

`route`部分的`receiver`字段定义了警报的默认目的地。这些目的地在下面的`receivers`部分定义。在我们的例子中，我们默认向`slack`接收器发送警报。

`repeat_interval`(设置为`3h`)定义了如果警报管理器继续接收警报，将重新发送警报的时间段。

`routes`部分定义了具体的规则。只有当它们都不匹配时，才会使用上面`route`部分中的那些。`routes`部分继承了上面的属性，因此只有我们在这一部分定义的属性会改变。我们将继续发送匹配的`routes`到`slack`，唯一的变化是`repeat_interval`从`3h`增加到`5d`。

`routes`的关键部分是`match`段。它定义了用于决定警报是否匹配的过滤器。在我们的例子中，只有那些标签为`severity: notify`和`frequency: low`的才会被认为是匹配的。

总之，`severity`标签设置为`notify`、`frequency`设置为`low`的提醒将每五天重新发送一次。所有其他警报的频率为三小时。

警报管理器配置的最后一部分是`receivers`。我们只有一个名为`slack`的接收器。`name`下面是`slack_config`。它包含特定于 Slack 的配置。我们可以使用`hipchat_config`、`pagerduty_config`或任何其他支持的。即使我们的目的地不是其中之一，我们也可以返回`webhook_config`并向我们选择的工具的应用编程接口发送定制请求。

For the list of all the supported `receivers`, please consult *Alertmanager Configuration* page ([https://prometheus.io/docs/alerting/configuration/](https://prometheus.io/docs/alerting/configuration/)).

在`slack_configs`部分，我们有`api_url`，它包含来自 *devops20* 频道的一个房间的带有令牌的 Slack 地址。

For information how to general an incoming webhook address for your Slack channel, please visit the *Incoming Webhooks* page ([https://api.slack.com/incoming-webhooks](https://api.slack.com/incoming-webhooks)).

接下来是`send_resolved`旗。当设置为`true`时，警报管理器不仅会在触发警报时发送通知，还会在导致警报的问题得到解决时发送通知。

我们使用`summary`注释作为消息的`title`，使用`description`注释作为`text`。两者都使用*围棋模板*([https://golang.org/pkg/text/template/](https://golang.org/pkg/text/template/))。这些是我们在普罗米修斯的警告中定义的相同注释。

最后将`title_link`设置为`http://my-prometheus.com/alerts`。这确实不是你的普罗米修斯用户界面的地址，但是，由于我不能提前知道你的域名，我放了一个不存在的。请随意将`my-prometheus.com`更改为环境变量`$PROM_ADDR`的值。或者让它保持原样，知道如果你点击链接，它不会带你到你的普罗米修斯用户界面。

现在我们已经探索了 Alertmanager 配置，我们可以继续升级图表了。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-nodes-am.yml
```

几分钟后，警报器管理器将被重新配置，下次它收到普罗米修斯的警报时，它会将其发送给 Slack。我们可以通过访问`devops20.slack.com`工作区来确认。如果您尚未注册，请前往[slack.devops20toolkit.com](http://slack.devops20toolkit.com)。一旦您成为会员，我们可以访问`devops25-tests`频道。

```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```

你应该看到`Cluster increased`通知。如果你看到其他信息，不要感到困惑。你可能不是唯一一个做这本书练习的人。

![](img/d1936f65-8d48-452a-91f0-03da31c126ca.png)

Figure 3-7: Slack with an alert message received from Alertmanager Sometimes, for reasons I could not figure out, Slack receives empty notifications from Alertmanager. For now, I'm ignoring the issue out of laziness.

现在，我们已经完成了普罗米修斯和警报器管理器的基本用法，我们将从实践练习中休息一下，讨论我们可能想要使用的指标类型。

# 我们应该使用哪些度量类型？

如果这是你第一次使用普罗米修斯从库贝应用编程接口连接到指标，纯粹的数量可能是压倒性的。除此之外，考虑到该配置排除了 Kube API 提供的许多指标，我们可以通过额外的导出器进一步扩展范围。

虽然每种情况都不同，并且您可能需要特定于您的组织和体系结构的一些指标，但是我们应该遵循一些指导原则。在本节中，我们将讨论关键指标。一旦你通过几个例子理解了它们，你应该能够将它们的使用扩展到你的特定用例。

The four key metrics everyone should utilize are latency, traffic, errors, and saturation.

这四个指标被谷歌**站点可靠性工程师** ( **SREs** )倡导为跟踪系统性能和健康的最基本指标。

**延迟**表示服务响应请求所需的时间。重点不仅应该放在持续时间上，还应该区分成功请求的延迟和失败请求的延迟。

**流量**是对服务需求的衡量。一个例子是每秒的 HTTP 请求数。

**错误**通过失败请求的比率来衡量。大多数情况下，这些失败是显式的(例如，HTTP 500 错误)，但也可以是隐式的(例如，HTTP 200 响应的正文描述查询没有返回任何结果)。

**饱和度**可以用服务或系统的“充满度”来描述。一个典型的例子是缺少中央处理器，这会导致节流，从而降低应用的性能。

随着时间的推移，出现了不同的监测方法。例如，我们得到了 **USE** 方法，该方法规定对于每个资源，我们应该检查**利用率**、**饱和度**和**错误**。另一种是 **RED** 方法，将**率**、**误差**、**持续时间**定义为关键指标。这些和许多其他的在本质上是相似的，并且与 SREs 测量延迟、流量、错误和饱和度的需求没有显著的不同。

我们将逐一介绍 SREs 描述的四种测量类型，并提供几个示例。我们甚至可以用不一定符合这四个类别中任何一个的度量标准来扩展它们。首先是延迟。

# 延迟相关问题的警报

我们将使用`go-demo-5`应用来测量延迟，因此我们的第一步是安装它。

```
 1  GD5_ADDR=go-demo-5.$LB_IP.nip.io
 2
 3  helm install \
 4      https://github.com/vfarcic/go-demo-5/releases/download/
    0.0.1/go-demo-5-0.0.1.tgz \
 5      --name go-demo-5 \
 6      --namespace go-demo-5 \
 7      --set ingress.host=$GD5_ADDR
```

我们生成了一个将用作入口入口点的地址，并使用 Helm 部署了应用。现在我们应该等到它推出。

```
 1  kubectl -n go-demo-5 \
 2      rollout status \
 3      deployment go-demo-5
```

在我们继续之前，我们将通过发送一个 HTTP 请求来检查应用是否确实正常工作。

```
 1  curl "http://$GD5_ADDR/demo/hello"
```

输出应该是熟悉的`hello, world!`消息。

现在，让我们看看我们是否可以，例如，获得通过入口进入系统的请求的持续时间。

```
 1  open "http://$PROM_ADDR/graph"
```

如果您单击光标处插入指标下拉列表，您将能够浏览所有可用的指标。我们要找的是`nginx_ingress_controller_request_duration_seconds_bucket`。顾名思义，该指标来自 NGINX 入口控制器，以秒为单位提供请求持续时间，并以桶为单位进行分组。

请键入以下表达式，然后单击“执行”按钮。

```
 1  nginx_ingress_controller_request_duration_seconds_bucket
```

在这种情况下，查看原始值可能不是很有用，因此请单击图表选项卡。

您应该会看到图表，每个入口一个。每一个都在增加，因为所讨论的度量是一个计数器([https://prometheus.io/docs/concepts/metric_types/#counter](https://prometheus.io/docs/concepts/metric_types/#counter))。它的价值随着每个请求而增长。

A Prometheus counter is a cumulative metric whose value can only increase, or be reset to zero on restart.

我们需要的是计算一段时间内的请求率。我们将通过组合`sum`和`rate`([https://Prometheus . io/docs/Prometheus/latest/query/functions/# rate()](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()))函数来实现。前者应该是不言自明的。

Prometheus' rate function calculates the per-second average rate of increase of the time series in the range vector.

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_count[5m]
 3  )) 
 4  by (ingress)
```

结果图显示了通过入口进入系统的所有请求的每秒速率。费率是根据五分钟间隔计算的。如果您将鼠标悬停在其中一行上，您将看到像值和入口这样的附加信息。`by`语句允许我们按`ingress`对结果进行分组。

尽管如此，结果本身并不是很有用，所以让我们重新定义我们的需求。我们应该能够找出有多少请求慢于 0.25 秒。我们不能直接这么做。相反，我们可以检索所有 0.25 秒或更快的时间。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  )) 
 6  by (ingress)
```

我们真正想要的是找到落入 0.25 秒桶的请求百分比。为此，我们将获得快于或等于 0.25 秒的请求速率，并将结果除以所有请求的速率。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  )) 
 6  by (ingress) / 
 7  sum(rate(
 8    nginx_ingress_controller_request_duration_seconds_count[5m]
 9  )) 
10  by (ingress)
```

除了偶尔与普罗米修斯和警报管理器的交互以及我们发送给`go-demo-5`的一个请求之外，您可能不会在图表中看到太多，因为我们还没有产生太多流量。然而，您可以看到的几行显示了在 0.25 秒内响应的请求的百分比。

目前，我们只对`go-demo-5`请求感兴趣，所以我们将进一步细化表达式，将结果限制在`go-demo-5`入口。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25", 
 4      ingress="go-demo-5"
 5    }[5m]
 6  )) 
 7  by (ingress) / 
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress="go-demo-5"
11    }[5m]
12  )) 
13  by (ingress)
```

这个图应该几乎是空的，因为我们只发送了一个请求。或者，也许你收到了`no datapoints found`信息。是时候产生一些流量了。

```
 1  for i in {1..30}; do
 2    DELAY=$[ $RANDOM % 1000 ]
 3    curl "http://$GD5_ADDR/demo/hello?delay=$DELAY"
 4  done
```

我们向`go-demo-5`发送了三十个请求。应用有一个“隐藏”功能来延迟对请求的响应。假设我们想要生成具有随机响应时间的流量，我们使用了随机值高达千毫秒的`DELAY`变量。现在我们可以重新运行同一个查询，看看是否能得到一些更有意义的数据。

请等待一段时间，直到收集到来自新请求的数据，然后键入后面的表达式(在普罗米修斯中)，并按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25", 
 4      ingress="go-demo-5"
 5    }[5m]
 6  )) 
 7  by (ingress) / 
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress="go-demo-5"
11    }[5m]
12  )) 
13  by (ingress)
```

这一次，我们可以看到一条新线的出现。在我的例子中(截图如下)，大约 25%的请求持续时间在 0.25 秒以内。或者，换句话说，大约四分之一的请求比预期的要慢。

![](img/399a7670-e5f9-4753-8c2e-a02ef8ef2241.png)

Figure 3-8: Prometheus' graph screen with the percentage of requests with 0.25 seconds duration

当我们确实知道存在问题并且想要进一步深入研究时，针对特定应用(入口)的过滤指标非常有用。然而，我们仍然需要一个警报，告诉我们有问题。为此，我们将执行类似的查询，但这一次没有将结果限制在特定的应用(入口)中。我们还必须定义一个触发警报的条件，因此我们将阈值设置为百分之九十五(0.95)。如果没有这样的阈值，每次单个请求变慢时，我们都会收到通知。结果，我们会被警报所包围，并且很可能在不久之后就开始忽略它们。毕竟，如果一个请求很慢，没有一个系统会有危险，但只有当其中相当多的请求很慢时，才会有危险。在我们的例子中，这是 5%的慢速请求，或者更准确地说，不到 95%的快速请求。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  ))
 6  by (ingress) /
 7  sum(rate(
 8    nginx_ingress_controller_request_duration_seconds_count[5m]
 9  ))
10  by (ingress) < 0.95
```

我们可以看到不到 95%的请求在 0.25 秒内的偶然情况。在我的情况下(截图如下)，我们可以看到普罗米修斯、警报器和`go-demo-5`偶尔会变慢。

![](img/35c99ae7-7fd0-453c-bc6f-0d4b25288425.png)

Figure 3-9: Prometheus' graph screen with the percentage of requests within 0.25 seconds duration and limited only to results higher than ninety-five percent

唯一缺少的是根据前面的表达式定义警报。因此，只要不到 95%的请求持续时间少于 0.25 秒，我们就会收到通知。

我准备了一组更新的普罗米修斯图表值，所以让我们看看与我们目前使用的图表值相比的差异。

```
 1  diff mon/prom-values-nodes-am.yml \
 2      mon/prom-values-latency.yml
```

输出如下。

```
53a54,62
> - name: latency
>   rules:
>   - alert: AppTooSlow
>     expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= "0.25"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95
>     labels:
>       severity: notify
>     annotations:
>       summary: Application is too slow
>       description: More then 5% of requests are slower than 0.25s
57c66
<     expr: count(kube_node_info) > 0
---
>     expr: count(kube_node_info) > 3
```

我们增加了一个新的提醒`AppTooSlow`。如果持续时间为 0.25 秒或更短的请求百分比小于百分之九十五(`0.95`)，就会触发。

我们还将`TooManyNodes`的阈值还原为其原始值`3`。

接下来，我们将使用新值更新`prometheus`图表，并打开警报屏幕，以确认是否确实添加了新警报。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-latency.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```

如果`AppTooSlow`提醒仍然不可用，请稍等片刻并刷新屏幕。

![](img/68c98ad5-8654-4de8-b3d3-f34e05f3ce6e.png)

Figure 3-10: Prometheus' alerts screen

新添加的警报(可能)是绿色的(未触发)。我们需要生成一些缓慢的请求才能看到它的运行。

请执行以下命令发送三十个请求，随机响应时间最长为一万毫秒(十秒)。

```
 1  for i in {1..30}; do
 2    DELAY=$[ $RANDOM % 10000 ]
 3    curl "http://$GD5_ADDR/demo/hello?delay=$DELAY"
 4  done
```

普罗米修斯需要一些时间来获取新的度量标准，并让警报检测到阈值已经达到。过一会儿，我们可以再次打开警报屏幕，检查警报是否确实正在触发。

```
 1  open "http://$PROM_ADDR/alerts"
```

我们可以看到警戒状态正在开火。如果不是你的情况，请再等一会儿，刷新屏幕。在我的例子中(下面的截图)，该值为 0.125，这意味着只有 12.5%的请求持续时间为 0.25 秒或更短。

There might be two or more active alerts inside `AppTooSlow` if `prometheus-server`, `prometheus-alertmanager`, or some other application is responding slow.

![](img/17e9cd6e-41dc-486b-9ee1-cfaf8eedb886.png)

Figure 3-11: Prometheus' alerts screen with one alert firing

警报是红色的，这意味着普罗米修斯将其发送给警报管理器，警报管理器又将其转发给 Slack。让我们确认一下。

```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```

如您所见(截图如下)，我们收到了两个通知。由于我们将`TooManyNodes`警报的阈值恢复到三个以上的节点，而我们的集群只有更少的节点，普罗米修斯向警报管理器发送了问题已解决的通知。因此，我们在 Slack 中获得了一个新的通知。这一次，信息的颜色是绿色。

再往前，一条新的红色信息出现了，表明一个`Application is too slow`。

![](img/f8549f2d-8a88-4179-933a-57b9f13b7e5a.png)

Figure 3-12: Slack with alerts firing (red) and resolved (green) messages

我们通常不能依赖一个适用于所有应用的单一规则。例如，普罗米修斯和詹金斯将是内部应用的一个很好的候选人，我们不能期望内部应用在 0.25 秒以上的响应时间少于 5%。因此，我们可能需要进一步过滤警报。我们可以为此使用任意数量的标签。为了简单起见，我们将继续利用`ingress`标签，但是这一次，我们将使用正则表达式从警报中排除一些应用(Ingresses)。

让我们再次打开图形屏幕。

```
 1  open "http://$PROM_ADDR/graph"
```

请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25", 
 4      ingress!~"prometheus-server|jenkins"
 5    }[5m]
 6  )) 
 7  by (ingress) / 
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress!~"prometheus-server|jenkins"
11    }[5m]
12  )) 
13  by (ingress)
```

对之前查询的补充是`ingress!~"prometheus-server|jenkins"`过滤器。`!~`用于选择标签与`prometheus-server|jenkins`字符串不匹配的指标。由于`|`相当于`or`声明，我们可以将该过滤器翻译为“不是`prometheus-server`或不是`jenkins`的所有内容。”我们的集群中没有詹金斯。我只是想向你展示一种排除多个值的方法。

![](img/0e6442a6-12b5-48d9-ab0c-b7da6c30abdc.png)

Figure 3-13: Prometheus graph screen with the percentage of requests with 0.25 seconds duration and the results excluding prometheus-server and jenkins

我们可以把它变得更复杂一点，并指定`ingress!~"prometheus.+|jenkins.+`作为过滤器。在这种情况下，它将排除所有以`prometheus`和`jenkins`开头的入口。关键在于`.+`的添加，在正则表达式中，匹配任何字符(`.`)的一个或多个条目(`+`)。

我们将不深入解释 RegEx 语法。我希望你已经熟悉了。如果你不是，你可能想谷歌一下或者访问*正则表达式维基*页面([https://en.wikipedia.org/wiki/Regular_expression](https://en.wikipedia.org/wiki/Regular_expression))。

前面的表达式只检索非`prometheus-server`和`jenkins`的结果。我们可能需要创建另一个只包含这两个的。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.5",
 4      ingress=~"prometheus-server|jenkins"
 5    }[5m]
 6  )) 
 7  by (ingress) /
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress=~"prometheus-server|jenkins"
11    }[5m]
12  ))
13  by (ingress)
```

唯一不同的是，与之前的表达式相比，这次我们使用了`=~`运算符。它选择与提供的字符串正则匹配的标签。此外，桶(`le`)现在被设置为`0.5`秒，考虑到两个应用可能需要更多的时间来响应，我们同意这一点。

在我的例子中，图表显示`prometheus-server`有 100%的请求持续时间在 0.5 秒以内(在你的例子中，这可能不是真的)。

![](img/1a0043b8-213c-4c76-9209-2fa3a1ca851a.png)

Figure 3-14: Prometheus graph screen with the percentage of requests with 0.5 seconds duration and the results including only prometheus-server and jenkins

几个延迟示例应该足以让您使用这种类型的指标，因此我们将转向流量。

# 交通相关问题警报

到目前为止，我们测量了应用的延迟，并创建了警报，当达到基于请求持续时间的特定阈值时，就会触发警报。这些警报不是基于传入的请求数(流量)，而是基于慢速请求的百分比。只要持续时间超过阈值，即使只有一个请求进入应用，也会触发`AppTooSlow`。为了完整性，我们需要开始测量流量，或者更准确地说，测量发送到每个应用和整个系统的请求数量。通过这一点，我们可以知道我们的系统是否承受了很大的压力，并决定是扩展我们的应用、增加更多的工作人员，还是应用一些其他解决方案来缓解问题。如果请求数量达到异常数量，明确表明我们受到**拒绝服务** ( **DoS** )攻击([https://en.wikipedia.org/wiki/Denial-of-service_attack](https://en.wikipedia.org/wiki/Denial-of-service_attack))，我们甚至可能选择阻止部分传入流量。

我们将从创建一点流量开始，我们可以用它来可视化请求。

```
 1  for i in {1..100}; do
 2      curl "http://$GD5_ADDR/demo/hello"
 3  done
 4
 5  open "http://$PROM_ADDR/graph"
```

我们向`go-demo-5`应用发送了一百个请求，并打开了普罗米修斯的图形屏幕。

我们可以通过`nginx_ingress_controller_requests`检索进入入口控制器的请求数量。既然是计数器，我们可以结合`sum`继续使用`rate`功能。最后，我们可能想知道按`ingress`标签分组的请求率。

请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。

```
 1  sum(rate(
 2    nginx_ingress_controller_requests[5m]
 3  ))
 4  by (ingress)
```

我们可以在图表的右侧看到一个尖峰。它显示了通过入口进入`go-demo-5`应用的同名请求。

在我的例子中(截图如下)，峰值接近每秒一个请求(你的会不同)。

![](img/713e0fd7-3d7e-414a-b4df-02526e34bb83.png)

Figure 3-15: Prometheus' graph screen with the rate of the number of requests

我们可能对应用的每个副本每秒的请求数更感兴趣，因此我们的下一个任务是找到检索该数据的方法。由于`go-demo-5`是部署，我们可以使用`kube_deployment_status_replicas`。

请键入下面的表达式，然后按“执行”按钮。

```
 1  kube_deployment_status_replicas
```

我们可以看到系统中每个部署的副本数量。`go-demo-5`应用，在我的例子中被涂成红色(截图如下)，有三个副本。

![](img/f1c0b08a-afb3-4988-87d2-d90ec102adb9.png)

Figure 3-16: Prometheus' graph screen with the number of replicas of Deployments

接下来，我们应该组合这两个表达式，以获得每个副本每秒的请求数。然而，我们面临一个问题。对于要加入的两个指标，它们需要有匹配的标签。`go-demo-5`的部署和入口都有相同的名称，因此我们可以将其用于我们的利益，假设我们可以重命名其中一个标签。我们将借助`label_join`([https://Prometheus . io/docs/Prometheus/latest/query/functions/# label _ join()](https://prometheus.io/docs/prometheus/latest/querying/functions/#label_join()))函数来实现。

For each timeseries in v, `label_join(v instant-vector, dst_label string, separator string, src_label_1 string, src_label_2 string, ...)` joins all the values of all the `src_labels` using the separator and returns the timeseries with the label `dst_label` containing the joined value.

如果之前对`label_join`函数的解释令人困惑，那么你并不孤单。相反，让我们通过添加包含来自`deployment`标签的值的`ingress`标签来完成转换`kube_deployment_status_replicas`的示例。如果我们成功了，我们将能够把结果与`nginx_ingress_controller_requests`结合起来，因为两者将具有相同的匹配标签(`ingress`)。

请键入下面的表达式，然后按“执行”按钮。

```
 1  label_join(
 2    kube_deployment_status_replicas,
 3    "ingress", 
 4    ",", 
 5    "deployment"
 6  )
```

由于我们这次主要对标签的值感兴趣，请通过单击选项卡切换到控制台视图。

从输出中可以看到，每个指标现在都包含一个额外的标签`ingress`，其值与`deployment`相同。

![](img/d8145093-7933-49c7-9fdf-65a9fcf30124.png)

Figure 3-17: Prometheus' console view of Deployment replicas status and a new label ingress created from the deployment label

现在我们可以将这两个指标结合起来。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_requests[5m]
 3  ))
 4  by (ingress) /
 5  sum(label_join(
 6    kube_deployment_status_replicas,
 7    "ingress",
 8    ",",
 9    "deployment"
10  ))
11  by (ingress)
```

切换回*图形*视图。

我们计算了每个应用的请求数量的比率(`ingress`)并将其除以每个应用的副本总数(`ingress`)。最终结果是每个应用(`ingress`)和每个副本的请求数量比率。

值得注意的是，我们无法检索每个特定副本的请求数量，而是检索每个副本的平均请求数量。考虑到 Kubernetes 网络在大多数情况下会执行循环调度，导致向每个副本发送或多或少相同数量的请求，因此这种方法应该有效。

总而言之，现在我们知道了副本每秒接收多少请求。

![](img/d4f2cc20-dd55-4a80-af2a-177918f74576.png)

Figure 3-18: Prometheus' graph screen with the rate of requests divided by the number of Deployment replicas

既然我们已经学习了如何编写一个表达式来检索每个副本每秒的请求数，我们应该将其转换为一个警报。

因此，让我们来看看普罗米修斯图表值的新旧定义之间的区别。

```
 1  diff mon/prom-values-latency.yml \
 2      mon/prom-values-latency2.yml
```

输出如下。

```
62a63,69
> - alert: TooManyRequests
>   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment")) by (ingress) > 0.1
>   labels:
>     severity: notify
>   annotations:
>     summary: Too many requests
>     description: There is more than average of 1 requests per second per replica for at least one application
```

我们可以看到，这个表达式几乎与我们在普罗米修斯的图形屏幕中使用的表达式相同。唯一不同的是我们设置的阈值`0.1`。因此，当副本每秒接收请求的速率超过`0.1`时，该警报会通知我们，该速率是在五分钟内计算的(`[5m]`)。正如您可能已经猜到的那样，`0.1`每秒的请求数太低，无法在生产中使用。然而，它将允许我们很容易地触发警报，并看到它在行动。

现在，让我们升级我们的图表，并打开普罗米修斯的警报屏幕。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-latency2.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```

请刷新屏幕，直到出现`TooManyRequests`提醒。

![](img/84c0a44a-b5bd-4f17-a428-6f1513dec3d7.png)

Figure 3-19: Prometheus' alerts screen

接下来，我们将生成一些流量，这样我们就可以看到警报已经生成，并通过 Alertmanager 发送给 Slack。

```
 1  for i in {1..200}; do
 2      curl "http://$GD5_ADDR/demo/hello"
 3  done
 4
 5  open "http://$PROM_ADDR/alerts"
```

我们发送了 200 个请求，重新打开了普罗米修斯的警报屏幕。现在我们应该刷新屏幕，直到`TooManyRequests`警报变为红色。

一旦普罗米修斯发出警报，它就会被发送到警报器管理器，并从那里转发到 Slack。让我们确认一下。

```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```

我们可以看到`Too many requests`通知，从而证明这个预警的流程是有效的。

![](img/499f9841-7cb1-4865-a6d1-b02dff0a4761.png)

Figure 3-20: Slack with alert messages

接下来，我们将跳转到与错误相关的度量。

# 关于错误相关问题的警报

我们应该始终意识到我们的应用或系统是否正在产生错误。然而，我们不能在第一次出现错误时就惊慌失措，因为这会产生太多的通知，最终我们很可能会忽略这些通知。

错误经常发生，许多错误是由自动修复的问题或我们无法控制的情况引起的。如果我们要对每一个错误都采取行动，我们需要一大群人 24/7 全天候工作，只解决通常不需要解决的问题。例如，因为代码在 500 范围内的单一响应而进入“恐慌”模式几乎肯定会产生永久性危机。相反，我们应该监控与请求总数相比的错误率，并且只有当它超过某个阈值时才做出反应。毕竟，如果错误持续存在，这个比率无疑会增加。另一方面，如果持续较低，则意味着该问题已由系统自动修复(例如，Kubernetes 从故障节点重新计划了 Pods)，或者这是一个不会重复的孤立案例。

我们的下一个任务是检索请求，并根据它们的状态来区分它们。如果我们能做到这一点，我们应该能计算出误差率。

我们将从产生一些流量开始。

```
 1  for i in {1..100}; do
 2      curl "http://$GD5_ADDR/demo/hello"
 3  done
 4
 5  open "http://$PROM_ADDR/graph"
```

我们发送了一百个请求，打开了普罗米修斯的图形屏幕。

让我们看看我们之前使用的`nginx_ingress_controller_requests`度量是否提供了请求的状态。

请键入下面的表达式，然后按“执行”按钮。

```
 1  nginx_ingress_controller_requests
```

我们可以看到普罗米修斯最近刮到的所有数据。如果我们更仔细地观察标签，我们可以看到，除此之外，还有`status`。我们可以用它来计算基于请求总数的错误百分比(例如，500 个范围)。

我们已经看到，我们可以使用`ingress`标签来分隔每个应用的计算，假设我们只对面向公众的应用感兴趣。

![](img/adbe5b3c-eff0-4170-b518-20e9cb90f82d.png)

Figure 3-21: Prometheus' console view with requests entering through Ingress

`go-demo-5`应用有一个特殊的端点`/demo/random-error`，它将生成随机错误响应。大约十分之一的对该地址的请求会产生错误。我们可以用它来测试我们的表情。

```
 1  for i in {1..100}; do
 2    curl "http://$GD5_ADDR/demo/random-error"
 3  done
```

我们向`/demo/random-error`端点发送了 100 个请求，其中大约 10%的响应是错误的(HTTP 状态代码`500`)。

接下来，我们必须等待一会儿，让普罗米修斯刮出新的一批指标。之后，我们可以打开 Graph 屏幕，尝试编写一个表达式来检索应用的错误率。

```
 1  open "http://$PROM_ADDR/graph"
```

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    nginx_ingress_controller_requests{
 3      status=~"5.."
 4    }[5m]
 5  ))
 6  by (ingress) /
 7  sum(rate(
 8    nginx_ingress_controller_requests[5m]
 9  ))
10  by (ingress)
```

我们使用`5..` RegEx 来计算有错误的请求的比率，按`ingress`分组，然后用所有请求的比率来划分结果。结果按`ingress`分组。在我的例子中(截图如下)，结果大约是 4%(`0.04`)。普罗米修斯还没有刮除所有的指标，我预计在下一次刮除迭代中，这个数字会接近 10%。

![](img/080bb7c6-42d1-486b-9b17-4b938b5c1624.png)

Figure 3-22: Prometheus' graph screen with the percentage with the requests with error responses

让我们将图表值文件的更新版本与之前使用的版本进行比较。

```
 1  diff mon/prom-values-cpu-memory.yml \
 2      mon/prom-values-errors.yml
```

输出如下。

```
127a128,136
> - name: errors
>   rules:
>   - alert: TooManyErrors
>     expr: sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) > 0.025
>     labels:
>       severity: error
>     annotations:
>       summary: Too many errors
>       description: At least one application produced more then 5% of error responses
```

如果错误率超过总请求率的 2.5%，则会触发警报。

现在我们可以升级我们的普罗米修斯图表了。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-errors.yml
```

可能不需要确认警报工作。我们已经看到普罗米修斯将所有警报发送给警报管理器，并从那里转发给 Slack。

接下来，我们将讨论饱和指标和警报。

# 关于饱和相关问题的警报

饱和度衡量我们的服务和系统的完善程度。我们应该知道我们服务的副本是否处理了太多的请求，并被迫对其中一些请求进行排队。我们还应该监控 CPU、内存、磁盘和其他资源的使用是否达到了临界限制。

现在，我们将关注 CPU 的使用情况。我们将从打开普罗米修斯的图形屏幕开始。

```
 1  open "http://$PROM_ADDR/graph"
```

我们看看能否得到节点(`instance`)使用 CPU 的速率。我们可以使用`node_cpu_seconds_total`度量。然而，它被分成不同的模式，我们将不得不排除其中的一些模式来获得“真实”的 CPU 使用情况。这些将是`idle`、`iowait`和任何类型的`guest`循环。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle", 
 4      mode!="iowait", 
 5      mode!~"^(?:guest.*)$"
 6   }[5m]
 7  ))
 8  by (instance)
```

切换到*图形*视图。

输出代表系统中 CPU 的实际使用情况。在我的例子中(截图如下)，除了临时峰值，所有节点使用的 CPU 时间都不到 100 毫秒。

该系统远未面临压力。

![](img/52dcc7f8-4b9f-4e7f-b176-b73855232337.png)

Figure 3-23: Prometheus' graph screen with the rate of used CPU grouped by node instances

正如你已经注意到的，绝对数字很少有用。我们应该尝试发现已用 CPU 的百分比。我们需要找出我们的节点有多少 CPU。我们可以通过计算指标的数量来做到这一点。每个中央处理器都有自己的数据条目，每个模式一个。如果我们把结果限制在单一模式(例如`system`)下，应该可以得到 CPU 的总数。

请键入下面的表达式，然后按“执行”按钮。

```
 1  count(
 2    node_cpu_seconds_total{
 3      mode="system"
 4    }
 5  )
```

在我的例子中(截图如下)，总共有六个内核。如果你用的是 GKE、EKS 或吉斯特的 AKS，你的可能也是 6 个。另一方面，如果您在 Docker for Desktop 或 minikube 中运行集群，结果应该是一个节点。

现在，我们可以将这两个查询结合起来，以获得已用 CPU 的百分比

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle", 
 4      mode!="iowait",
 5      mode!~"^(?:guest.*)$"
 6    }[5m]
 7  )) /
 8  count(
 9    node_cpu_seconds_total{
10      mode="system"
11    }
12  )
```

我们总结了使用 CPU 的比率，并将其除以 CPU 总数。在我的例子中(截图如下)，目前只有 3%到 4%的 CPU 被使用。

这并不奇怪，因为大部分系统都处于静止状态。我们的集群目前没有发生太多事情。

![](img/2d2bf1f8-7ac6-4d21-a067-c6c523f0e45e.png)

Figure 3-24: Prometheus' graph screen with the percentage of available CPU

现在我们知道了如何获取整个集群中已用 CPU 的百分比，我们将把注意力转移到应用上。

我们将尝试发现我们有多少可分配的内核。从应用的角度来看，至少当它们在 Kubernetes 中运行时，可分配的 CPU 会显示 Pods 可以请求多少。可分配的中央处理器总是低于总中央处理器。

请键入下面的表达式，然后按“执行”按钮。

```
 1  kube_node_status_allocatable_cpu_cores
```

输出应该低于虚拟机使用的内核数量。可分配的内核显示了可分配给容器的 CPU 数量。更准确地说，可分配内核是指分配给节点的 CPU 数量减去系统级进程保留的 CPU 数量。在我的例子中(截图如下)，几乎有两个完全可分配的 CPU。

![](img/7283ecb4-bc9a-4432-ad59-3d1469a62c5f.png)

Figure 3-25: Prometheus' graph screen with the allocatable CPU for each of the nodes in the cluster

然而，在这种情况下，我们对可分配的 CPU 总量感兴趣，因为我们试图发现我们的 Pods 在整个集群中使用了多少。因此，我们将对可分配的内核进行求和。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(
 2    kube_node_status_allocatable_cpu_cores
 3  )
```

在我的例子中，总的可分配 CPU 大约是 5.8 个核心。具体数字，请悬停在图表线上。

现在我们知道了我们有多少可分配的 CPU，我们应该尝试发现 Pods 请求了多少。

请注意，请求的资源与使用的资源不同。我们稍后将讨论这个用例。目前，我们想知道我们向系统请求了多少。

请键入下面的表达式，然后按“执行”按钮。

```
 1  kube_pod_container_resource_requests_cpu_cores
```

我们可以看到请求的 CPU 相对较低。在我的例子中，所有请求了 CPU 的容器的值都低于 0.15(一百五十毫秒)。你的结果可能会不同。

就像可分配的中央处理器一样，我们对请求的中央处理器的总和感兴趣。稍后，我们将能够组合这两个结果，并推断集群中还有多少未保留。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(
 2    kube_pod_container_resource_requests_cpu_cores
 3  )
```

我们汇总了所有的 CPU 资源请求。因此，在我的情况下(截图如下)，所有请求的 CPU 都略低于 1.5。

![](img/0a3f5d81-4d68-4f87-8a3a-6c8c8bcfc4dd.png)

Figure 3-26: Prometheus' graph screen with the sum of the requested CPU

现在，让我们将这两个表达式结合起来，看看请求的 CPU 百分比。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(
 2    kube_pod_container_resource_requests_cpu_cores
 3  ) /
 4  sum(
 5    kube_node_status_allocatable_cpu_cores
 6  )
```

在我的例子中，输出显示大约四分之一(0.25)的所有可分配 CPU 被保留。这意味着，在我们达到扩展集群的需求之前，我们可能会有四倍的 CPU 请求。当然，您已经知道，如果存在，集群自动缩放器将在此之前添加节点。尽管如此，知道我们接近达到 CPU 极限还是很重要的。群集自动缩放器可能工作不正常，或者甚至可能不活动。后一种情况适用于大多数(如果不是所有)内部集群。

让我们看看是否可以将我们探索的表达式转换为警报。

我们将探索一组新的图表值和我们以前使用的图表值之间的另一个区别。

```
 1  diff mon/prom-values-latency2.yml \
 2      mon/prom-values-cpu.yml
```

输出如下。

```
64c64
<   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment")) by (ingress) > 0.1
---
>   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment")) by (ingress) > 1
87a88,103
> - alert: NotEnoughCPU
>   expr: sum(rate(node_cpu_seconds_total{mode!="idle", mode!="iowait", mode!~"^(?:guest.*)$"}[5m])) / count(node_cpu_seconds_total{mode="system"}) > 0.9
```

```
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough CPU
>     description: CPU usage of the cluster is above 90%
> - alert: TooMuchCPURequested
>   expr: sum(kube_pod_container_resource_requests_cpu_cores) / sum(kube_node_status_allocatable_cpu_cores) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough allocatable CPU
>     description: More than 90% of allocatable CPU is requested
```

从差异中我们可以看出，我们将`TooManyRequests`的原始阈值恢复到了`1`，并且增加了两个新的警报，分别叫做`NotEnoughCPU`和`TooMuchCPURequested`。

如果整个集群中超过 90%的 CPU 使用时间超过 30 分钟，将会触发`NotEnoughCPU`警报。这样，如果 CPU 使用率出现暂时峰值，我们将避免设置警报。

`TooMuchCPURequested`也有百分之九十的阈值，持续超过三十分钟就会触发。该表达式计算请求的总量除以可分配的 CPU 总量。

这两个警告都是我们不久前执行的普罗米修斯表达式的反映，所以您应该已经熟悉了它们的目的。

让我们用新的值升级普罗米修斯的图表，并打开警报屏幕。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-cpu.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```

剩下的就是等待两个新的警报出现。如果它们还没有出现，请刷新屏幕。

可能没有必要看到新的警报在起作用。到目前为止，你应该相信流量，没有理由相信它们不会触发。

![](img/73c6acaf-55dd-42ec-a35c-ccfadef135cd.png)

Figure 3-27: Prometheus' alerts screen

在“真实世界”场景中，根据我们使用的 Kubernetes 风格，接收两个警报之一可能会引发不同的反应。

如果我们有集群自动缩放器，我们可能不需要`NotEnoughCPU`和`TooMuchCPURequested`警报。只要我们的中央处理器请求设置正确，90%的节点中央处理器正在使用的事实并不妨碍集群正常运行。同样，保留 90%的可分配 CPU 也不是问题。如果 Kubernetes 由于所有 CPU 都被保留而无法安排一个新的 Pod，它将扩展集群。事实上，达到几乎满的 CPU 使用率或保留几乎所有可分配的 CPU 是一件好事。这意味着我们拥有所需数量的 CPU，并且我们不会为未使用的资源付费。不过，这种逻辑主要适用于云提供商，甚至不是所有提供商。今天(2018 年 10 月)，Cluster Autoscaler 仅在 AWS、GCE 和 Azure 中工作。

所有这些并不意味着我们应该只依赖集群自动缩放器。它会像其他东西一样失灵。然而，由于 CA 是基于观察不可聚合的 Pod，如果它确实无法工作，我们应该通过观察 Pod 的状态来检测，而不是 CPU 的使用。尽管如此，当 CPU 使用率过高时收到警报可能不是一个坏主意，但在这种情况下，我们可能希望将阈值提高到接近百分之百的值。

如果我们的群集是本地的，或者更准确地说，如果它没有群集自动缩放器，那么如果我们的群集扩展过程不是自动的或者速度很慢，那么我们探索的警报是必不可少的。逻辑很简单。如果我们需要几分钟以上的时间向集群添加新节点，我们不能等到 Pods 不可聚合。那就太晚了。相反，我们需要知道在群集变满(饱和)之前，我们已经没有可用容量了，这样我们就有足够的时间通过向群集添加新节点来做出反应。

尽管如此，拥有一个因为集群自动缩放器不起作用而不能自动缩放的集群并不是一个足够好的借口。我们可以使用许多其他工具来自动化我们的基础架构。当我们成功到达可以自动向集群添加新节点的位置时，警报的目的地应该会改变。我们可能不想接收对 Slack 的通知，而是希望向服务发送一个请求，该服务将执行脚本，从而将新节点添加到集群中。如果我们的集群运行在虚拟机上，我们总是可以通过脚本(或一些工具)添加更多。

接收这些通知给 Slack 的唯一真正借口是我们的集群是否在裸机上运行。在这种情况下，我们不能指望脚本神奇地创建新的服务器。对于其他所有人来说，在适当的自动化到位之前，当使用了太多的 CPU 或所有分配的 CPU 都被保留时，Slack 通知应该只是一个临时的解决方案。

现在，让我们尝试完成类似的目标，但是，这一次，通过测量内存使用和保留。

测量内存消耗类似于 CPU，但是我们应该考虑一些差异。但是，在我们到达那里之前，让我们回到普罗米修斯的图形屏幕，探索我们的第一个与内存相关的指标。

```
 1  open "http://$PROM_ADDR/graph"
```

就像 CPU 一样，首先我们需要找出每个节点有多少内存。

请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。

```
 1  node_memory_MemTotal_bytes
```

你的结果可能和我的不同。在我的例子中，每个节点都有大约 4 GB 的内存。

如果不知道当前有多少可用内存，知道每个节点有多少内存是没有用的。我们可以通过`node_memory_MemAvailable_bytes`度量来获得这个信息。

请键入下面的表达式，然后按“执行”按钮。

```
 1  node_memory_MemAvailable_bytes
```

我们可以看到集群中每个节点上的可用内存。在我的例子中(截图如下)，每个都有大约 3 GB 的可用内存。

![](img/67a644d2-43b6-4eb3-902f-56c07fffd073.png)

Figure 3-28: Prometheus' graph screen with available memory in each of the nodes of the cluster

现在我们知道了如何从每个节点获取总的可用内存，我们应该组合查询来获取整个集群的已用内存百分比。

请键入下面的表达式，然后按“执行”按钮。

```
 1  1 -
 2  sum(
 3    node_memory_MemAvailable_bytes
 4  ) /
 5  sum(
 6    node_memory_MemTotal_bytes
 7  )
```

因为我们正在搜索已用内存的百分比，并且我们有可用内存的度量，所以我们以`1 -`开始表达式，它将反转结果。表达式的其余部分是可用内存和总内存的简单划分。在我的例子中(下面的截图)，每个节点上使用的内存不到 30%。

![](img/9144a5c6-047c-4b6d-b6cf-d3e1e63235a4.png)

Figure 3-29: Prometheus' graph screen with the percentage of available memory

就像 CPU 一样，可用内存和总内存并不能描绘出全貌。虽然这是有用的信息，也是潜在警报的基础，但我们还需要知道有多少内存是可分配的，以及 Pods 使用了多少内存。我们可以通过`kube_node_status_allocatable_memory_bytes`度量得到第一个数字。

请键入下面的表达式，然后按“执行”按钮。

```
 1  kube_node_status_allocatable_memory_bytes
```

根据 Kubernetes 风格和您使用的主机提供商，总内存和可分配内存之间可能会有很小或很大的差异。我正在 AKS 中运行集群，可分配内存比总内存少一整 GB。前者约为 3 GB 内存，后者约为 4 GB 内存。差别很大。我的 Pods 没有完整的 4 GB，但比这少了大约四分之一。剩下的大约 1 GB 内存用于系统级服务。更糟糕的是，每个节点上花费了 1 GB 的内存，在我的例子中，由于我的集群有三个节点，所以总共减少了 3 GB。考虑到内存总量和可分配内存量之间的巨大差异，拥有更少数量的更大节点有明显的好处。尽管如此，并不是每个人都需要大节点，如果我们希望将节点分布在所有区域，将节点数量减少到 3 个以下可能不是一个好主意。

现在我们知道了如何检索可分配的内存量，让我们看看如何为每个应用获取请求的内存量。

请键入下面的表达式，然后按“执行”按钮。

```
 1  kube_pod_container_resource_requests_memory_bytes
```

我们可以看到，普罗米修斯(服务器)的内存请求最多(500 MB)，其他的都在下面。请记住，我们只看到有预订的豆荚。没有的不会出现在查询结果中。正如您已经知道的，只有在特殊情况下才定义保留和限制是可以的，例如，在 CI/CD 流程中使用的短期 Pods。

![](img/6968bff2-2962-4746-946f-61dbd8f5dd69.png)

Figure 3-30: Prometheus' graph screen with requested memory for each of the Pods

前面的表达式返回了每个 Pod 使用的内存量。然而，我们的任务是发现我们在整个系统中有多少请求的内存。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(
 2    kube_pod_container_resource_requests_memory_bytes
 3  )
```

在我的例子中，请求的内存总量大约是 1.6 GB 内存。

剩下的就是用集群中所有可分配内存的数量除以总请求内存。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(
 2    kube_pod_container_resource_requests_memory_bytes
 3  ) / 
 4  sum(
 5    kube_node_status_allocatable_memory_bytes
 6  )
```

在我的例子中(下面的截图)，请求的内存总量大约是集群可分配内存的百分之二十(T0)。我远离任何类型的危险，也没有必要扩大集群。如果有的话，我有太多未使用的内存，可能想要缩减。然而，我们目前只关心扩大规模。稍后，我们将探讨可能导致缩减的警报。

![](img/f2bbaa51-31e9-4115-80a1-7994bc281168.png)

Figure 3-31: Prometheus' graph screen with the percentage of the requested memory of the total allocatable memory in the cluster

让我们看看旧图表的值和我们将要使用的值之间的差异。

```
 1  diff mon/prom-values-cpu.yml \
 2      mon/prom-values-memory.yml
```

输出如下。

```
103a104,119
> - alert: NotEnoughMemory
>   expr: 1 - sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough memory
>     description: Memory usage of the cluster is above 90%
> - alert: TooMuchMemoryRequested
>   expr: sum(kube_pod_container_resource_requests_memory_bytes) / sum(kube_node_status_allocatable_memory_bytes) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough allocatable memory
>     description: More than 90% of allocatable memory is requested
```

我们添加了两个新的警报(`NotEnoughMemory`和`TooMuchMemoryRequested`)。定义本身应该很简单，因为我们已经创建了相当多的警报。这些表达式与我们在普罗米修斯图形屏幕中使用的表达式相同，只是增加了大于百分之九十(`> 0.9`)的阈值。所以，我们将跳过进一步的解释。

我们将用新的值升级我们的普罗米修斯图表，并打开警报屏幕以确认它们

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-memory.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```

如果提醒`NotEnoughMemory`和`TooMuchMemoryRequested`还不可用，请稍等片刻，并刷新屏幕。

![](img/c87a9f02-ac71-489f-907e-bd38b2233064.png)

Figure 3-32: Prometheus' alerts screen

到目前为止，基于我们创建的基于内存的警报的操作应该类似于我们与 CPU 讨论的操作。我们可以通过手动操作或自动脚本，使用它们来决定是否以及何时扩展我们的集群。就像以前一样，如果我们的群集由群集自动缩放器(CA)支持的供应商之一托管，这些警报应该纯粹是信息性的，而在本地或不受支持的云提供商那里，它们不仅仅是简单的通知。它们表明我们即将耗尽容量，至少就内存而言。

CPU 和内存示例都集中在需要知道何时是扩展集群的合适时间。我们可能会创建类似的警报，当 CPU 或内存使用率过低时会通知我们。这将清楚地表明，集群中的节点太多，我们可能需要删除一些节点。这再次假设我们没有启动和运行集群自动缩放器。尽管如此，只考虑 CPU 或内存进行缩减还是太冒险，可能会导致意想不到的结果。

让我们假设只保留了 12%的可分配 CPU，并且集群中有三个工作节点。如此低的 CPU 使用率肯定不能保证许多节点，因为平均来说，每个节点都有相对少量的保留 CPU。因此，我们可以选择缩小规模，并删除其中一个节点，从而允许其他集群重用它。这样做好吗？嗯，这取决于其他资源。如果内存预留的百分比也很低，删除一个节点是个好主意。另一方面，如果保留的内存超过 66%，删除节点将导致资源不足。当我们删除三个节点中的一个时，三个节点上超过 66%的保留内存在两个节点上变成超过 100%。

总而言之，如果我们要接收到我们的集群需要缩减的通知(并且我们没有集群自动缩放器)，我们需要将内存和 CPU 以及一些其他指标结合起来作为警报阈值。幸运的是，这些表达与我们之前使用的非常相似。我们只需要将它们组合成一个单一的警报并更改阈值。

提醒一下，我们之前使用的表达式如下(无需重新运行)。

```
 1  sum(rate(
 2    node_cpu_seconds_total{
 3      mode!="idle",
 4      mode!="iowait",
 5      mode!~"^(?:guest.*)$"
 6    }[5m]
 7  ))
 8  by (instance) /
 9  count(
10    node_cpu_seconds_total{
11      mode="system"
12    }
13  )
14  by (instance)
15
16  1 -
17  sum(
18    node_memory_MemAvailable_bytes
19  ) 
20  by (instance) /
21  sum(
22    node_memory_MemTotal_bytes
23  )
24  by (instance)
```

现在，让我们将图表的另一个更新值与我们现在正在使用的值进行比较。

```
 1  diff mon/prom-values-memory.yml \
 2      mon/prom-values-cpu-memory.yml
```

输出如下。

```
119a120,127
> - alert: TooMuchCPUAndMemory
>   expr: (sum(rate(node_cpu_seconds_total{mode!="idle", mode!="iowait", mode!~"^(?:guest.*)$"}[5m])) by (instance) / count(node_cpu_seconds_total{mode="system"}) by (instance)) < 0.5 and (1 - sum(node_memory_MemAvailable_bytes) by (instance) / sum(node_memory_MemTotal_bytes) by (instance)) < 0.5
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: Too much unused CPU and memory
>     description: Less than 50% of CPU and 50% of memory is used on at least one node
```

我们正在添加一个名为`TooMuchCPUAndMemory`的新警报。它是前两个警报的组合。只有当 CPU 和内存使用率都低于 50%时，它才会触发。这样，我们将避免发送误报，并且不会因为一个资源预留(CPU 或内存)太低，而另一个资源预留可能很高而试图缩减集群。

在我们进入下一个主题(或度量类型)之前，剩下的就是升级普罗米修斯的图表，并确认新的警报确实可以运行。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-cpu-memory.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```

如果警报仍然不存在，请刷新警报屏幕。在我的情况下(截图如下)，保留的内存和 CPU 总数低于百分之五十，警报处于挂起状态。在您的情况下，这可能不是真的，并且警报可能没有达到其阈值。然而，我将继续解释我的情况，其中 CPU 和内存的使用都不到总可用空间的 50%。

三十分钟后(`for: 30m`)，警报响起。它等待了一段时间(`30m`)来确认内存和 CPU 使用率的下降不是暂时的。假设我正在 AKS 中运行我的集群，集群自动缩放器将在三十分钟之前移除其中一个节点。但是，由于它被配置为至少在三个节点上运行，CA 将不会执行该操作。因此，我可能需要重新考虑为三个节点付费是否是一项值得的投资。另一方面，如果我的群集没有群集自动缩放器，并且假设我不想浪费资源，而其他群集可能需要更多资源，我将需要删除其中一个节点(手动或自动)。如果删除是自动的，那么目标不是 Slack，而是负责删除节点的工具的 API。

![](img/88738e53-69bf-4683-9ca4-c8fdf35a59b8.png)

Figure 3-33: Prometheus' alerts screen with one alert in the pending state

现在我们有了几个饱和的例子，我们涵盖了谷歌网站可靠性工程师支持的每一个指标和几乎任何其他监控方法。不过，我们还没完。我还想探索一些其他指标和警报。它们可能不属于任何讨论的类别，但它们可能被证明非常有用。

# 对不可切割或故障的吊舱发出警报

了解我们的应用是否难以快速响应请求，它们是否被超出其处理能力的请求轰炸，它们是否产生太多错误，以及它们是否饱和，如果它们甚至没有运行，都是没有用的。即使我们的警报通过通知我们有太多错误或由于副本数量不足而导致响应时间变慢来检测出问题，例如，如果一个甚至所有副本无法运行，我们仍然应该得到通知。在最佳情况下，此类通知将提供有关问题原因的附加信息。在更糟糕的情况下，我们可能会发现数据库的一个副本没有运行。这不一定会减慢它的速度，也不会产生任何错误，但会使我们处于无法复制数据的情况(额外的副本没有运行)，如果最后一个持续的副本也失败了，我们可能会面临状态的完全丢失。

应用无法运行的原因有很多。群集中可能没有足够的未预留资源。如果我们有集群自动缩放器，它会处理这个问题。但是，还有许多其他潜在的问题。也许新版本的映像在注册表中不可用。或者，可能是 Pods 正在请求无法声明的持久卷。正如你可能已经猜到的，可能导致我们的 Pods 失败、不可分解或处于未知状态的事情的列表几乎是无限的。

我们无法单独解决 Pods 问题的所有原因。但是，如果一个或多个 Pods 的相位是`Failed`、`Unknown`或`Pending`，我们会得到通知。随着时间的推移，我们可能会扩展我们的自我修复脚本，以解决这些状态的一些特定原因。目前，我们最好的第一步是通知 Pod 是否长时间处于其中一个阶段(例如，15 分钟)。一旦 Pod 的状态表明有问题，就发出警报是愚蠢的，因为这会产生太多的误报。我们应该得到一个警报，并在等待一段时间后选择如何行动，从而给 Kubernetes 时间来解决问题。只有当 Kubernetes 无法补救这种情况时，我们才应该采取一些被动的行动。

随着时间的推移，我们会注意到我们收到的警报中的一些模式。当我们这样做时，警报应该转换为自动响应，在没有我们参与的情况下纠正选定的问题。我们已经通过水平自动缩放器和集群自动缩放器探索了一些低挂水果。目前，我们将专注于接收所有其他情况的警报，失败和不可修复的 Pods 就是其中的几个。稍后，我们可能会探索如何自动响应。但是，那个时刻不是现在，所以我们将继续进行另一个警报，它将导致通知 Slack。

让我们打开普罗米修斯的图形屏幕。

```
 1  open "http://$PROM_ADDR/graph"
```

请键入以下表达式，然后单击“执行”按钮。

```
 1  kube_pod_status_phase
```

输出向我们显示了集群中的每个 Pods。如果你仔细看一下，你会注意到每个 Pod 有五个结果，五个可能的阶段各有一个。如果你关注`phase`字段，你会看到`Failed`、`Pending`、`Running`、`Succeeded`和`Unknown`都有一个条目。所以，每个 Pod 有五个结果，但只有一个有`1`值，而其他四个的值都设置为`0`。

![](img/5f93fbd0-1d80-4b28-9ee1-fd26930ac478.png)

Figure 3-34: Prometheus' console view with the phases of the Pods

目前，我们的兴趣主要在于警报，在大多数情况下，警报应该是通用的，与特定节点、应用、副本或其他类型的资源无关。只有当我们被提醒有问题时，我们才应该开始更深入地挖掘并寻找更细粒度的数据。考虑到这一点，我们将重写我们的表达式来检索每个阶段中 Pods 的数量。

请键入以下表达式，然后单击“执行”按钮。

```
 1  sum(
 2    kube_pod_status_phase
 3  ) 
 4  by (phase)
```

输出应显示所有吊舱都处于`Running`阶段。在我的例子中，有 27 个正在运行的 Pods，在任何其他阶段都没有。

现在，我们不应该真正关心健康的豆荚。他们正在逃跑，对此我们无能为力。相反，我们应该关注那些有问题的。因此，我们不妨重写前面的表达式，只检索那些处于`Failed`、`Unknown`或`Pending`阶段的总和。

请键入以下表达式，然后单击“执行”按钮。

```
 1  sum(
 2    kube_pod_status_phase{
 3      phase=~"Failed|Unknown|Pending"
 4    }
 5  ) 
 6  by (phase)
```

不出所料，除非你搞砸了什么，否则输出的值都设置为`0`。

![](img/d0db1323-5bea-4eb6-bdf0-4a1717233f5e.png)

Figure 3-35: Prometheus' console view with the sum of the Pods in Failed, Unknown, or Pending phases

到目前为止，没有我们应该担心的 Pods。我们将通过创建一个会故意失败的映像，通过使用一个显然不存在的映像来改变这一点。

```
 1  kubectl run problem \
 2      --image i-do-not-exist \
 3      --restart=Never
```

从输出中我们可以看到`pod/problem`是`created`。如果我们通过脚本(例如，配置项/光盘管道)创建它，我们会认为一切正常。即使我们用`kubectl rollout status`跟随它，我们也只会确保它开始工作，而不是继续工作。

但是，由于我们不是通过配置项/目录管道而是手动创建的，我们也可以列出`default`名称空间中的所有目录。

```
 1  kubectl get pods
```

输出如下。

```
NAME    READY STATUS       RESTARTS AGE
problem 0/1   ErrImagePull 0        27s
```

我们会想象我们只有短期记忆，已经忘记`image`设置为`i-do-not-exist`。会有什么问题？嗯，第一步是描述吊舱。

```
 1  kubectl describe pod problem
```

输出仅限于`Events`部分的消息，如下所示。

```
...
Events:
...  Message
...  -------
...  Successfully assigned default/problem to aks-nodepool1-29770171-2
...  Back-off pulling image "i-do-not-exist"
...  Error: ImagePullBackOff
...  pulling image "i-do-not-exist"
...  Failed to pull image "i-do-not-exist": rpc error: code = Unknown desc = Error response from daemon: repository i-do-not-exist not found: does not exist or no pull access
 Warning  Failed     8s (x3 over 46s)   kubelet, aks-nodepool1-29770171-2  Error: ErrImagePull
```

这个问题通过`Back-off pulling image "i-do-not-exist"`消息明显体现出来。再往下，我们可以看到来自容器服务器的消息称`it failed to pull image "i-do-not-exist"`。

当然，我们事先知道会有这样的结果，但是类似的事情可能会发生，而我们没有注意到有问题。原因可能是拉不动形象，或者是无数其他原因中的一个。然而，我们不应该坐在终端前，列出并描述 Pods 和其他类型的资源。相反，我们应该收到 Kubernetes 未能运行 Pod 的警报，只有在此之后，我们才应该开始挖掘问题的原因。因此，让我们再创建一个警报，当 Pods 失败并且没有恢复时，它会通知我们。

像以前一样，我们将看看普罗米修斯图表值的新旧定义之间的差异。

```
 1  diff mon/prom-values-errors.yml \
 2      mon/prom-values-phase.yml
```

输出如下。

```
136a137,146
> - name: pods
>   rules:
>   - alert: ProblematicPods
>     expr: sum(kube_pod_status_phase{phase=~"Failed|Unknown|Pending"}) by (phase) > 0
>     for: 1m
>     labels:
>       severity: notify
>     annotations:
>       summary: At least one Pod could not run
>       description: At least one Pod is in a problematic phase
```

我们定义了一组新的警报，称为`pod`。在它里面，我们有一个名为`ProblematicPods`的`alert`，如果有一个或多个带有`Failed`、`Unknown`或`Pending`相位的吊舱超过一分钟(`1m`)就会开火。我故意把它设置得很低`for`持续时间，这样我们可以很容易地测试它。稍后，我们将切换到 15 分钟的时间间隔，这将足以让 Kubernetes 在我们收到通知将我们发送到紧急模式之前解决问题。

让我们用更新的值来更新普罗米修斯的图表。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-phase.yml
```

由于我们还没有解决`problem`吊舱的问题，我们应该很快会在 Slack 中看到新的通知。让我们确认一下。

```
 1  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```

如果您的通知尚未到达，请稍等片刻。

我们得到消息说`at least one Pod could not run`。

![](img/a4035775-47d5-4dbe-a3bd-8142a286be0b.png)

Figure 3-36: Slack with an alert message

现在，我们收到通知，其中一个 Pods 有问题，我们应该去普罗米修斯，挖掘数据，直到我们找到问题的原因，并修复它。但是，由于我们已经知道问题是什么(我们故意制造的)，我们将跳过所有这些，并删除有问题的 Pod，然后再进入下一个主题。

```
 1  kubectl delete pod problem
```

# 升级旧豆荚

我们的主要目标应该是通过主动预防问题的发生。在我们无法预测问题即将出现的情况下，我们至少必须迅速采取反应行动，在问题发生后缓解问题。尽管如此，还有第三类人只能被笼统地描述为积极主动。我们应该保持我们的系统干净和最新。

我们可以做很多事情来保持系统最新，其中之一就是确保我们的软件相对较新(打补丁、更新等等)。一个合理的规则可能是尝试在 90 天后更新软件，如果不是更早的话。这并不意味着我们在集群中运行的所有东西都应该比 90 天更新，但这可能是一个很好的起点。此外，我们可能会创建更好的策略，允许某些类型的应用(通常是第三方)在半年内不升级。其他软件，尤其是我们正在积极开发的软件，可能会更频繁地升级。尽管如此，我们的出发点是检测所有未在 90 天或更长时间内升级的应用。

正如本章几乎所有其他练习一样，我们将从打开普罗米修斯的图形屏幕开始，并探索可能有助于我们实现目标的指标。

```
 1  open "http://$PROM_ADDR/graph"
```

如果我们检查可用的指标，我们会看到有`kube_pod_start_time`。它的名字清楚地表明了它的目的。它以仪表的形式提供了代表每个 Pod 开始时间的 Unix 时间戳。让我们看看它在行动。

请键入以下表达式，然后单击“执行”按钮。

```
 1  kube_pod_start_time
```

这些值本身是没有用的，也没有必要教你如何根据这些值计算人类的日期。重要的是现在和那些时间戳的区别。

![](img/d9204c74-681c-4097-8ce8-b76c8b78b6ab.png)

Figure 3-37: Prometheus' console view with the start time of the Pods

我们可以使用普罗米修斯的`time()`函数返回自 1970 年 1 月 1 日起的秒数(UTC 或 Unix 时间戳)。

请键入以下表达式，然后单击“执行”按钮。

```
 1  time()
```

就像`kube_pod_start_time`一样，我们得到了一个代表 1970 年以来秒数的长数字。除了值之外，唯一值得注意的区别是只有一个条目，而通过`kube_pod_start_time`我们得到了集群中每个 Pod 的结果。

现在，让我们结合这两个指标来尝试检索每个 Pods 的年龄。

请键入以下表达式，然后单击“执行”按钮。

```
 1  time() -
 2  kube_pod_start_time
```

结果是这一次更小的数字代表从现在到每一个豆荚创造之间的秒。在我的例子中(截图如下)，第一个 Pod(其中一个`go-demo-5`副本)的历史略超过 6000 秒。这大约需要一百分钟(6096 / 60)，或者不到两个小时(100 分钟/ 60 分钟= 1.666 小时)。

![](img/d38868b1-8abb-4ad3-aab7-915d496b32bc.png)

Figure 3-38: Prometheus' console view with the time passed since the creation of the Pods

由于可能没有比我们 90 天的目标更早的吊舱，我们将暂时将其降低到一分钟(60 秒)。

请键入以下表达式，然后单击“执行”按钮。

```
 1  (
 2    time() -
 3    kube_pod_start_time{
 4      namespace!="kube-system"
 5    }
 6  ) > 60
```

在我的例子中，所有的豆荚都超过一分钟(可能你的也是)。我们确认它有效，因此我们可以将阈值提高到 90 天。要达到 90 天，我们应该将阈值乘以 60 得到分钟，再乘以 60 得到小时，乘以 24 得到天，最后乘以 90。公式是`60 * 60 * 24 * 90`。我们可以使用`7776000`的最终值，但这将使查询更难破译。我更喜欢用公式来代替。

请键入以下表达式，然后单击“执行”按钮。

```
 1  (
 2    time() -
 3    kube_pod_start_time{
 4      namespace!="kube-system"
 5    }
 6  ) >
 7  (60 * 60 * 24 * 90)
```

没有(可能)结果应该不足为奇。如果你为这一章创建了一个新的集群，你需要成为地球上最慢的读者，如果你花了 90 天到达这里。这可能是我迄今为止写的最长的一章，但仍然不值得读 90 天。

现在我们知道使用哪个表达式了，我们可以在设置中再添加一个警报。

```
 1  diff mon/prom-values-phase.yml \
 2      mon/prom-values-old-pods.yml
```

输出如下。

```
146a147,154
> - alert: OldPods
>   expr: (time() - kube_pod_start_time{namespace!="kube-system"}) > 60
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Old Pods
>     description: At least one Pod has not been updated to more than 90 days
```

我们可以看到新旧值的区别在`OldPods`警戒。它包含了我们刚才使用的同一个表达。

我们保持`60`秒的低阈值，这样我们就可以看到警报在起作用。稍后，我们会将该值增加到 90 天。

没有必要指定`for`持续时间。一旦其中一个豆荚的年龄达到三个月，警报就会响起。

让我们用更新的值升级我们的普罗米修斯图表，并打开松弛通道，在那里我们应该会看到新的消息。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-old-pods.yml
 8
 9  open "https://devops20.slack.com/messages/CD8QJA8DS/"
```

剩下的就是等一会儿，直到新的消息到达。它应该包含标题*旧吊舱*和文字说明*至少有一个吊舱没有更新到超过 90 天*。

![](img/83874a98-4079-4787-9dce-83f28f3c7db4.png)

Figure 3-39: Slack with multiple fired and resolved alert messages

这样的通用警报可能不适用于您的所有用例。但是，我相信您将能够根据名称空间、名称或类似的东西将其分成多个警报。

现在我们有了一个机制，可以在 Pods 太旧并且可能需要升级时接收通知，我们将跳到下一个主题，并探索如何检索容器使用的内存和 CPU。

# 测量容器内存和中央处理器的使用情况

如果您熟悉 Kubernetes，您就会理解定义资源请求和限制的重要性。由于我们已经探索了`kubectl top pods`命令，您可能已经设置了请求的资源以匹配当前的使用，并且您可能已经将限制定义为高于请求。这种方法可能会在第一天奏效。但是，随着时间的推移，这些数字会发生变化，我们将无法通过`kubectl top pods`了解全部情况。我们需要知道内存和中央处理器容器在峰值负载时使用了多少内存和中央处理器，以及在压力较小时使用了多少内存和中央处理器。我们应该随着时间的推移观察这些指标，并定期调整。

即使我们设法猜测一个容器需要多少内存和中央处理器，这些数字可能会随着版本的不同而变化。也许我们引入了一个需要更多内存或 CPU 的特性？

我们需要的是随着时间的推移观察资源使用情况，并确保它不会随着新版本或用户数量的增加(或减少)而改变。现在，我们将关注前一种情况，并探索如何查看我们的容器随着时间的推移使用了多少内存和 CPU。

像往常一样，我们将从打开普罗米修斯的图形屏幕开始。

```
 1  open "http://$PROM_ADDR/graph"
```

我们可以通过`container_memory_usage_bytes`检索容器内存使用情况。

请输入下面的表达式，按下执行按钮，切换到*图形*屏幕。

```
 1  container_memory_usage_bytes
```

如果你仔细看看最常用的用法，你可能会感到困惑。似乎有些容器使用的内存远远超出了预期。

事实是，一些`container_memory_usage_bytes`记录包含累积值，我们应该排除它们，以便只检索单个容器的内存使用情况。我们可以通过只检索在`container_name`字段中有值的记录来实现。

请键入下面的表达式，然后按“执行”按钮。

```
 1  container_memory_usage_bytes{
 2    container_name!=""
 3  }
```

现在这个结果更有意义了。它反映了运行在集群内部的容器的内存使用情况。

稍后我们将讨论基于容器资源的警报。现在，我们将假设我们想要检查特定容器(例如，`prometheus-server`)的内存使用情况。既然我们已经知道其中一个可用的标签是`container_name`，检索我们需要的数据应该很简单。

请键入下面的表达式，然后按“执行”按钮。

```
 1  container_memory_usage_bytes{
 2    container_name="prometheus-server"
 3  }
```

我们可以看到过去一个小时容器内存使用的波动。通常，我们会对更长的时间感兴趣，比如一天或一周。我们可以通过单击图表上方的-和+按钮，或者直接在它们之间的字段中键入值(例如，`1w`)来实现这一点。但是，更改持续时间可能没有多大帮助，因为我们运行集群的时间还不长。除非你是一个阅读速度很慢的人，否则我们可能无法压缩超过几个小时的数据。

![](img/a6c281c9-3cdc-451b-88db-a3e4409d6e53.png)

Figure 3-40: Prometheus' graph screen with container memory usage limited to prometheus-server

同样，我们也应该能够检索容器的 CPU 使用情况。在这种情况下，我们要寻找的指标可能是`container_cpu_usage_seconds_total`。然而，不像`container_memory_usage_bytes`是一个量规，`container_cpu_usage_seconds_total`是一个计数器，我们必须将`sum`和`rate`结合起来才能得到数值随时间的变化。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(rate(
 2    container_cpu_usage_seconds_total{
 3      container_name="prometheus-server"
 4    }[5m]
 5  ))
 6  by (pod_name)
```

该查询显示五分钟间隔内的 CPU 秒数总和。我们在混合中加入了`by (pod_name)`，这样我们就可以区分不同的豆荚，看看一个是什么时候被创造的，另一个是什么时候被摧毁的。

![](img/3c443319-d0bf-42c8-bb32-fd7c0f423ed4.png)

Figure 3-41: Prometheus' graph screen with the rate of container CPU usage limited to prometheus-server

如果这是一个“真实世界”的情况，我们的下一步将是将实际资源使用情况与我们定义的普罗米修斯`resources`进行比较。如果我们定义的与实际情况有很大的不同，我们可能应该更新我们的 Pod 定义(第一节)。

问题是，使用“真实”资源使用来更好地定义 Kubernetes `resources`只会暂时提供有效值。随着时间的推移，我们的资源使用会发生变化。负载可能会增加，新特性可能更需要资源，等等。不管原因是什么，需要注意的关键是一切都是动态的，没有理由为了资源而不这样想。本着这种精神，我们的下一个挑战是找出当实际资源使用与我们在容器`resources`中定义的相差太大时，如何获得通知。

# 将实际资源使用情况与定义的请求进行比较

如果我们在 Pod 内部定义容器`resources`，并且不依赖实际使用，我们只是猜测我们期望一个容器使用多少内存和中央处理器。我相信你已经知道为什么猜测，在软件行业，是一个可怕的想法，所以我将只关注 Kubernetes 方面。

Kubernetes 将带有没有指定资源的容器的 Pods 视为**尽力服务质量** ( **服务质量**)。因此，如果它耗尽内存或中央处理器来为所有的 Pods 服务，这些 Pods 将首先被强制移除，以便为其他 Pods 留出空间。如果这样的 Pods 寿命很短，例如那些用作连续交付过程的一次性代理的 Pods，那么 BestEffort QoS 是个不错的主意。但是，当我们的应用长期存在时，尽力服务质量应该是不可接受的。这意味着在大多数情况下，我们确实必须定义容器`resources`。

如果容器`resources`是(几乎总是)必须的，我们需要知道放哪些值。我经常看到仅仅猜测的团队。“这是一个数据库；因此，它需要大量的 RAM”和“它只是一个 API，应该不需要太多”只是我经常听到的几个句子。这些猜测往往是无法衡量实际使用情况的结果。当事情发生时，这些团队会将分配的内存和 CPU 增加一倍。问题解决了！

我一直不明白为什么会有人发明一个应用需要多少内存和 CPU。即使没有任何“花哨”的工具，我们在 Linux 中也总是有`top`命令。我们可以知道我们的应用使用了多少。随着时间的推移，更好的工具被开发出来，我们所要做的就是谷歌“如何测量我的应用的内存和 CPU。”

当你需要当前数据时，你已经看到`kubectl top pods`在起作用，你也越来越熟悉普罗米修斯给予你更多的力量。你没有理由猜测。

但是，与请求的资源相比，我们为什么要关心资源的使用呢？除了可能揭示潜在问题(例如，内存泄漏)的事实之外，不准确的资源请求和限制会妨碍 Kubernetes 高效地完成工作。例如，如果我们将内存请求定义为 1 GB 内存，那么 Kubernetes 将从可分配内存中移除多少内存。如果一个节点有 2 GB 的可分配内存，那么只有两个这样的容器可以在那里运行，即使每个容器只使用 50 MB 的内存。我们的节点将只使用一小部分可分配内存，如果我们有集群自动缩放器，即使旧节点仍然有大量未使用的内存，也会添加新节点。

即使现在我们知道如何获得实际的内存使用情况，但每天开始将 YAML 文件与普罗米修斯中的结果进行比较也是浪费时间。相反，我们将创建另一个警报，当请求的内存和 CPU 与实际使用相差太大时，它会向我们发送通知。那是我们的下一个任务。

首先，我们将重新打开普罗米修斯的图形屏幕。

```
 1 open "http://$PROM_ADDR/graph"
```

我们已经知道如何通过`container_memory_usage_bytes`获取内存使用情况，所以我们将直接进入检索请求的内存。如果我们能把两者结合起来，我们将得到请求的内存使用量和实际内存使用量之间的差异。

我们要找的指标是`kube_pod_container_resource_requests_memory_bytes`，所以让我们用`prometheus-server` Pod 来旋转一下。

请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。

```
 1  kube_pod_container_resource_requests_memory_bytes{
 2    container="prometheus-server"
 3  }
```

从结果中我们可以看到，我们为`prometheus-server`容器请求了 500 MB 的内存。

![](img/c2002dc2-0574-48b0-93a5-22d701f67efa.png)

Figure 3-42: Prometheus' graph screen with container requested memory limited to prometheus-server

问题是`kube_pod_container_resource_requests_memory_bytes`指标有`pod`标签，而另一方面，`container_memory_usage_bytes`使用`pod_name`。如果我们要将两者结合起来，我们需要将标签`pod`转换为`pod_name`。幸运的是，这不是我们第一次面临这个问题，我们已经知道解决方案是使用`label_join`功能，该功能将基于一个或多个现有标签创建新标签。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      container_name="prometheus-server"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod)
```

这一次，我们不仅给度量增加了一个新的标签，而且我们还按照相同的标签(`by (pod)`)对结果进行了分组。

![](img/ca12f3cb-0be1-487f-93c4-626630e0b2b0.png)

Figure 3-43: Prometheus' graph screen with container memory usage limited to prometheus-server and grouped by the pod label extracted from pod_name

现在，我们可以将这两个指标结合起来，找出请求的内存使用量和实际内存使用量之间的差异。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      container_name="prometheus-server"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_requests_memory_bytes{
12      container="prometheus-server"
13    }
14  )
15  by (pod)
```

在我的例子中(截图如下)，差异逐渐变小。从大约百分之六十开始，现在大约是百分之七十五。这样的差异还不足以让我们采取任何纠正措施。

![](img/2f253a04-5003-490d-8be4-ed23bc20e24f.png)

Figure 3-44: Prometheus' graph screen with the percentage of container memory usage based on requested memory

既然我们已经看到了如何获得单个容器的保留内存使用量和实际内存使用量之间的差异，我们可能应该使表达式更加通用，并获得集群中的所有容器。然而，这一切可能有点过分。我们可能不想干扰在`kube-system`命名空间中运行的 Pods。它们可能是预先安装在集群中的，我们可能希望保持原样，至少目前是这样。所以，我们将它们排除在查询之外。

请键入下面的表达式，然后按“执行”按钮。

```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      namespace!="kube-system"
 4    },
 5    "pod",
 6    ",",
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_requests_memory_bytes{
12      namespace!="kube-system"
13    }
14  )
15  by (pod)
```

结果应该是请求内存和实际内存之间差异的百分比列表，不包括`kube-system`中的 Pods。

在我的例子中，有相当多的容器使用了比我们请求的多得多的内存。罪魁祸首是`prometheus-alertmanager`，它使用的内存是我们请求的三倍多。这可能有几个原因。也许我们请求的内存太少，或者它包含没有指定`requests`的容器。无论是哪种情况，我们都应该重新定义请求，不仅针对 Alertmanager，还针对所有其他 Pods，这些 Pods 使用的内存比请求的内存多 50%。

![](img/9f028f63-5a85-46be-8f15-2727d4f2c585.png)

Figure 3-45: Prometheus' graph screen with the percentage of container memory usage based on requested memory and with those from the kube-system Namespace excluded

我们即将定义一个新的警报，它将处理请求的内存比实际使用量多得多或少得多的情况。但是，在我们这样做之前，我们应该讨论我们应该使用的条件。当实际内存使用量超过请求内存的 150%超过一小时时，可能会触发一个警报。这将消除由内存使用暂时激增引起的误报(这也是我们有`limits`的原因)。另一个警报可以处理内存使用率比请求量低 50%以上的情况。但是，在这种情况下，我们可能会添加另一个条件。

有些应用太小，我们可能永远无法微调它们的请求。我们可以通过添加另一个条件来排除这些情况，该条件将忽略只有 5 MB 或更少保留内存的 Pods。

最后，此警报可能不需要像前一个警报那样频繁触发。我们应该相对快速地知道我们的应用使用的内存是否超过了我们的预期，因为这可能是内存泄漏、流量显著增加或其他一些潜在危险情况的迹象。但是，如果内存使用远远低于预期，问题就不那么严重了。我们应该纠正它，但没有必要采取紧急行动。因此，我们将后一个警报的持续时间设置为六个小时。

现在，我们设定了一些应该遵循的规则，我们可以看看新旧图表值之间的另一个差异。

```
 1  diff mon/prom-values-old-pods.yml \
 2      mon/prom-values-req-mem.yml
```

输出如下。

```
148c148
<   expr: (time() - kube_pod_start_time{namespace!="kube-system"}) > 60
---
>   expr: (time() - kube_pod_start_time{namespace!="kube-system"}) > (60 * 60 * 24 * 90)
154a155,172
> - alert: ReservedMemTooLow
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system", namespace!="ingress-nginx"}, "pod", ",", "pod_name")) by (pod) /
 sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod) > 1.5
>   for: 1m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved memory is too low
>     description: At least one Pod uses much more memory than it reserved
> - alert: ReservedMemTooHigh
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system", namespace!="ingress-nginx"}, "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod) < 0.5 and sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod) > 5.25e+06
>   for: 6m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved memory is too high
>     description: At least one Pod uses much less memory than it reserved
```

首先，我们将`OldPods`警报的阈值设置回 90 天(`60 * 60 * 24 * 90`)的预期值。这样我们就可以阻止它仅仅为了测试目的而触发警报。

接下来，我们定义了一个名为`ReservedMemTooLow`的新警报。如果使用的内存比请求的内存大`1.5`倍以上，它就会触发。警报的未决状态的持续时间被设置为`1m`，只是为了我们可以看到结果，而不需要等待整整一个小时。稍后，我们会将其恢复到`1h`。

`ReservedMemTooHigh`警报与前一个(部分)类似，只是如果实际内存和请求内存之间的差异小于`0.5`，并且如果继续超过`6m`(我们稍后会将其更改为`6h`)的情况，它将触发警报。表达式的第二部分是新的。它要求 Pod 中的所有容器都有超过 5 MB 的请求内存(`5.25e+06`)。通过第二种说法(用`and`隔开)，我们避免了处理太小的应用。如果它需要少于 5 MB 的内存，我们应该忽略它，并可能祝贺它背后的团队使它如此高效。

现在，让我们用更新的值升级我们的普罗米修斯图表，并打开图表屏幕。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-req-mem.yml
```

我们不会等到警报响起。相反，我们将尝试完成类似的目标，但使用 CPU。

可能没有必要解释我们将要使用的表达方式。我们将通过探索新旧图表值之间的差异，直接进入基于 CPU 的警报。

```
 1  diff mon/prom-values-req-mem.yml \
 2      mon/prom-values-req-cpu.yml
```

输出如下。

```
157c157
<   for: 1m
---
>   for: 1h
166c166
<   for: 6m
---
>   for: 6h
172a173,190
> - alert: ReservedCPUTooLow
>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!="kube-system", namespace!="ingress-nginx", pod_name!=""}[5m]), "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod) > 1.5
>   for: 1m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved CPU is too low
>     description: At least one Pod uses much more CPU than it reserved
> - alert: ReservedCPUTooHigh
>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!="kube-system", pod_name!=""}[5m]), "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod) < 0.5 and 
sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod) > 0.005
>   for: 6m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved CPU is too high
>     description: At least one Pod uses much less CPU than it reserved
```

前两组差异为我们之前探讨的`ReservedMemTooLow`和`ReservedMemTooHigh`警报定义了更合理的阈值。再往下，我们可以看到两个新的警报。

如果中央处理器使用量超过请求量的 1.5 倍，将触发`ReservedCPUTooLow`警报。类似地，`ReservedCPUTooHigh`警报只有在 CPU 使用率低于请求的一半，并且我们请求的 CPU 时间超过 5 毫秒时才会触发。因为 5 MB 内存太大而收到通知是浪费时间。

如果问题持续很短的时间(`1m`和`6m`)，两个警报都会被触发，这样我们就可以看到它们在运行，而不必等待太长时间。

现在，让我们用更新的值升级我们的普罗米修斯图表。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-req-cpu.yml
```

我将让您检查是否有任何警报触发，以及它们是否从 Alertmanager 转发到 Slack。你现在应该知道怎么做了。

接下来，我们将进入本章的最后一个警报。

# 将实际资源使用情况与定义的限制进行比较

与请求相比，知道容器何时使用过多或过少的资源有助于我们更精确地定义资源，并最终帮助 Kubernetes 更好地决定在哪里调度 Pods。在大多数情况下，请求的资源使用量和实际资源使用量之间有太大的差异不会导致故障。相反，这更有可能导致 Pods 分布不均衡，或者节点数量超过我们的需求。另一方面，极限是另一回事。

如果封装为 Pods 的容器的资源使用达到指定的`limits`，如果没有足够的内存，Kubernetes 可能会杀死这些容器。它这样做是为了保护系统其余部分的完整性。被杀死的豆荚不是一个永久的问题，因为如果有足够的容量，Kubernetes 几乎会立即重新安排它们。

如果我们使用集群自动缩放，即使没有足够的容量，只要它检测到一些 Pods 处于挂起状态(不可聚集)，就会添加新节点。因此，如果资源使用超过极限，世界不太可能结束。

然而，杀死和重新安排 Pods 会导致停机。显然，可能会发生更糟糕的情况。但我们不会深入其中。相反，我们将假设我们应该意识到一个 Pod 即将达到它的极限，我们可能想要调查发生了什么，我们可能需要采取一些纠正措施。也许最新版本引入了内存泄漏？或者，负载的增加超出了我们的预期和测试，这导致了内存使用的增加。使用接近极限的内存的原因不是现在的焦点。发现我们已经达到极限。

首先，我们将回到普罗米修斯的图形屏幕。

```
 1  open "http://$PROM_ADDR/graph"
```

我们已经知道，我们可以通过`container_memory_usage_bytes`度量获得实际的内存使用情况。既然我们已经探索了如何获得请求的内存，我们可以猜测限制是相似的。他们确实是，他们可以通过`kube_pod_container_resource_limits_memory_bytes`取回。由于其中一个指标与之前相同，另一个非常相似，我们将直接执行完整的查询。

请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。

```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      namespace!="kube-system"
 4    }, 
 5    "pod", 
 6    ",", 
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_limits_memory_bytes{
12      namespace!="kube-system"
13    }
14  )
15  by (pod)
```

在我的例子中(下面的截图)，我们可以看到相当多的 Pods 使用了超过其极限的内存。

幸运的是，我的集群中确实有多余的容量，Kubernetes 没有立即杀死任何一个 Pods 的需要。此外，问题可能不在于 Pods 中使用了超过设定的限制，而是这些 Pods 中并非所有容器都有设定的限制。无论是哪种情况，我都应该更新这些 Pods/容器的定义，并确保它们的限制高于几天甚至几周的平均使用量。

![](img/c57b7c9d-4904-4167-b320-ac2a31877b8a.png)

Figure 3-46: Prometheus' graph screen with the percentage of container memory usage based on memory limits and with those from the kube-system Namespace excluded

接下来，我们将进行探索新旧价值观差异的演练。

```
 1  diff mon/prom-values-req-cpu.yml \
 2      mon/prom-values-limit-mem.yml
```

输出如下。

```
175c175
<   for: 1m
---
>   for: 1h
184c184
<   for: 6m
---
>   for: 6h
190a191,199
> - alert: MemoryAtTheLimit
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system"}, "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_limits_memory_bytes{namespace!="kube-system"}) by (pod) > 0.8
>   for: 1h
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Memory usage is almost at the limit
>     description: At least one Pod uses memory that is close it its limit
```

除了恢复我们之前使用的警报的合理阈值之外，我们还定义了一个名为`MemoryAtTheLimit`的新警报。如果实际使用量超过限值的百分之八十(`0.8`)超过一小时(`1h`)，则会触发。

接下来是我们普罗米修斯图表的升级。

```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-limit-mem.yml
```

最后，我们可以打开普罗米修斯的警报屏幕，并确认新的警报确实被添加到混合中。

```
 1  open "http://$PROM_ADDR/alerts"
```

我们不会为 CPU 创建类似的警报。你自己应该知道怎么做。

# 现在怎么办？

我们探索了相当多的普罗米修斯度量、表达式和警报。我们看到了如何将普罗米修斯警报与警报管理器连接起来，并从那里将它们从一个应用转发到另一个应用。

到目前为止，我们所做的只是冰山一角。如果花太多时间(和空间)来探索我们可能使用的所有度量和表达式。尽管如此，我相信现在你知道了一些更有用的方法，你将能够用那些特定于你的方法来扩展它们。

我敦促你给我发送你认为有用的表达和提醒。你知道哪里可以找到我(*devo ps20*([http://slack.devops20toolkit.com/](http://slack.devops20toolkit.com/))Slack，`viktor@farcic` email，`@vfarcic`在推特上，等等)。

现在，我将让您决定是直接进入下一章，销毁整个集群，还是仅删除我们安装的资源。如果您选择后者，请使用以下命令。

```
 1  helm delete prometheus --purge
 2
 3  helm delete go-demo-5 --purge
 4
 5  kubectl delete ns go-demo-5 metrics
```

在你离开之前，你可能要复习一下本章的要点。

*   普罗米修斯是一个数据库(种类)，设计用于获取(提取)和存储高维时间序列数据。
*   每个人都应该利用的四个关键指标是延迟、流量、错误和饱和度。