# 五、保持数据的持久性

在本章中，我们将通过介绍 Docker 卷的所有内容来介绍如何保持重要数据的持久性、安全性和独立于容器。我们将讨论各种主题，包括以下内容:

*   Docker 内部映像
*   部署您自己的存储库实例
*   瞬态存储
*   持久存储
    *   绑定安装
    *   命名卷
    *   可重定位卷
*   用户和组标识处理

虽然我们不会涵盖所有可用的存储选项，尤其是那些特定于编排工具的选项，但本章应该让您更好地了解 Docker 如何处理数据，以及您可以做些什么来确保数据保持在您想要的方式。

# Docker 内部映像

为了更好地理解为什么我们需要持久数据，我们首先需要详细理解 Docker 如何处理容器层。在前面的章节中，我们已经详细介绍了这个主题，但是在这里，我们将花一些时间来了解封面下发生了什么。我们将首先讨论 Docker 目前如何处理容器中的书面数据。

# 映像是如何分层的

如前所述，Docker 将组成映像的数据存储在一组离散的只读文件系统层中，当您构建映像时，这些文件系统层相互堆叠。对文件系统所做的任何更改都像透明幻灯片一样堆叠在彼此之上，以创建完整的树，任何具有较新内容的文件(包括被完全删除的文件)都将通过每个新层来掩盖旧文件。对于容器的基本处理，我们以前的理解深度可能已经足够了，但是对于高级使用，我们需要知道数据如何被处理的全部内部信息。

当您用同一个基本映像启动多个容器时，所有容器都被赋予了与原始映像相同的文件系统层集，因此它们从完全相同的文件系统历史开始(除了任何装载的卷或变量)，正如我们所期望的那样。但是，在启动过程中，会在映像的顶部添加一个额外的可写层，该层会保留写入该特定容器中的任何数据:

![](img/ff574c3c-14b2-4922-a4b8-b3dfee7d644f.png)

如您所料，任何新文件都会写入这个顶层，但这个层实际上与其他层不是同一类型，而是一种特殊的**写入时复制** ( **CoW** )类型。如果您正在容器中写入的文件已经是其中一个底层的一部分，Docker 将在新的层中复制它，屏蔽旧的层，从那时起，如果您读取或写入该文件，CoW 层将返回其内容。

如果您在不尝试保存这个新的 CoW 层或不使用卷的情况下销毁这个容器，正如我们之前在不同的上下文中所经历的那样，这个可写层将被删除，并且该容器写入文件系统的所有数据都将丢失。事实上，如果您通常认为容器只是具有薄且可写的 CoW 层的映像，您可以看到这个分层系统是多么简单而有效。

# 持久化可写 CoW 层

在某个时候，您可能希望保存可写容器层，以便以后用作常规映像。虽然这种类型的映像拼接是非常不鼓励的，我也倾向于同意这一点，但是当您无法以其他方式研究容器代码时，您可能会发现它可以为您提供无价的调试工具。要从现有容器创建映像，需要`docker commit`命令:

```
$ docker commit --help

Usage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]

Create a new image from a container's changes

Options:
 -a, --author string    Author (e.g., "John Hannibal Smith <hannibal@a-team.com>")
 -c, --change list      Apply Dockerfile instruction to the created image
 --help             Print usage
 -m, --message string   Commit message
 -p, --pause            Pause container during commit (default true)
```

如你所见，我们只需要一些基本信息，其余的由 Docker 负责。不如我们自己试试:

```
$ # Run a new NGINX container and add a new file to it
$ docker run -d nginx:latest
2020a3b1c0fdb83c1f70c13c192eae25e78ca8288c441d753d5b42461727fa78
$ docker exec -it \
              2020a3b1 \
              /bin/bash -c "/bin/echo test > /root/testfile"

$ # Make sure that the file is in /root
$ docker exec -it \
              2020a3b1 \
              /bin/ls /root
testfile

$ # Check what this container's base image is so that we can see changes
$ docker inspect 2020a3b1 | grep Image
 "Image": "sha256:b8efb18f159bd948486f18bd8940b56fd2298b438229f5bd2bcf4cedcf037448",
 "Image": "nginx:latest",

$ # Commit our changes to a new image called "new_nginx_image"
$ docker commit -a "Author Name <author@site.com>" \
                -m "Added a test file" \
                2020a3b1 new_nginx_image
sha256:fda147bfb46277e55d9edf090c5a4afa76bc4ca348e446ca980795ad4160fc11

$ # Clean up our original container
$ docker stop 2020a3b1 && docker rm 2020a3b1
2020a3b1
2020a3b1

$ # Run this new image that includes the custom file
$ docker run -d new_nginx_image
16c5835eef14090e058524c18c9cb55f489976605f3d8c41c505babba660952d

$ # Verify that the file is there
$ docker exec -it \
              16c5835e \
              /bin/ls /root
testfile

$ # What about the content?
$ docker exec -it \
              16c5835e \
              /bin/cat /root/testfile
test

$ See what the new container's image is recorded as
$ docker inspect 16c5835e | grep Image
 "Image": "sha256:fda147bfb46277e55d9edf090c5a4afa76bc4ca348e446ca980795ad4160fc11",
 "Image": "new_nginx_image",

$ # Clean up
$ docker stop 16c5835e && docker rm 16c5835e
16c5835e
16c5835e
```

The `docker commit -c` switch is very useful and adds a command to the image just like the Dockerfile would and accepts the same directives that the Dockerfile does, but since this form is so rarely used, we have decided to skip it. If you would like to know more about this particular form and/or more about `docker commit`, feel free to explore [https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations](https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations) at leisure.

# 运行您自己的映像注册表

在前一章中，在 Swarm 部署期间，我们收到了关于不为我们的映像使用注册表的警告，这是有充分理由的。我们所做的所有工作都是基于我们的映像只对我们本地的 Docker Engine 可用，因此多个节点无法使用我们构建的任何映像。对于绝对简单的设置，您可以使用 Docker Hub([https://hub.docker.com/](https://hub.docker.com/))作为托管您的公共映像的选项，但是由于几乎每个**虚拟私有云(VPC)** 集群都使用自己的私有注册表内部实例来实现安全性、速度和隐私，如果您想探索它，我们将把 Docker Hub 留给您作为练习，我们将在此介绍如何运行我们自己的注册表。

Docker has recently come out with a service called Docker Cloud ([https://cloud.docker.com/](https://cloud.docker.com/)), which has private registry hosting and continuous integration and may cover a decent amount of use cases for small-scale deployments, though the service is not free past a single private repository at this time. Generally, though, the most preferred way of setting up scalable Docker-based clusters is a privately hosted registry, so we will focus on that approach, but keep an eye on Docker Cloud's developing feature set as it may fill some operational gaps in your clusters that you can defer as you build other parts of your infrastructure.

为了在本地托管注册表，Docker 提供了 Docker 注册表映像(`registry:2`)，您可以将其作为具有各种后端的常规容器运行，包括以下内容:

*   `inmemory`:带有本地内存映射的临时映像存储器。这仅建议用于测试。
*   `filesystem`:使用常规文件系统树存储映像。
*   `s3`、`azure`、`swift`、`oss`、`gcs`:存储后端的云供应商特定实现。

让我们部署一个带有本地文件系统后端的注册表，看看如何使用它。

Warning! The following section does not use TLS-secured or authenticated registry configuration. While this configuration might be acceptable in some rare circumstances in isolated VPCs, generally, you would want to both secure the transport layer with TLS certificates and add some sort of authentication. Luckily, since the API is HTTP-based, you can do most of this with an unsecured registry with a reverse-proxied web server in front of it, like we did earlier with NGINX. Since the certificates need to be "valid" as evaluated by your Docker client and this procedure is different for pretty much every operating system out there, doing the work here would generally not be portable in most configurations, which is why we are skipping it.

```
$ # Make our registry storage folder
$ mkdir registry_storage

$ # Start our registry, mounting the data volume in the container
$ # at the expected location. Use standard port 5000 for it.
$ docker run -d \
 -p 5000:5000 \
 -v $(pwd)/registry_storage:/var/lib/registry \
 --restart=always \
 --name registry \
 registry:2 
19e4edf1acec031a34f8e902198e6615fda1e12fb1862a35442ac9d92b32a637

$ # Pull a test image into our local Docker storage
$ docker pull ubuntu:latest
latest: Pulling from library/ubuntu
<snip>
Digest: sha256:2b9285d3e340ae9d4297f83fed6a9563493945935fc787e98cc32a69f5687641
Status: Downloaded newer image for ubuntu:latest

$ # "Tag our image" by marking it as something that is linked to our local registry
$ # we just started
$ docker tag ubuntu:latest localhost:5000/local-ubuntu-image

$ # Push our ubuntu:latest image into our local registry under "local-ubuntu-image" name
$ docker push localhost:5000/local-ubuntu-image
The push refers to a repository [localhost:5000/local-ubuntu-image]
<snip>
latest: digest: sha256:4b56d10000d71c595e1d4230317b0a18b3c0443b54ac65b9dcd3cac9104dfad2 size: 1357

$ # Verify that our image is in the right location in registry container
$ ls registry_storage/docker/registry/v2/repositories/
local-ubuntu-image

$ # Remove our images from our main Docker storage
$ docker rmi ubuntu:latest localhost:5000/local-ubuntu-image
Untagged: ubuntu:latest
Untagged: localhost:5000/local-ubuntu-image:latest
<snip>

$ # Verify that our Docker Engine doesn't have either our new image
$ # nor ubuntu:latest
$ docker images
REPOSITORY                TAG                 IMAGE ID            CREATED             SIZE

$ # Pull the image from our registry container to verify that our registry works
$ docker pull localhost:5000/local-ubuntu-image
Using default tag: latest
latest: Pulling from local-ubuntu-image
<snip>
Digest: sha256:4b56d10000d71c595e1d4230317b0a18b3c0443b54ac65b9dcd3cac9104dfad2
Status: Downloaded newer image for localhost:5000/local-ubuntu-image:latest

$ # Great! Verify that we have the image.
$ docker images
REPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE
localhost:5000/local-ubuntu-image   latest              8b72bba4485f        23 hours ago        120MB
```

如您所见，使用本地注册表实际上似乎非常容易！这里引入的唯一可能需要一点注册表之外的覆盖的新东西是`--restart=always`，它确保容器在意外退出时自动重启。标记需要将映像与注册表相关联，因此通过执行`docker tag [<source_registry>/]<original_tag_or_id> [<target_registry>/]<new_tag>`，我们可以有效地将新标签分配给现有的映像标签，或者我们可以创建新标签。如这个小代码片段所示，如果没有指定，源和目标都可以用默认为`docker.io` (Docker Hub)的可选存储库位置作为前缀。

可悲的是，从个人经验来看，尽管这个例子让事情看起来非常简单，但是注册表的真正部署绝对不容易，因为外观可能具有欺骗性，在使用它时，您需要记住一些事情:

*   如果您使用不安全的注册表，要从不同的机器访问它，您必须将`"insecure-registries" : ["<ip_or_dns_name>:<port>"]`添加到`/etc/docker/daemon.json`到将使用该注册表映像的每个 Docker 引擎。
    *   注意:出于大量安全原因，不建议使用此配置。
*   如果您使用无效的 HTTPS 证书，您还必须在所有客户端上将它标记为不安全的注册表。

    *   也不建议使用这种配置，因为由于可能的传输降级**中间人(MITM)** 攻击，它只比不安全的注册表略好一点

关于注册表，我最后要给你的建议是，根据我的经验，注册表的云提供商后端文档是出了名的持久(我敢说是故意的吗？)不正确。如果注册表拒绝您的设置，我强烈建议您仔细检查源代码，因为设置正确的变量非常不直观。您也可以使用装载的文件来配置注册表，但是如果您不想在集群刚刚启动时构建新的映像，环境变量是一个不错的选择。环境变量是带有“`_`”分段连接名称的全大写名称，并与可用选项的层次结构相匹配:

```
parent
└─ child_option
 └─ some_setting
```

注册表的这个字段将被设置为`-e PARENT_CHILD_OPTION_SOME_SETTING=<value>`。

For a complete list of the available registry options, you can visit [https://github.com/docker/docker-registry/blob/master/config/config_sample.yml](https://github.com/docker/docker-registry/blob/master/config/config_sample.yml) and see which ones you would need to run your registry. As mentioned earlier, I have found the main documentation on [docs.docker.com](https://docs.docker.com/) and a large percentage of documentation on the code repository itself extremely unreliable for configurations, so don't be afraid to read the source code in order to find out what the registry is actually expecting.

为了帮助那些最有可能在`filesystem`之外，也就是`s3`部署带有后备存储的注册表的人，我会给你们留下一个有效的(在撰写本文时)配置:

```
$ docker run -d \
 -p 5000:5000 \
 -v $(pwd)/registry_storage:/var/lib/registry \
             -e REGISTRY_STORAGE=s3 \
             -e REGISTRY_STORAGE_CACHE_BLOBDESCRIPTOR=inmemory \
 -e REGISTRY_STORAGE_S3_ACCESSKEY=<aws_key_id> \
 -e REGISTRY_STORAGE_S3_BUCKET=<bucket> \
 -e REGISTRY_STORAGE_S3_REGION=<s3_region> \
 -e REGISTRY_STORAGE_S3_SECRETKEY=<aws_key_secret> \
 --restart=always \
 --name registry \
 registry:2
```

```
 --name registry
```

# 底层存储驱动程序

This section may be a bit too advanced for some readers and does not strictly require reading, but in the interest of fully understanding how Docker handles images and what issues you might encounter on large-scale deployments, I would encourage everyone to at least skim through it as the identification of backing-storage driver issues may be of use. Also, be aware that issues mentioned here may not age gracefully as the Docker code base evolves, so check out their website for up-to-date information.

与您对 Docker 守护程序的预期不同，对本地映像层的处理实际上是以非常模块化的方式完成的，因此几乎任何分层文件系统驱动程序都可以插入守护程序。存储驱动程序控制映像在 docker 主机上的存储和检索方式，虽然从客户端的角度来看可能没有任何区别，但每个驱动程序在许多方面都是独特的。

首先，我们将提到的所有可用存储驱动程序都是由 Docker 使用的底层容器化技术提供的，称为`containerd`。尽管对于大多数 Docker 用法来说，知道最后一句话以外的任何东西通常都是多余的，但可以说，这只是 Docker 用作映像处理 API 的底层模块之一。`containerd`为存储和检索映像及其指定的层提供了一个稳定的 API，这样任何构建在其上的软件(如 Docker 和 Kubernetes)都可以不用担心只是将它们捆绑在一起。

You may see references in code and/or documentation about things called graphdrivers, which is pedantically the high-level API that interacts with storage drivers, but in most cases, when it is written, it is used to describe a storage driver that implements the graphdriver API; for example, when a new type of storage driver is talked about, you will often see it referred to as a new graphdriver.

要查看您使用的是哪个支持文件系统，您可以键入`docker info`并查找`Storage Driver`部分:

```
$ docker info
<snip>
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
<snip>
```

Warning! Changing the storage driver will, in most cases, remove access to any and all images and layers from your machine that were stored by the old driver, so proceed with care! Also, I believe that by changing the storage driver without manually cleaning images and containers either through CLI and/or by deleting things from `/var/lib/docker/` will leave those images and containers dangling, so make sure to clean things up if you consider these changes.

如果您想将您的存储驱动程序更改为我们将在此讨论的任何选项，您可以编辑(或在缺少时创建)`/etc/docker/daemon.json`并向其中添加以下内容，之后您应该重新启动 docker 服务:

```
{
  "storage-driver": "driver_name"
}
```

如果`daemon.json`不起作用，也可以尝试通过在`DOCKER_OPTS`上增加一个`-s`标志，重新启动服务，来改变`/etc/default/docker`:

```
DOCKER_OPTS="-s driver_name"
```

In general, Docker is transitioning from `/etc/default/docker` (the path dependent on distribution) to `/etc/docker/daemon.json` as its configuration file, so if you see somewhere on the Internet or other documentation that the former file is referenced, see whether you can find the equivalent configuration for `daemon.json` as I believe that it will fully replace the other one at some point in the future (as with all books, probably under a week after this book gets released).

既然我们知道了什么是存储驱动程序以及如何更改它们，那么我们在这里可以使用哪些选项呢？

# 上啊

`aufs`(也称为`unionfs`)是 Docker 最古老但可能也是最成熟、最稳定的分层文件系统。该存储驱动程序通常启动速度快，并且在存储和内存开销方面效率高。如果你的内核支持这个驱动程序，Docker 将默认支持它，但是一般来说，在 Ubuntu 之外，只有安装了`linux-image-extra-$(uname -r)`包，大多数发行版不会将这个驱动程序添加到他们的内核中，也没有这个驱动程序，所以很可能你的机器无法运行它。您可以下载内核源代码，并在`aufs`支持下重新编译，但一般来说，这是一个维护的噩梦，如果不容易获得，您还不如选择不同的存储驱动程序。您可以使用`grep aufs /proc/filesystems`检查您的机器是否启用了`aufs`内核模块。

注意`aufs`驱动程序只能在`ext4`和`xfs`文件系统上使用。

# btrfs

从概念上来说，这些驱动程序比你在`/var/lib/docker`下挂载的实际文件系统要少，而且每一个都有自己的优缺点。一般来说，与其他一些选项相比，它们都有性能影响，并且内存开销较高，但可能会为您提供更简单的管理工具和/或更高密度的存储。由于这些驱动程序目前得到的支持微乎其微，而且我听说过许多仍然影响它们的关键 bug，所以我不建议在生产中使用它们，除非您有非常好的理由这样做。如果系统在`/var/lib/docker`安装了合适的驱动器，并且相关的内核模块可用，Docker 将在`aufs`之后选择这些模块。

请注意，这里的优先顺序并不意味着这两个存储驱动程序比本节中提到的其他驱动程序更理想，而只是说，如果安装了适当(且不常见)文件系统的驱动程序位于预期的 Docker 位置，Docker 将假设这是用户想要的配置。

# 叠加和叠加 2

这些特定的存储驱动程序正慢慢成为 Docker 安装的最爱。它们与`aufs`非常相似，但实现起来更快、更简单。与`aufs`一样，`overlay`和`overlay2`都需要包含并加载内核覆盖模块，一般来说，内核 3.18 及更高版本应该会提供该模块。此外，两者都只能在`ext4`或`xfs`文件系统上运行。`overlay`和`overlay2`的区别在于，新版本有一些改进，这些改进是在内核 4.0 中添加的，以减少`inode`的使用，但旧版本在该领域的记录更长。如果你有任何疑问，`overlay2`几乎在任何情况下都是坚如磐石的选择。

If you have not worked with inodes before, note that they contain the metadata about each individual file on the filesystem and the maximum count allowed is in most cases hardcoded when the filesystem is created. While this hardcoded maximum is fine for most general usages, there are edge cases where you may run out of them, in which case the filesystem will give you errors on any new file creation even though you will have available space to store the file. If you want to learn more about these structures, you can visit [http://www.linfo.org/inode.html](http://www.linfo.org/inode.html) for more information. Both `overlay` and `overlay2` backing storage driver have been known to cause heavy inode usage due to how they handle file copies internally. While `overlay2` is advertised not to have these issues, I have personally run into inode problems numerous times, with large Docker volumes built with default inode maximums. If you ever use these drivers and notice that the disk is full with messages but you still have space on the device, check your inodes for exhaustion with `df -i` to ensure it is not the docker storage that is causing issues.

# 设备映射器

该驱动程序不在文件级设备上工作，而是直接在 Docker 实例所在的块设备上运行。虽然默认设置通常会设置一个环回设备，并且对于本地测试来说基本上没问题，但是这种特定的设置非常不适合生产系统，因为它会在环回设备中创建稀疏文件。对于生产系统，我们鼓励您将其与`direct-lvm`结合使用，但是这种复杂的设置需要一种比`overlay`存储驱动程序特别棘手和慢的配置，所以我通常不会推荐使用它，除非您无法使用`aufs`或`overlay` / `overlay2`。

# 清理 Docker 存储

如果您使用 Docker 映像和容器，您会注意到，一般来说，Docker 会相对快速地消化您给它的任何存储，因此建议不时进行适当的维护，以确保您不会在主机上出现无用的垃圾或耗尽某些存储驱动程序的信息节点。

# 手动清理

首先是使用`docker rm`清理所有已经运行但忘记使用`--rm`的容器:

```
$ docker rm $(docker ps -aq)
86604ed7bb17
<snip>
7f7178567aba
```

该命令有效地查找所有容器(`docker ps`)，甚至是您停止的容器(`-a`标志)，并且只返回它们的标识(`-q`标志)。这然后被传递给`docker rm`，它将试图一个一个地移除它们。如果有任何容器仍在运行，它会给你一个警告并跳过它们。一般来说，如果您的容器是无状态的或者有一个存储在容器本身之外的状态，这是一件很好的事情。

接下来的事情，虽然可能更具破坏性和更节省空间，是删除你积累的 Docker 映像。如果你的空间问题频繁，手动移除会非常有效。一个很好的经验法则是，任何以`<none>`为标签的映像(也称为悬空)通常可以使用`docker rmi`移除，因为在大多数情况下，它们表明该映像被更新版本的`Dockerfile`所取代:

```
$ docker images --filter "dangling=true"
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
<none>              <none>              873473f192c8        7 days ago          129MB
<snip>
registry            <none>              751f286bc25e        7 weeks ago         33.2MB

$ # Use those image IDs and delete them
$ docker rmi $(docker images -q --filter "dangling=true")
 Deleted: sha256:873473f192c8977716fcf658c1fe0df0429d4faf9c833b7c24ef269cacd140ff
<snip>
Deleted: sha256:2aee30e0a82b1a6b6b36b93800633da378832d623e215be8b4140e8705c4101f
```

# 自动清理

我们刚刚做的所有事情看起来都很痛苦，很难记住，所以 Docker 最近增加了`docker image prune`来帮助这方面的工作。通过使用`docker image prune`，所有悬空映像将通过一个命令被移除:

```
$ docker image prune 
WARNING! This will remove all dangling images.
Are you sure you want to continue? [y/N] y 
Deleted Images:
untagged: ubuntu@sha256:2b9285d3e340ae9d4297f83fed6a9563493945935fc787e98cc32a69f5687641
deleted: sha256:8b72bba4485f1004e8378bc6bc42775f8d4fb851c750c6c0329d3770b3a09086
<snip>
deleted: sha256:f4744c6e9f1f2c5e4cfa52bab35e67231a76ede42889ab12f7b04a908f058551

Total reclaimed space: 188MB
```

If you are intent on cleaning any and all images not tied to containers, you can also run `docker image prune -a`. Given that this command is pretty destructive I would not recommend it in most cases other than maybe running it on Docker slave nodes in clusters on a nighty/weekly timer to reduce space usage.

这里需要注意的是，您可能已经注意到，删除对映像层的所有引用也会级联到子层。

最后但同样重要的是卷清理，可以使用`docker volume`命令进行管理。我建议您在执行此操作时格外小心，以避免删除您可能需要的数据，并且仅使用手动卷选择或`prune`:

```
$ docker volume ls
DRIVER              VOLUME NAME
local               database_volume
local               local_storage
local               swarm_test_database_volume

$ docker volume prune 
WARNING! This will remove all volumes not used by at least one container.
Are you sure you want to continue? [y/N] y 
Deleted Volumes:
local_storage
swarm_test_database_volume
database_volume

Total reclaimed space: 630.5MB
```

作为参考，在我写本章的那一周，我相当轻松地运行了 Docker，删除陈旧的容器、映像和卷已经减少了大约 3 GB 的文件系统使用。虽然这个数字大多是轶事，可能看起来不多，但在具有小实例硬盘的云节点上，以及在添加了持续集成的集群上，保留这些东西会让您比您可能意识到的更快地用完磁盘空间，因此，希望花一些时间手动完成这个过程，或者在`systemd`计时器或`crontab`之类的东西中为您的节点自动化这个过程。

# 持久存储

由于我们已经介绍了临时本地存储，现在我们可以考虑在容器死亡或移动时还有哪些其他选项来保证数据安全。正如我们之前所讨论的，如果一个节点或容器在提供服务时意外死亡(例如您的数据库)，而无法以某种方式将数据从容器保存到外部源，您很可能会丢失其中包含的部分或全部数据，这肯定是我们希望避免的。使用某种形式的容器外部存储您的数据，就像我们在前面几章中对装载卷所做的那样，我们可以开始使集群真正具有弹性，并使在其上运行的容器无状态。

通过使容器无状态化，您获得了信心，不用太担心哪个容器在哪个 Docker Engine 上运行，只要他们能够提取正确的映像并使用正确的参数运行它。如果你想一想，你甚至可能会注意到这种方法与线程有很多相似之处，但都是固步自封的。你可以把 Docker Engine 想象成一个虚拟的 CPU 核心，把每个服务想象成一个进程，把每个任务想象成一个线程。考虑到这一点，如果系统中的所有东西都是无状态的，那么您的集群实际上也是无状态的，因此，您必须利用容器之外的某种形式的数据存储来保证数据的安全。

Caution! Lately, I have noticed a number of sources online that have been recommending that you should keep data through massive replication of services with sharding and clustering of backing databases without persisting data on disk, relying on the cloud provider's distributed availability zones and trusting **Service Level Agreements** (**SLA**) to provide you with resilience and self-healing properties for your cluster. While I would agree that these clusters are somewhat resilient, without some type of permanent physical representation of your data on some type of a volume, you may hit cascade outages on your clusters that will chain before the data is replicated fully and risk losing data with no way to restore it. As a personal advice here, I would highly recommend that at least one node in your stateful services uses storage that is on physical media that is not liable to be wiped when issues arise (e.g. NAS, AWS EBS storage, and so on).

# 节点本地存储

容器外部的这种类型的存储特别适合于将数据从容器实例中分离出来，正如我们所期望的那样，但仅限于部署到同一节点的容器中的可用性。这种存储允许无状态的容器设置，并具有许多面向开发的用途，例如隔离构建和读取配置文件，但对于集群部署来说，这是非常有限的，因为在其他节点上运行的容器将无法访问在原始节点上创建的数据。无论是哪种情况，我们都将在这里介绍所有这些节点本地存储类型，因为大多数大型集群都使用节点本地存储和可重定位存储的某种组合。

# 绑定安装

我们以前见过这些，但也许我们不知道它们是什么。绑定挂载获取一个特定的文件或文件夹，并将其挂载到容器沙箱中的指定位置，中间用`:`隔开。到目前为止，我们使用的一般语法应该类似于以下内容:

```
$ docker run <run_params> \
             -v /path/on/host:/path/on/container \
             <image>...
```

这个功能的更新的 Docker 语法正在成为一个标准，其中`-v`和`--volume`现在被`--mount`取代，所以你也应该习惯这个语法。事实上，从现在开始，我们将尽可能多地使用这两种风格，以便您对这两种风格都感到舒适，但是在编写本书时，`--mount`还没有完全像替代方案那样起作用，因此根据什么有效，什么无效，期待一些互换。

In particular here, at this time, a simple bind mount volume with an absolute path source just does not work with `--mount` style which is almost all the examples we have used so far which is why we have not introduced this form earlier.

尽管如此，与`--volume`不同的是，`--mount`是一个由逗号分隔的参数列表`<key>=<value>`:

*   `type`:坐骑的类型，可以是`bind`、`volume`或者`tmpfs`。
*   `source`:坐骑的来源。
*   `target`:容器中将要装载源的位置的路径。
*   `readonly`:使挂载以只读方式挂载。
*   `volume-opt`:音量的额外选项。可以输入多次。

这是我们用于`--volume`的一个比较版本:

```
$ docker run <run_params> \
             --mount source=/path/on/host,target=/path/on/container \
             <image>...
```

# 只读绑定装载

另一种绑定挂载类型是只读绑定挂载，我们之前并没有真正介绍过。当装载到容器中的数据需要保持只读时，使用此配置，这在从主机将配置文件传递到多个容器中时非常有用。对于两种语法风格，这种挂载卷的形式看起来都有点像这样:

```
$ # Old-style
$ docker run <run_params> \
             -v /path/on/host:/path/on/container:ro \
             <image>...

$ # New-style
$ docker run <run_params> \
             --mount source=/path/on/host,target=/path/on/container,readonly \
             <image>...
```

如前所述，与常规装载相反，只读卷可以为我们提供的是从主机向容器传递配置文件。当 Docker Engine 主机的配置中有影响运行代码的容器(即存储或获取数据的路径前缀、我们在哪个主机上运行、机器从`/etc/resolv.conf`使用什么 DNS 解析器以及许多其他)时，通常会使用这种方法。因此，在大型部署中，它被广泛使用，预计会经常看到。

As a good rule of thumb, unless you explicitly need to write data to a volume, always mount it as read-only to the container. This will prevent the inadvertent opening of security holes from a compromised container spreading onto the other containers and the host itself.

# 命名卷

卷装载的另一种形式是使用命名卷。与绑定装载不同，命名数据卷(通常称为数据卷容器)提供了一种更方便的方法来引用卷，因为它们不依赖于对主机的任何了解。在封面下，它们的工作方式几乎与绑定安装完全相同，但由于使用更简单，它们更容易处理。此外，它们还有一个额外的好处，即能够在容器之间轻松共享，甚至可以由独立于主机的解决方案或完全独立的后端来管理。

Caution! If the named data volume is created by simply running the container, unlike bind-mounts that literally replace all content the container had at that mounted path, the named data volume will copy the content that the container image had at that location into the named data volume when the container launches. This difference is very subtle but can cause serious issues, as you might end up with unexpected content in the volume if you forget about this detail or assume that it behaves the same way as bind-mounts.

现在我们知道了什么是命名数据卷，让我们使用早期配置方法来创建一个(而不是通过直接运行容器来创建):

```
$ # Create our volume
$ docker volume create mongodb_data
mongodb_data

$ docker volume inspect mongodb_data
[
 {
 "Driver": "local",
 "Labels": {},
 "Mountpoint": "/var/lib/docker/volumes/mongodb_data/_data",
 "Name": "mongodb_data",
 "Options": {},
 "Scope": "local"
 }
]

$ # We can start our container now
$ # XXX: For non-bind-mounts, the new "--mount" option
$ #      works fine so we will use it here
$ docker run -d \
             --mount source=mongodb_data,target=/data/db \
             mongo:latest
888a8402d809174d25ac14ba77445c17ab5ed371483c1f38c918a22f3478f25a

$ # Did it work?
$ docker exec -it 888a8402 ls -la /data/db
total 200
drwxr-xr-x 4 mongodb mongodb  4096 Sep 16 14:10 .
drwxr-xr-x 4 root    root     4096 Sep 13 21:18 ..
-rw-r--r-- 1 mongodb mongodb    49 Sep 16 14:08 WiredTiger
<snip>
-rw-r--r-- 1 mongodb mongodb    95 Sep 16 14:08 storage.bson

$ # Stop the container
$ docker stop 888a8402 && docker rm 888a8402
888a8402
888a8402

$ # What does our host's FS have in the
$ # volume storage? (path used is from docker inspect output)
$ sudo ls -la /var/lib/docker/volumes/mongodb_data/_data
total 72
drwxr-xr-x 4  999 docker 4096 Sep 16 09:08 .
drwxr-xr-x 3 root root   4096 Sep 16 09:03 ..
-rw-r--r-- 1  999 docker 4096 Sep 16 09:08 collection-0-6180071043564974707.wt
<snip>
-rw-r--r-- 1  999 docker 4096 Sep 16 09:08 WiredTiger.wt

$ # Remove the new volume
$ docker volume rm mongodb_data
mongodb_data
```

在使用卷之前手动创建卷(使用`docker volume create`)通常是不必要的，但是在这里已经演示了这样做的长形式，但是我们可以作为第一步启动我们的容器，Docker 会自己创建卷:

```
$ # Verify that we don't have any volumes
$ docker volume ls
DRIVER              VOLUME NAME

$ # Run our MongoDB without creating the volume beforehand
$ docker run -d \
             --mount source=mongodb_data,target=/data/db \
             mongo:latest
f73a90585d972407fc21eb841d657e5795d45adc22d7ad27a75f7d5b0bf86f69

$ # Stop and remove our container
$ docker stop f73a9058 && docker rm f73a9058
f73a9058
f73a9058

$ # Check our volumes
$ docker volume ls
DRIVER              VOLUME NAME
local               4182af67f0d2445e8e2289a4c427d0725335b732522989087579677cf937eb53
local               mongodb_data

$ # Remove our new volumes
$ docker volume rm mongodb_data 4182af67f0d2445e8e2289a4c427d0725335b732522989087579677cf937eb53
mongodb_data
4182af67f0d2445e8e2289a4c427d0725335b732522989087579677cf937eb53
```

不过，你可能已经注意到了，我们最终得到了两卷，而不仅仅是我们期望的`mongodb_data`，如果你用这一卷遵循前面的例子，你可能实际上有三卷(一卷命名，两卷随机命名)。这是因为启动的每个容器都将创建`Dockerfile`中定义的所有本地卷，无论您是否命名它们，我们的 MongoDB 映像实际上定义了两个卷:

```
$ # See what volumes Mongo image defines
$ docker inspect mongo:latest | grep -A 3 Volumes
<snip>
            "Volumes": {
                "/data/configdb": {},
                "/data/db": {}
            },
```

我们只给第一个起了个名字，所以`/data/configdb`卷收到了一个随机的名字。要注意这样的事情，如果你不够专注，你可能会遇到空间耗尽的问题。每隔一段时间运行一次`docker volume prune`可以帮助回收该空间，但是要小心该命令，因为它将销毁所有未绑定到容器的卷。

# 可重定位卷

当在单个主机上工作时，我们前面讨论的所有这些选项都很好，但是它们缺少的是不同物理主机之间真正的数据可移植性。例如，当前保持数据持久性的方法实际上可以扩展到但不会超过(没有一些极端的黑客攻击)具有单个 Docker Engine 和共享附加存储的单个物理服务器。这对于功能强大的服务器来说可能没问题，但在真正的集群配置中却开始缺乏任何用处，因为您可能要处理未知数量的服务器、混合的虚拟和物理主机、不同的地理区域等等。

此外，当容器重新启动时，您很可能无法轻松预测它将在哪里启动，以便在启动时将卷后端放在那里。对于这个用例，有一些东西叫做可重定位卷。它们有各种不同的名称，如“共享多主机存储”、“编排数据卷”和许多其他名称，但总体来说，想法几乎是一样的:无论容器走到哪里，都有一个数据卷跟随它。

为了说明这个例子，这里我们有三个主机和两个状态服务，它们都使用相同的可重定位卷存储驱动程序连接:

*   **有状态容器 1** ，带有**主机 1** 上的**卷 D**
*   **有状态** **容器 2** 和**主机 3** 上的**卷 G**

![](img/fc61905b-b24d-4fcd-835e-383d4cd8a330.png)

在本例中，假设**主机 3** 已经死亡。在正常的卷驱动程序情况下，来自**有状态** **容器 2** 的所有数据都将丢失，但因为您将使用可重定位存储:

*   编排平台将通知您的存储驱动程序容器已死亡。
*   编排平台将指示它想要在具有可用资源的主机上重新启动被终止的服务。
*   卷驱动程序会将同一卷装载到运行该服务的新主机上。
*   编排平台将启动服务，将卷详细信息传递到新容器中。

在我们假设的例子中，新的系统状态看起来应该有点像这样:

![](img/daefcd75-6b1e-4dde-ac1e-5b57660c2427.png)

从外部角度可以看到，没有任何变化，数据无缝地转换到新的容器并保持其状态，这正是我们想要的。出于这一特定目的，有许多 Docker 卷驱动程序可供选择，每个驱动程序都有自己的配置方法，适用于各种存储后端，但 Docker 开箱即用的 Azure 和 AWS 预构建映像中唯一包含的驱动程序是 CloudStor，它仅适用于 Docker Swarm，使其具有超特定性和完全不可移植性。

For various reasons, including the age of technology and lackluster support by Docker and plugin developers, having to do this type of volume handling is most likely going to be the part that you sink a lot of time into when building your infrastructure. I do not want to discourage you, but at the time of writing this, the state of things is really dire regardless of what easy tutorials may like you to believe.

你可以在[https://docs . docker . com/engine/extend/legacy _ plugins/# volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins)找到大部分驱动。配置完成后，如果您是在没有编排的情况下手动执行这些操作以管理装载，请按照以下方式使用它们:

```
$ # New-style volume switch (--mount)
$ docker run --mount source=<volume_name>,target=/dest/path,volume-driver=<name> \
             <image>...

$ # Old-style volume switch
$ docker run -v <volume_name>:/dest/path \
             --volume-driver <name> \
             <image>...
```

作为参考，目前，我认为最流行的处理可重定位卷的插件是 Flocker、REX-Ray([https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray))和 GlusterFS，尽管有很多可供选择，其中许多具有类似的功能。如前所述，对于如此重要的功能，这个生态系统的状态相当糟糕，似乎几乎每个运行其集群的大公司要么分叉并构建自己的存储解决方案，要么自己制造并保持封闭来源。一些部署甚至选择对其节点使用标签来完全避免此主题，并强制特定容器转到特定主机，以便它们可以使用本地装载的卷。

Flocker's parent company, ClusterHQ, shut down its operations in December 2016 for financial reasons, and while the lack of support would give a bit of a push to not be mentioned here, it is still the most popular one by an order of magnitude for this type of volume management at the time of writing this book. All the code is open sourced on GitHub at [https://github.com/ClusterHQ](https://github.com/ClusterHQ) so you can build, install, and run it even without official support. If you want to use this plugin in an enterprise environment and would like to have support for it, some of the original developers are available for hire through a new company called ScatterHQ at [https://www.scatterhq.com/](https://www.scatterhq.com/) and they have their own source code repositories at [https://github.com/ScatterHQ](https://github.com/ScatterHQ). GlusterFS is unmaintained in its original source like Flocker, but just like Flocker, you can build, install, and run the full code from the source repository located at [https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs). If you would like code versions that have received updates, you can find a few in the fork network at [https://github.com/calavera/docker-volume-glusterfs/network](https://github.com/calavera/docker-volume-glusterfs/network).

在所有这些生态系统碎片的基础上，这种与 Docker 集成的特殊方式开始被弃用，取而代之的是`docker plugin`系统，该系统将这些插件作为 Docker Hub 的 Docker 映像进行管理和安装，但是由于缺乏这些新型插件的可用性，您可能不得不根据您的特定用例使用传统插件。

Sadly at the time of writing this book, `docker plugin` system is, like many of these features, so new that there are barely any available plugins for it. For example, the only plugin from the ones earlier mentioned in legacy plugins that is built using this new system is REX-Ray but the most popular storage backend (EBS) plugin does not seem to install cleanly. By the time you get to read this book, things will probably have changed here but be aware that there is a significant likelihood that in your own implementation you will be using the tried-and-tested legacy plugins.

因此，在提到所有这些警告后，让我们实际上尝试使用新的`docker plugin install`系统获得唯一可以工作的插件(`sshfs`):

To duplicate this work, you will need access to a secondary machine (though you can run it loopback too) with SSH enabled and reachable from wherever you have Docker Engine running from, since that is the backing storage system that it uses. You will also need the target folder `ssh_movable_volume` made on the device and possibly the addition of `-o odmap=user` to the `sshfs` volume parameters depending on your setup.

```
$ # Install the plugin
$ docker plugin install vieux/sshfs 
Plugin "vieux/sshfs" is requesting the following privileges:
 - network: [host]
 - mount: [/var/lib/docker/plugins/]
 - mount: []
 - device: [/dev/fuse]
 - capabilities: [CAP_SYS_ADMIN]
Do you grant the above permissions? [y/N] y
latest: Pulling from vieux/sshfs
2381f72027fc: Download complete 
Digest: sha256:72c8cfd1a6eb02e6db4928e27705f9b141a2a0d7f4257f069ce8bd813784b558
Status: Downloaded newer image for vieux/sshfs:latest
Installed plugin vieux/sshfs

$ # Sanity check
$ docker plugin ls
ID                  NAME                 DESCRIPTION               ENABLED
0d160591d86f        vieux/sshfs:latest   sshFS plugin for Docker   true

$ # Add our password to a file
$ echo -n '<password>' > password_file

$ # Create a volume backed by sshfs on a remote server with SSH daemon running
$ docker volume create -d vieux/sshfs \
 -o sshcmd=user@192.168.56.101/ssh_movable_volume \
 -o password=$(cat password_file) \
 ssh_movable_volume
ssh_movable_volume

$ # Sanity check
$ docker volume ls
DRIVER               VOLUME NAME
vieux/sshfs:latest   ssh_movable_volume

$ # Time to test it with a container
$ docker run -it \
 --rm \
 --mount source=ssh_movable_volume,target=/my_volume,volume-driver=vieux/sshfs:latest \
 ubuntu:latest \
 /bin/bash

root@75f4d1d2ab8d:/# # Create a dummy file
root@75f4d1d2ab8d:/# echo 'test_content' > /my_volume/test_file

root@75f4d1d2ab8d:/# exit
exit

$ # See that the file is hosted on the remote server
$ ssh user@192.168.56.101
user@192.168.56.101's password: 
<snip>
user@ubuntu:~$ cat ssh_movable_volume/test_file 
test_content

$ # Get back to our Docker Engine host
user@ubuntu:~$ exit
logout
Connection to 192.168.56.101 closed.

$ # Clean up the volume
$ docker volume rm ssh_movable_volume
ssh_movable_volume
```

由于卷的使用方式，该卷大部分都是可移植的，并允许我们使用所需的可重定位功能，尽管大多数其他插件使用在 Docker 之外运行的进程，并在每台主机上并行运行，以便管理卷的装载、卸载和移动，因此这些插件的说明会有很大不同。

# 可重定位卷同步丢失

在这一节中还必须提到的最后一件事是，大多数处理卷移动的插件在任何时候都只能处理连接到单个节点，因为卷可由多个源写入，这通常会导致严重的问题，因此大多数驱动程序不允许这样做。

然而，这与大多数编排引擎的主要特性相冲突，在对 Docker 服务进行更改时，将保持原始服务运行，直到新服务启动并通过运行状况检查，从而导致需要在新旧服务任务上装载相同的卷，这实际上造成了一个“先有鸡还是先有蛋”的悖论。

在大多数情况下，这可以通过确保 Docker 在启动新服务之前完全终止旧服务来解决，但即使这样，您也可以预期，有时旧卷不会从旧节点足够快地卸载，因此新服务将无法启动。

# 卷的 UID/GID 和安全注意事项

这一部分不像我放在其他地方那样放在一个小的信息框中，因为这是一个足够大的问题，也足够有问题，值得拥有自己的部分。要了解容器**用户标识** ( **UID** )和**组标识** ( **GID** )会发生什么，我们需要了解主机的系统权限是如何工作的。当您有一个具有组和用户权限的文件时，它们实际上都在内部映射到数字，而不是作为用户名或组名保存，当您使用常规`ls`开关列出内容时会看到这些用户名或组名:

```
$ # Create a folder and a file that we will mount in the container
$ mkdir /tmp/foo
$ cd /tmp/foo
$ touch foofile

$ # Let's see what we have. Take note of owner and group of the file and directory
$ ls -la
total 0
drwxrwxr-x  2 user user   60 Sep  8 20:20 .
drwxrwxrwt 56 root root 1200 Sep  8 20:20 ..
-rw-rw-r--  1 user user    0 Sep  8 20:20 foofile

$ # See what our current UID and GID are
$ id
uid=1001(user) gid=1001(user) <snip>

$ # How about we see the actual values that the underlying system uses
$  ls -na
total 0
drwxrwxr-x  2 1001 1001   60 Sep  8 20:20 .
drwxrwxrwt 56    0    0 1200 Sep  8 20:20 ..
-rw-rw-r--  1 1001 1001    0 Sep  8 20:20 foofile
```

当您执行`ls`时，系统读入`/etc/passwd`和`/etc/group`来显示权限的实际用户名和组名，这是 UID/GID 映射到权限的唯一方式，但基础值是 UID 和 GID。

正如您可能已经猜到的，这种用户到 UID 和组到 GID 的映射可能(并且通常不会)很好地转换为容器化系统，因为容器将不会有相同的`/etc/passwd`和`/etc/group`文件，但是外部卷上的文件权限与数据一起存储。例如，如果容器有一个 GID 为`1001`的组，它将匹配我们的`foofile`上的组权限位`-rw`，如果它有一个 UID 为`1001`的用户，它将匹配我们的`-rw`文件上的用户权限。相反，如果您的 uid 和 GID 不匹配，即使您在容器和主机上有一个同名的组或用户，您也没有正确的 uid 和 GID 来进行适当的权限处理。是时候看看我们能把这弄得一团糟了:

```
$ ls -la
total 0
drwxrwxr-x  2 user user   60 Sep  8 21:16 .
drwxrwxrwt 57 root root 1220 Sep  8 21:16 ..
-rw-rw-r--  1 user user    0 Sep  8 21:16 foofile 
$ ls -na
total 0
drwxrwxr-x  2 1001 1001   60 Sep  8 21:16 .
drwxrwxrwt 57    0    0 1220 Sep  8 21:16 ..
-rw-rw-r--  1 1001 1001    0 Sep  8 21:16 foofile

$ # Start a container with this volume mounted
$ # Note: We have to use the -v form since at the time of writing this
$ #       you can't mount a bind mount with absolute path :(
$ docker run --rm \
             -it \
             -v $(pwd)/foofile:/tmp/foofile \
             ubuntu:latest /bin/bash

root@d7776ec7b655:/# # What does the container sees as owner/group?
root@d7776ec7b655:/# ls -la /tmp
total 8
drwxrwxrwt 1 root root 4096 Sep  9 02:17 .
drwxr-xr-x 1 root root 4096 Sep  9 02:17 ..
-rw-rw-r-- 1 1001 1001    0 Sep  9 02:16 foofile 
root@d7776ec7b655:/# # Our container doesn't know about our users
root@d7776ec7b655:/# # so it only shows UID/GID 
root@d7776ec7b655:/# # Let's change the owner/group to root (UID 0) and set setuid flag
root@d7776ec7b655:/# chown 0:0 /tmp/foofile 
root@d7776ec7b655:/# chmod +x 4777 /tmp/foofile 

root@d7776ec7b655:/# # See what the permissions look like now in container
root@d7776ec7b655:/# ls -la /tmp
total 8
drwxrwxrwt 1 root root 4096 Sep  9 02:17 .
drwxr-xr-x 1 root root 4096 Sep  9 02:17 ..
-rwsrwxrwx 1 root root    0 Sep  9 02:16 foofile

root@d7776ec7b655:/# # Exit the container
root@d7776ec7b655:/# exit
exit

$ # What does our unmounted volume looks like?
$ ls -la
total 0
drwxrwxr-x  2 user user   60 Sep  8 21:16 .
drwxrwxrwt 57 root root 1220 Sep  8 21:17 ..
-rwsrwxrwx  1 root root    0 Sep  8 21:16 foofile
$ # Our host now has a setuid file! Bad news! 
```

Warning! The ability to set the `setuid` flag on files is a really big security hole that executes the file with the file owner's permissions. If we decided to compile a program and set this flag on it, we could have done a massive amount of damage on the host. Refer to [https://en.wikipedia.org/wiki/Setuid](https://en.wikipedia.org/wiki/Setuid) for more information on this flag.

正如你所看到的，如果我们决定对我们的`setuid`旗帜更加恶意，这可能是一个严重的问题。此问题扩展到我们使用的任何已装载的卷，因此请确保在处理它们时保持适当的谨慎。

Docker has been working on getting user namespaces working in order to avoid some of these security issues, which work by re-mapping the UIDs and GIDs to something else within the container through `/etc/subuid` and `/etc/subgid` files so that there is no `root` UID clashing between the host and the container, but they're not without their problems (and there's plenty of them at the time of writing this book). For more information on using user namespaces, you can find more information at [https://docs.docker.com/engine/security/userns-remap/](https://docs.docker.com/engine/security/userns-remap/).

使这个 UID/GID 问题复杂化的是这种单独的环境中发生的另一个问题:即使您在两个容器之间以相同的顺序安装所有相同的包，由于用户和组通常是按名称创建的，而不是特定的 UID/GID，您不能保证在容器运行之间保持这些一致，如果您想在升级或重建的容器之间重新装载相同的卷，这是一个严重的问题。因此，在为将要处理卷数据的用户和组安装软件包之前，您必须通过执行类似于下面的操作来确保 uid 和 GID 在卷上是稳定的，就像我们在前面的一些示例中所做的那样:

```
RUN groupadd -r -g 910 mongodb && \
 useradd -r -u 910 -g 910 mongodb && \
 mkdir -p /data/db && \
 chown -R mongodb:mongodb /data/db && \
 chmod -R 700 /data/db && \
 apt-get install mongodb-org
```

在这里，我们用 GID `910`创建一个组`mongodb`，用 UID `910`创建一个用户`mongodb`，然后在安装 MongoDB 之前确保我们的数据目录归它所有。通过这样做，当安装`mongodb-org`包时，运行数据库的组和用户已经在那里，并且具有不会改变的确切 UID/GID。有了稳定的 UID/GID，我们可以在任何已构建的容器上装载和重新装载卷，配置相同，因为两个数字都将匹配，并且它应该可以在我们将卷移动到的任何计算机上工作。

唯一需要担心的最后一件事(在最后一个示例中，这也是一个问题)是，挂载文件夹会将其自身覆盖在主机上已经创建的文件夹上，并替换其权限。这意味着，如果您将新文件夹装载到容器上，您必须手动更改卷的权限，或者在容器启动时更改所有权。让我们看看我的意思:

```
$ mkdir /tmp/some_folder
$ ls -la /tmp | grep some_folder
drwxrwxr-x  2 sg   sg        40 Sep  8 21:56 some_folder

$ # Mount this folder to a container and list the content
$ docker run -it \
             --rm \
             -v /tmp/some_folder:/tmp/some_folder \
             ubuntu:latest \
             ls -la /tmp
total 8
drwxrwxrwt 1 root root 4096 Sep  9 02:59 .
drwxr-xr-x 1 root root 4096 Sep  9 02:59 ..
drwxrwxr-x 2 1000 1000   40 Sep  9 02:56 some_folder

$ # Somewhat expected but we will do this now by overlaying
$ # an existing folder (/var/log - root owned) in the container

$ # First a sanity chech
$ docker run -it \
             --rm \
             ubuntu:latest \
             ls -la /var | grep log
drwxr-xr-x 4 root root  4096 Jul 10 18:56 log 
$ # Seems ok but now we mount our folder here
$ docker run -it \
             --rm \
             -v /tmp/some_folder:/var/log \
             ubuntu:latest \
             ls -la /var | grep log
drwxrwxr-x 2 1000  1000   40 Sep  9 02:56 log
```

正如您所看到的，容器内的文件夹上已经设置的任何权限都被我们挂载的目录卷完全践踏了。如前所述，避免有限用户在容器和装载的卷中运行服务时出现权限错误的最佳方法是从包装脚本开始更改容器上装载路径的权限，或者从装载的卷开始容器并手动更改，前者是更好的选择。最简单的包装脚本是这样的:

```
#!/bin/bash -e

# Change owner of volume to the one we expect
chown mongodb:mongodb /path/to/volume

# Optionally you can use this recursive version too
# but in most cases it is a bit heavy-handed
# chown -R mongodb:mongodb /path/to/volume

su - <original limited user> -c '<original cmd invocation>'
```

将它放在容器的`/usr/bin/wrapper.sh`中，并将下面的代码片段添加到`Dockerfile`的某个地方，在那里它作为根运行，应该足以解决这个问题:

```
<snip>
CMD [ "/usr/bin/wrapper.sh" ]
```

当容器启动时，卷将已经装入，脚本将在将命令传递给容器的原始运行程序之前，将卷的用户和组更改为正确的用户和组，从而解决了我们的问题。

本节最大的收获应该是，当您处理卷时，您应该注意用户权限，因为如果您不小心，它们可能会导致可用性和安全性问题。当您开发您的服务和基础设施时，这些类型的陷阱可能会导致从小问题到灾难性故障的一切，但是现在您对它们有了更多的了解，我们有希望防止最坏的情况。

# 摘要

在本章中，您已经学习了大量围绕 Docker 数据处理的新内容，包括 Docker 映像内部和运行您自己的 Docker 注册表。我们还介绍了临时的、节点本地的和可重定位的数据存储，以及在云中有效部署服务所需的相关卷管理。后来，我们花了一些时间来介绍卷编排生态系统，以帮助您了解 Docker 卷驱动程序不断变化的环境，因为这个领域的情况变化很快。当我们到达最后时，包含了对各种陷阱(如 UID/GID 问题)的覆盖，以便您可以在自己的部署中避免它们。

当我们继续进入下一章时，我们将讨论集群强化以及如何以有序的方式在大量服务之间传递数据。