# 构建我们自己的平台

在前面的章节中，我们花了大量时间在基础设施的各个部分上，在这里和那里构建小的独立部分，但是在这一章中，我们将尝试将尽可能多的概念放在一起，构建一个最低限度可行的**平台即服务** ( **PaaS** )。在以下几节中，我们将介绍这些主题:

*   **配置管理** ( **CM** )工装
*   **亚马逊网络服务** ( **AWS** )部署
*   持续集成/持续交付

当我们构建服务的核心时，我们将看到将一个小服务部署到真正的云中需要什么。

这里需要注意的一点是，本章仅作为快速入门和云中真实部署的基本示例提供，因为创建一个完整的带有所有花哨功能的 PaaS 基础架构通常非常复杂，需要大型团队几个月或几年才能解决所有问题。使问题复杂化的是，解决方案通常是专门为在此基础上运行的服务和编排工具的选择而定制的，因此，将您在本章中看到的内容视为当前生态系统的示例，您可以在自己的部署中使用，但其他工具可能更适合您的特定需求。

# 结构管理

对于依赖于大量类似配置的机器(无论是物理机器还是虚拟机器)的每一个系统，总是需要简单易行的重建工具来帮助自动化过去手工完成的大部分任务。在 PaaS 集群的情况下，理想情况下，基础架构的所有部分都能够在用户干预最少的情况下重建到所需的确切状态。在裸机 PaaS 服务器节点的情况下，这一点至关重要，因为您必须手动执行的任何操作都要乘以您拥有的节点数量，因此简化这一过程对于任何类型的生产就绪型集群基础架构都应该是至关重要的。

现在你可能会问自己，“为什么我们关心覆盖 CM 工装？”事实是，如果您的容器基础架构周围没有适当的 CM，您将保证自己在下班后接到紧急电话，原因是各种各样的问题，例如:节点从未加入集群、配置不匹配、未应用的更改、版本不兼容以及许多其他会让您抓狂的问题。因此，为了防止这种情况发生在您身上，我们将真正深入这个支持软件的生态系统。

解释完毕后，我们可以看到一些可供 CM 工具选择的选项:

*   ansi ble([https://www.ansible.com](https://www.ansible.com))
*   木偶([https://puppet.com](https://puppet.com))
*   厨师〔t 0〕https://www .厨师. io/厨师/
*   salt stack(https://salt stack . com
*   其他几个在功能和稳定性方面更弱。

由于 Puppet 和 Chef 都需要基于代理的部署，并且 SaltStack 在 Ansible 中的受欢迎程度相差甚远，因此对于我们在这里的工作，我们将把 Ansible 作为首选的 CM 工具，但是您的需求可能会有所不同。使用您自己的要求来选择最适合这项工作的工具。

As a relevant side note from my interactions with the DevOps online communities, it seems that at the time of writing this material, Ansible is becoming the de facto standard for CM tooling but it is not without its flaws. While I would love to recommend its use everywhere for a myriad of great features, expect complex edge cases of bigger modules to be marginally reliable and keep in mind that most bugs you will find are likely to have been already fixed by an unmerged pull request on GitHub that you might have to apply locally as needed. WARNING! Choice of configuration management tooling should not be taken lightly and you should weigh the pros and cons of each before committing to a single one as this tooling is the hardest to switch out once you have a few machines managed with it! While many IT and DevOps professionals treat this choice almost as a way of life (similar to polarization between `vim` and `emacs` users), make sure that you evaluate your options carefully and logically due to the high costs of switching to a different one down the road. I have personally never heard of a company switch CM tooling after running with one for a while though I am sure there are a few out there.

# Ansible

如果您以前没有使用过 Ansible，它有以下好处:

*   它相对容易使用(基于 YAML/Ninja2)
*   它只需要到目标的 SSH 连接
*   它包含大量可插拔模块来扩展其功能([https://docs . ansi ble . com/ansi ble/latest/modules _ by _ category . html](https://docs.ansible.com/ansible/latest/modules_by_category.html))，其中许多模块都在基本安装中，因此您通常不必担心依赖关系

如果这个列表听起来不够好，那么整个 Ansible 架构都是可扩展的，因此如果没有满足您需求的可用模块，那么它们在某种程度上很容易编写和集成，因此 Ansible 能够容纳您可能拥有或想要构建的几乎任何基础架构。在封面下，Ansible 使用 Python 和 SSH 直接在目标主机上运行命令，但是使用更高级别的**特定于域的语言** ( **DSL** )，这使得某人编写服务器配置变得非常容易和快速，而不是像 Bash 这样直接编写 SSH 命令。

The current Ubuntu LTS version (16.04) comes with Ansible 2.0.0.2, which should be adequate for most purposes, but using versions that are closer to upstream ones is often advised for both bug fixes and for new module additions. If you choose the latter route, make sure to have the version pinned to ensure consistently working deployments.

# 装置

要在大多数基于 Debian 的发行版上安装 Ansible，通常过程非常简单:

```
$ # Make sure we have an accurate view of repositories
$ sudo apt-get update 
<snip>
Fetched 3,971 kB in 22s (176 kB/s) 
Reading package lists... Done

$ # Install the package
$ sudo apt-get install ansible 
Reading package lists... Done
Building dependency tree 
Reading state information... Done
The following NEW packages will be installed:
 ansible
0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.
<snip>
Setting up ansible (2.0.0.2-2ubuntu1) ...

$ # Sanity check
$ ansible --version 
ansible 2.0.0.2
 config file = /home/user/checkout/eos-administration/ansible/ansible.cfg
 configured module search path = /usr/share/ansible
```

# 基础

项目的标准布局通常被分成定义功能片的角色，其余的配置基本上只支持这些角色。Ansible 项目的基本文件结构如下所示(尽管通常需要更复杂的设置):

```
.
├── group_vars
│   └── all
├── hosts
├── named-role-1-server.yml
└── roles
 ├── named-role-1
 │   ├── tasks
 │   │   └── main.yml
 │   ├── files
 │   │   └── ...
 │   ├── templates
 │   │   └── ...
 │   └── vars
 │       └── main.yml
 ...
```

让我们分解这个文件系统树的基本结构，看看每个部分在更大的图中是如何使用的:

*   `group_vars/all`:该文件用于定义用于您所有行动手册的变量。这些可用于具有可变扩展的行动手册和模板(`"{{ variable_name }}"`)。
*   `hosts/`:该文件或目录列出了您想要管理的主机和组，以及任何特定的连接细节，如协议、用户名、SSH 密钥等。在文档中，这个文件通常被称为清单文件。
*   `roles/`:这里有一个角色定义列表，可以以分层的方式应用到目标机器上。通常，它进一步细分为`tasks/`、`files/`、`vars/`，以及每个角色中的其他布局敏感结构:
    *   `<role_name>/tasks/main.yml`:列出作为角色一部分要执行的主要步骤的 YAML 文件。
    *   `<role_name>/files/...`:在这里，您可以添加静态文件，这些文件将被复制到不需要任何预处理的目标机器上。
    *   `<role_name>/templates/...`:在这个目录中，可以添加角色相关任务的模板文件。这些通常包含将通过变量替换复制到目标机器的模板。
    *   `<role_name>/vars/main.yml`:就像父目录暗示的那样，这个 YAML 文件保存角色特定的变量定义。
*   `playbooks/`:在这个目录中，你可以添加所有不适合角色定义的顶级辅助剧本。

# 使用

既然我们已经了解了 Ansible 的外观和工作原理，现在是时候用它做一些实际的事情了。此时，我们要做的是制作一个 Ansible 部署配置，以应用我们在上一章中介绍的一些系统调整，并在运行行动手册后让 Docker 在机器上为我们做好准备。

This example is relatively simple but it should show pretty well the ease of use and power of a decent configuration management tooling. Ansible is also a massive topic that a small section like this just can not cover in as much detail as I would like to but the documentation is relatively good and you can find it at [https://docs.ansible.com/ansible/latest/index.html](https://docs.ansible.com/ansible/latest/index.html). This example (and others) can be found at [https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_8/ansible_deployment](https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_8/ansible_deployment) if you want to skip the manual typing; however, it might be good practice to do it once to get the hang of the Ansible YAML file structure.

首先，我们需要创建保存文件的文件结构。我们将把我们的主要角色称为`swarm_node`，由于我们的整个机器将只是一个群节点，我们将把我们的顶层部署剧本命名为相同的名称:

```
$ # First we create our deployment source folder and move there
$ mkdir ~/ansible_deployment
$ cd ~/ansible_deployment/

$ # Next we create the directories we will need
$ mkdir -p roles/swarm_node/files roles/swarm_node/tasks

$ # Make a few placeholder files
$ touch roles/swarm_node/tasks/main.yml \
        swarm_node.yml \
        hosts

$ # Let's see what we have so far
$ tree
.
├── hosts
├── roles
│   └── swarm_node
│       ├── files
│       └── tasks
│           └── main.yml
└── swarm_node.yml
4 directories, 3 files
```

现在我们在顶层`swarm_node.yml`增加以下内容。这将是 Ansible 的主要入口点，它基本上只是定义了我们希望在其上运行的目标主机和角色:

```
---
- name: Swarm node setup
 hosts: all

 become: True

 roles:
 - swarm_node
```

YAML files are whitespace structured so make sure that you do not omit any spacing when editing this file. In general, all nesting levels are two spaces farther than the parent, key/values are defined with colons, and lists are itemized with a `-` (minus) prefix. For more information, about the YAML structure go to  [https://en.wikipedia.org/wiki/YAML#Syntax](https://en.wikipedia.org/wiki/YAML#Syntax).

我们在这里做的应该是显而易见的:

*   `hosts: all`:在清单文件中定义的所有服务器上运行。一般来说，这只是一个域名，但由于我们只有一个机器目标，`all`应该没问题。
*   `become: True`:由于我们使用 SSH 在目标上运行东西，而 SSH 用户通常不是 root，所以我们需要告诉 Ansible，它需要用`sudo`提升我们将要运行的命令的权限。如果用户需要密码才能使用`sudo`，您可以在调用带有`ansible-playbook -K`标志的剧本时指定，但是我们将在本章后面使用不需要密码的 AWS 实例。
*   `roles: swarm_mode`:这是我们想要应用到目标的角色列表，目前只是一个名为`swarm_node`的列表。该名称*必须与`roles/`中的文件夹名称*匹配。

下一个要定义的是我们的系统调整配置文件，我们在上一章中已经介绍过，比如增加文件描述符最大值、ulimits 和其他一些东西。将以下文件及其各自的内容添加到`roles/swarm_node/files/`文件夹中:

*   `conntrack.conf`:

```
net.netfilter.nf_conntrack_tcp_timeout_established = 43200
net.netfilter.nf_conntrack_max = 524288
```

*   `file-descriptor-increase.conf`:

```
fs.file-max = 1000000
```

*   `socket-buffers.conf`:

```
net.core.optmem_max = 40960
net.core.rmem_default = 16777216
net.core.rmem_max = 16777216
net.core.wmem_default = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 87380 16777216
```

*   `ulimit-open-files-increase.conf`:

```
root soft nofile 65536
root hard nofile 65536
* soft nofile 65536
* hard nofile 65536
```

加上这些，我们的树现在应该看起来更像这样:

```
.
├── hosts
├── roles
│   └── swarm_node
│       ├── files
│       │   ├── conntrack.conf
│       │   ├── file-descriptor-increase.conf
│       │   ├── socket-buffers.conf
│       │   └── ulimit-open-files-increase.conf
│       └── tasks
│           └── main.yml
└── swarm_node.yml
```

大部分文件就绪后，我们现在终于可以进入主配置文件了- `roles/swarm_mode/tasks/main.yml`。在其中，我们将使用 Ansible 的模块和 DSL 逐一安排我们的配置步骤，以便:

*   `apt-get dist-upgrade`形象为安全
*   对机器配置文件进行各种改进，以便作为 Docker 主机更好地运行
*   安装 Docker

为了简化对以下 Ansible 配置代码的理解，最好也记住这个结构，因为它是我们将要使用的每个独立步骤的基础，并且在您看了几次之后非常容易理解:

```
- name: A descriptive step name that shows in output
 module_name:
 module_arg1: arg_value
 module_arg2: arg2_value
 module_array_arg3:
 - arg3_item1
 ...
 ...
```

You can find all module documentation we use in the playbook below at the main Ansible website ([https://docs.ansible.com/ansible/latest/list_of_all_modules.html](https://docs.ansible.com/ansible/latest/list_of_all_modules.html)). We will avoid getting too deep in module documentation here due to the sheer volume of information that will generally be a distraction to the purpose of this section.

You can also find module-specific documentation that we used here too:
- [https://docs.ansible.com/ansible/latest/apt_module.html](https://docs.ansible.com/ansible/latest/apt_module.html)
- [https://docs.ansible.com/ansible/latest/copy_module.html](https://docs.ansible.com/ansible/latest/copy_module.html)
- [https://docs.ansible.com/ansible/latest/lineinfile_module.html](https://docs.ansible.com/ansible/latest/lineinfile_module.html)
- [https://docs.ansible.com/ansible/latest/command_module.html](https://docs.ansible.com/ansible/latest/command_module.html)
- [https://docs.ansible.com/ansible/latest/apt_key_module.html](https://docs.ansible.com/ansible/latest/apt_key_module.html)
- [https://docs.ansible.com/ansible/latest/apt_repository_module.html](https://docs.ansible.com/ansible/latest/apt_repository_module.html) 

让我们看看主安装剧本(`roles/swarm_mode/tasks/main.yml`)应该是什么样子:

```
---
- name: Dist-upgrading the image
 apt:
 upgrade: dist
 force: yes
 update_cache: yes
 cache_valid_time: 3600

- name: Fixing ulimit through limits.d
 copy:
 src: "{{ item }}.conf"
 dest: /etc/security/limits.d/90-{{ item }}.conf
 with_items:
 - ulimit-open-files-increase

- name: Fixing ulimits through pam_limits
 lineinfile:
 dest: /etc/pam.d/common-session
 state: present
 line: "session required pam_limits.so"

- name: Ensuring server-like kernel settings are set
 copy:
 src: "{{ item }}.conf"
 dest: /etc/sysctl.d/10-{{ item }}.conf
 with_items:
 - socket-buffers
 - file-descriptor-increase
 - conntrack

# Bug: https://github.com/systemd/systemd/issues/1113
- name: Working around netfilter loading order
 lineinfile:
 dest: /etc/modules
 state: present
 line: "{{ item }}"
 with_items:
 - nf_conntrack_ipv4
 - nf_conntrack_ipv6

- name: Increasing max connection buckets
 command: echo '131072' > /sys/module/nf_conntrack/parameters/hashsize

# Install Docker
- name: Fetching Docker's GPG key
 apt_key:
 keyserver: hkp://pool.sks-keyservers.net
 id: 58118E89F3A912897C070ADBF76221572C52609D

- name: Adding Docker apt repository
 apt_repository:
 repo: 'deb https://apt.dockerproject.org/repo {{ ansible_distribution | lower }}-{{ ansible_distribution_release | lower }} main'
 state: present

- name: Installing Docker
 apt:
 name: docker-engine
 state: installed
 update_cache: yes
 cache_valid_time: 3600
```

WARNING! This configuration has *no* hardening for the image to be comfortably placed on the internet live so use care and add whatever securing steps and tooling you require into this playbook before doing your real deployment. At the absolute least I would suggest installing the `fail2ban` package but you may have alternative strategies (e.g. seccomp, grsecurity, AppArmor, etc).

在这个文件中，我们通过使用一些核心 Ansible 模块和我们之前创建的配置文件，按顺序一个接一个地将机器从基础配置到完全能够运行 Docker 容器的系统。有一点可能不是很明显，那就是我们使用了`{{ ansible_distribution | lower }}`类型的变量，但是在这些变量中，我们使用了关于我们正在运行的系统的 ansi ble facts(https://docs . ansi ble . com/ansi ble/latest/playbooks _ variables . html)并通过 Ninja2 `lower()`过滤器传递它们，以确保变量是小写的。通过为存储库端点这样做，我们可以在几乎任何基于 deb 的服务器目标上使用相同的配置，而不会有太多麻烦，因为变量将被替换为适当的值。

此时，我们需要做的唯一一件事就是将我们的服务器 IP/DNS 添加到`hosts`文件中，并用`ansible-playbook <options> swarm_node.yml`运行剧本。但是由于我们想在亚马逊基础设施上运行这个，我们将在这里停下来，看看我们如何采取这些配置步骤，并从它们创建一个**亚马逊机器映像** ( **AMI** )在其上我们可以启动任意数量的**弹性计算云** ( **EC2** )实例，这些实例是相同的并且已经被完全配置。

# 亚马逊网络服务设置

为了继续我们的亚马逊机器映像(AMI)构建部分，我们不能没有一个工作的 AWS 帐户和一个相关的 API 密钥，所以我们将首先这样做，然后继续进一步。为了避免歧义，请注意，几乎所有的 AWS 服务都需要花钱才能使用，并且您对应用编程接口的使用可能会为您带来费用，即使是您可能不容易预料到的事情(即带宽使用、AMI 快照存储等)，因此请相应地使用它。

AWS is a massively complex piece of machinery, exponentially more than Ansible, and covering everything that you might need to know about it is impossible to do within the scope of this book. But we will try here to provide you with enough relevant instructions for you to have a place to start from. If you decide you want to learn more about AWS, their documentation is generally pretty great and you can find it at [https://aws.amazon.com/documentation/](https://aws.amazon.com/documentation/).

# 创建帐户

虽然这个过程非常简单，但它已经以非常重要的方式改变了几次，所以在这里详细说明整个过程，而没有办法更新它，最终会对您不利，所以要创建帐户，我将引导您到包含如何操作的最新信息的链接，即[https://AWS . Amazon . com/premium support/knowledge-center/create-and-activate-AWS-account/](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/)。一般来说，流程在[https://aws.amazon.com/](https://aws.amazon.com/)开始，您可以点击屏幕右上角的黄色“注册”或“创建 AWS 帐户”按钮，并按照说明进行操作:

![](img/ba741a7d-9ba5-44c8-81ab-39f166142b2a.png)

# 获取应用编程接口密钥

创建 AWS 帐户后，我们现在需要获取 API 密钥，以便通过我们想要使用的各种工具来访问和使用我们的资源:

1.  通过转到`https://<account_id or alias>.signin.aws.amazon.com/console`登录到您的控制台。请注意，如果在注册帐户时没有创建用户，您可能需要首先以根帐户身份登录(登录按钮下方的蓝色小链接，如下图所示):

![](img/e647ee3d-b822-41f8-ac38-cc6d47fc735b.png)

2.  导航至位于[https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)的 IAM 页面，点击屏幕左侧的用户链接。

3.  单击添加用户开始用户创建过程。

![](img/a9a05e9a-effc-4d22-aadc-f38fe919310f.png)

CAUTION! Make sure that the Programmatic access checkbox is ticked, or else your AWS API keys will not work for our examples.

4.  对于权限，我们将授予该用户完全管理员访问权限。对于生产服务，您将希望将其限制在所需的访问级别:

![](img/37d4b7e7-4f0d-4714-ad38-a0f712635762.png)

5.  遵循向导的其余部分，记录密钥标识和密钥秘密，因为这些将是您用于 AWS 的 API 凭据:

![](img/097f8cab-2eb9-4f41-8ac8-47d3cb00a7cb.png)

# 使用应用编程接口密钥

为了以最简单的方式使用 API 键，您可以导出 shell 中的变量，这些变量将被工具获取；但是，您需要在使用自动气象站应用程序接口的每个终端上这样做:

```
$ export AWS_ACCESS_KEY_ID="AKIABCDEFABCDEF"
$ export AWS_SECRET_ACCESS_KEY="123456789ABCDEF123456789ABCDEF"
$ export AWS_REGION="us-west-1"
```

或者，如果你安装了`awscli`工具(`sudo apt-get install awscli`)，你可以直接运行`aws configure`:

```
$ aws configure
AWS Access Key ID [None]: AKIABCDEFABCEF
AWS Secret Access Key [None]: 123456789ABCDEF123456789ABCDEF
Default region name [None]: us-west-1
Default output format [None]: json
```

还有许多其他方法可以通过配置文件等设置您的凭据，但这实际上完全取决于您预期的使用情况。有关这些选项的更多信息，您可以参考[https://docs . AWS . Amazon . com/CLI/latest/user guide/CLI-chap-入门. html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) 上的官方文档。

因此，有了可供命令行界面使用的密钥和配置，我们现在可以使用 Packer 构建定制的 AMI 映像了。

# 哈希公司包装商

正如我们之前所暗示的，如果我们每次都必须在添加到集群或云基础架构的新机器上运行 CM 脚本，那么我们的 CM 脚本就不是那么理想了。虽然我们可以做到这一点，但我们真的不应该这样做，因为在一个完美的世界中，集群节点应该是一个灵活的组，它可以根据使用情况生成和删除实例，用户干预最少，因此即使在最小的集群规模下，要求手动设置每台新机器也是站不住脚的。通过创建 AMI 映像，我们可以在制作映像时用 Ansible 预烘焙一次模板化的基本系统映像。通过这样做，我们可以用这个相同的映像启动任何新的机器，并且我们与正在运行的系统的交互将保持在最低限度，因为理想情况下一切都已经配置好了。

为了制作这些机器映像，HashiCorp Packer([https://www.packer.io/](https://www.packer.io/))允许我们通过应用我们选择的 CM 工具(Ansible)的预配运行并为任何大型云提供商输出一个现成的映像来完成这一任务。通过这样做，您可以将集群节点(或任何其他服务器配置)的所需状态永久保存在映像中，对于集群的任何节点添加需求，您需要做的就是基于相同的 Packer 映像生成更多的虚拟机实例。

# 装置

由于 Packer 是用 Go 编程语言编写的，要安装 Packer，只需要从他们位于[https://www.packer.io/downloads.html](https://www.packer.io/downloads.html)的网站下载二进制文件。为了快速安装，您通常可以执行如下操作:

```
$ # Get the archive
$ wget -q --show-progress https://releases.hashicorp.com/packer/1.1.1/packer_<release>.zip
packer_<release>.zip 100%[==============================================>] 15.80M 316KB/s in 40s

$ # Extract our binary
$ unzip packer_<release>.zip
Archive: packer_<release>.zip
 inflating: packer

$ # Place the binary somewhere in your path
$ sudo mv packer /usr/local/bin/

$ packer --version
1.1.1
```

CAUTION! Packer binaries only provide TLS authentication for their runner without any form of signature checking, so the assurance that the program was published by HashiCorp itself is orders of magnitude lower than a GPG-signed `apt` repository that Docker uses; so, exercise extra care when getting it this way or build it from source ([https://github.com/hashicorp/packer](https://github.com/hashicorp/packer)).

# 使用

使用 Packer 实际上相对容易，因为在大多数情况下，您只需要 Ansible 设置代码和一个相对较小的`packer.json`文件。将此内容添加到前面章节中我们的 Ansible 部署配置中的`packer.json`中:

```
{
  "builders": [
    {
      "ami_description": "Cluster Node Image",
      "ami_name": "cluster-node",
      "associate_public_ip_address": true,
      "force_delete_snapshot": true,
      "force_deregister": true,
      "instance_type": "m3.medium",
      "region": "us-west-1",
      "source_ami": "ami-1c1d217c",
      "ssh_username": "ubuntu",
      "type": "amazon-ebs"
    }
  ],
  "provisioners": [
    {
      "inline": "sudo apt-get update && sudo apt-get install -y ansible",
      "type": "shell"
    },
    {
      "playbook_dir": ".",
      "playbook_file": "swarm_node.yml",
      "type": "ansible-local"
    }
  ]
}
```

如果不明显，我们在这个配置文件中有`provisioners`和`builders`部分，它们通常分别对应于 Packer 输入和输出。在前面的示例中，我们首先通过`shell`资源调配器安装 Ansible，因为下一步需要它，然后在基本 AMI 上使用`ansible-local`资源调配器从当前目录运行`main.yml`剧本。应用所有更改后，我们将结果保存为新的**弹性块存储** ( **EBS** )优化的 AMI 图像。

AWS **Elastic Block Store** (**EBS**) is a service that provides block device storage to EC2 instances (these instances are basically just VMs). To the machine, these look like regular hard disks and can be formatted to whatever filesystem you want and are used to persist data in a permanent manner in the Amazon Cloud. They have configurable size and levels of performance; however, as you might expect, the price goes up as those two settings increase. The only other thing to keep in mind is that while you can move the drive around EC2 instances just like you would move a physical disk, you cannot move an EBS volume across availability zones. A simple workaround is to copy the data over. "AMI image" phrase expands into "Amazon Machine Image image", which is a really quirky way to phrase things, but just like the sister phrase "PIN number", it flows much better when used that way and will be intentionally referred to in that way in this section. If you're curious about this particularity of the English language, you should peruse the Wiki page for RAS syndrome at [https://en.wikipedia.org/wiki/RAS_syndrome](https://en.wikipedia.org/wiki/RAS_syndrome).

对于构建器部分，更详细地解释一些参数将会有所帮助，因为它们在阅读 JSON 文件时可能并不明显:

```
- type: What type of image are we building (EBS-optimized one in our case).
- region: What region will this AMI build in.
- source_ami: What is our base AMI? See section below for more info on this.
- instance_type: Type of instance to use when building the AMI - bigger machine == faster builds.
- ami_name: Name of the AMI that will appear in the UI.
- ami_description: Description for the AMI.
- ssh_username: What username to use to connect to base AMI. For Ubuntu, this is usually "ubuntu".
- associate_public_ip_address: Do we want this builder to have an external IP. Usually this needs to be true.
- force_delete_snapshot: Do we want to delete the old block device snapshot if same AMI is rebuilt?
- force_deregister: Do we want to replace the old AMI when rebuilding?
```

You can find more information on this particular builder type and its available options at [https://www.packer.io/docs/builders/amazon-ebs.html](https://www.packer.io/docs/builders/amazon-ebs.html).

# 选择正确的急性心肌梗死基础图像

与我们在前面章节中介绍的选择基本 Docker 图像进行扩展不同，选择正确的 AMI 来使用 Packer 并不是一件简单的事情。有些发行版会定期更新，所以标识会改变。每个 AWS 区域的标识也是唯一的，您可能需要硬件或半虚拟化(`HVM` vs `PV`)。除此之外，在撰写本书时，您还必须根据您的存储需求选择合适的存储设备(`instance-store`、`ebs`和`ebs-ssd`，从而创建一个绝对不直观的选项矩阵。

如果您没有使用过亚马逊**弹性计算云** ( **EC2** )和 EBS，存储选项对新手来说有点混乱，但它们的含义如下:

*   `instance-store`:这种类型的存储对于正在运行的 EC2 虚拟机来说是本地的，其空间因虚拟机类型而异(尽管通常很少)，并且在虚拟机终止时被完全丢弃(尽管停止或重新启动的虚拟机保持其状态)。实例存储非常适合不需要保留任何状态的节点，但不应用于希望保留数据的计算机；但是，如果您希望拥有持久存储并利用无状态存储，您可以将单独的 EBS 驱动器独立装载到实例存储虚拟机。
*   `ebs`:这种存储类型会在 EC2 实例使用此特定映像启动时，创建并关联一个 EBS 卷，该卷由较旧的磁旋转硬盘(相对较慢的固态硬盘)支持，因此数据始终保留在周围。如果您想保留数据或者`instance-store`卷不够大，这个选项是很好的。然而，从今天开始，这个选项被积极地否决了，所以它很可能会在未来消失。
*   `ebs-ssd`:该选项与前面的选项基本相同，但使用速度更快但每千兆字节分配成本更高的**固态设备**(固态硬盘)作为后备存储。

我们需要选择的另一件事是虚拟化类型:

*   半虚拟化/ `pv`:这种类型的虚拟化比较老，使用软件来链式加载您的映像，因此它能够在更加多样化的硬件上运行。虽然很久以前速度更快，但现在通常比硬件虚拟化慢。
*   硬件虚拟化/ `hvm`:这种类型的虚拟化使用 CPU 级指令在完全隔离的环境中运行您的映像，类似于直接在裸机硬件上运行映像。虽然它取决于特定的英特尔虚拟处理器技术实施，但它的性能通常比`pv`虚拟化好得多，因此在大多数情况下，您可能应该将其用于其他选项，尤其是在您不确定选择哪一个的情况下。

随着我们对可用选项的新了解，我们现在可以找出我们将使用什么图像作为基础。对于我们指定的操作系统版本(Ubuntu LTS)，您可以使用位于[https://cloud-images.ubuntu.com/locator/ec2/](https://cloud-images.ubuntu.com/locator/ec2/)的助手页面来找到正确的版本:

![](img/f511a6d3-c407-4306-9ed9-96b2001ee9e9.png)

对于我们的测试构建，我们将使用`us-west-1`区域、Ubuntu 16.04 LTS 版本(`xenial`)、64 位架构(`amd64`)、`hvm`虚拟化和`ebs-ssd`存储，因此我们可以使用页面底部的过滤器来缩小范围:

![](img/eec87b2e-beb9-4914-8734-b2fa6cb52c4c.png)

如您所见，列表折叠成一个选项，在我们的`packer.json`中，我们将使用`ami-1c1d217c`。

Since this list is updated with AMIs that have newer security patches, it is very likely that by the time you are reading this section the AMI ID will be something else on your end. Because of that, do not be alarmed if you see discrepancies between values we have found here and what you have available to you while reading of this chapter.

# 构建 AMI

WARNING! Running this Packer build will for sure incur some (albeit barely a couple of US dollars at the time of writing this book) charges on your AWS account due to usage of non-free instance type, snapshot use, and AMI use, some possibly recurring. Refer to the pricing documentation of AWS for those services to estimate the amount that you will be charged. As an additional note, it is also good practice to clean up everything either from the console or CLI after you finish working with AWS objects that will not be kept around since it will ensure that you do not get additional charges after working with this code. With the `packer.json` in place, we can now do a build of our image. We will first install the pre-requisites (`python-boto` and `awscli`), then check the access, and finally build our AMI:

```
$ # Install python-boto as it is a prerequisite for Amazon builders
$ # Also get awscli to check if credentials have been set correctly
$ sudo apt-get update && sudo apt-get install -y python-boto awscli
<snip>

$ # Check that AWS API credentials are properly set. 
$ # If you see errors, consult the previous section on how to do this
$ aws ec2 describe-volumes 
{
 "Volumes": [
 ]
}

$ # Go to the proper directory if we are not in it
$ cd ~/ansible_deployment

$ # Build our AMI and use standardized output format
$ packer build -machine-readable packer.json 
<snip>
1509439711,,ui,say,==> amazon-ebs: Provisioning with shell script: /tmp/packer-shell105349087
<snip>
1509439739,,ui,message, amazon-ebs: Setting up ansible (2.0.0.2-2ubuntu1) ...
1509439741,,ui,message, amazon-ebs: Setting up python-selinux (2.4-3build2) ...
1509439744,,ui,say,==> amazon-ebs: Provisioning with Ansible...
1509439744,,ui,message, amazon-ebs: Uploading Playbook directory to Ansible staging directory...
<snip>
1509439836,,ui,message, amazon-ebs: TASK [swarm_node : Installing Docker] ******************************************
1509439855,,ui,message, amazon-ebs: [0;33mchanged: [127.0.0.1][0m
1509439855,,ui,message, amazon-ebs:
1509439855,,ui,message, amazon-ebs: PLAY RECAP *********************************************************************
1509439855,,ui,message, amazon-ebs: [0;33m127.0.0.1[0m : [0;32mok[0m[0;32m=[0m[0;32m10[0m [0;33mchanged[0m[0;33m=[0m[0;33m9[0m unreachable=0 failed=0
1509439855,,ui,message, amazon-ebs:
1509439855,,ui,say,==> amazon-ebs: Stopping the source instance...
<snip>
1509439970,,ui,say,Build 'amazon-ebs' finished.
1509439970,,ui,say,--> amazon-ebs: AMIs were created:\nus-west-1: ami-a694a8c6\n
```

成功！有了这个新的图像 ID，你可以在输出的最后看到(`ami-a694a8c6`)，我们现在可以用这个 AMI 在 EC2 中启动实例，它们将拥有我们已经应用的所有调整以及预安装的 Docker！

# 部署到 aws

由于只有裸映像，没有虚拟机运行它们，我们之前的 Packer 工作还没有让我们完全进入自动化工作状态。为了真正做到这一点，我们现在需要用更多的粘合剂将所有东西绑在一起，以完成部署。不同阶段的封装层次结构在概念上应该如下所示:

![](img/fb5786d9-1667-4f2e-ad80-13b41a517013.png)

从图中可以看出，我们将采用分层的部署方法:

*   在最内层，我们有 Ansible 脚本将一台裸机、虚拟机或一个 AMI 带到我们希望它处于的配置状态。
*   Packer 封装了这一过程，并生成静态的 AMI 图像，这些图像可在亚马逊 EC2 云产品上进一步使用。
*   Ansible 最后通过用那些静态的、Packer 创建的映像部署机器来封装前面提到的一切。

# 自动化基础架构部署之路

既然我们知道自己想要什么，我们该怎么做？对我们来说幸运的是，正如前面列表中所暗示的，Ansible 可以为我们完成这一部分；我们只需要编写几个配置文件。但是 AWS 在这里非常复杂，所以它不会像仅仅启动一个实例那么简单，因为我们想要一个隔离的 VPC 环境。然而，由于我们将只管理一台服务器，我们并不真正关心 VPC 内部的网络，所以这将使事情变得简单一点。

我们首先需要考虑需要采取的所有步骤。其中一些对大多数人来说是非常陌生的，因为 AWS 非常复杂，大多数开发人员通常不会在网络上工作，但它们是在不破坏您的帐户默认设置的情况下获得孤立的 VPC 的最基本的必要步骤:

*   为特定的虚拟网络设置 VPC。
*   创建并绑定一个子网。没有这个，我们的机器将无法在上面使用网络。
*   设置一个虚拟互联网网关，并将其连接到 VPC，以查找带有路由表的无法解析的地址。如果我们不这样做，机器将无法使用互联网。
*   设置一个安全组(防火墙)白名单的端口，我们希望能够访问我们的服务器(SSH 和 HTTP 端口)。默认情况下，所有端口都被阻止，这样可以确保启动的实例是可访问的。
*   最后，使用为网络配置的 VPC 配置虚拟机实例。

为了推倒一切，我们需要做同样的事情，但只是反过来。

首先，我们需要一些将在部署和拆卸行动手册中共享的变量。在与本章中我们一直在研究的大型 Ansible 示例相同的目录中创建一个`group_vars/all`文件:

```
# Region that will accompany all AWS-related module usages
aws_region: us-west-1

# ID of our Packer-built AMI
cluster_node_ami: ami-a694a8c6

# Key name that will be used to manage the instances. Do not
# worry about what this is right now - we will create it in a bit
ssh_key_name: swarm_key

# Define the internal IP network for the VPC
swarm_vpc_cidr: "172.31.0.0/16"
```

现在我们可以在`packer.json`所在的同一个目录中编写`deploy.yml`，使用其中的一些变量:

The difficulties of this deployment is starting to scale up significantly from our previous examples and there is no good way to cover all the information that is spread between dozens of AWS, networking, and Ansible topics to describe it in a concise way, but here are some links to the modules we will use that, if possible, you should read before proceeding:
- [https://docs.ansible.com/ansible/latest/ec2_vpc_net_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_net_module.html)
- [https://docs.ansible.com/ansible/latest/set_fact_module.html](https://docs.ansible.com/ansible/latest/set_fact_module.html)
- [https://docs.ansible.com/ansible/latest/ec2_vpc_subnet_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_subnet_module.html)
- [https://docs.ansible.com/ansible/latest/ec2_vpc_igw_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_igw_module.html)
- [https://docs.ansible.com/ansible/latest/ec2_vpc_route_table_module.html](https://docs.ansible.com/ansible/latest/ec2_vpc_route_table_module.html)
- [https://docs.ansible.com/ansible/latest/ec2_group_module.html](https://docs.ansible.com/ansible/latest/ec2_group_module.html)
- [https://docs.ansible.com/ansible/latest/ec2_module.html](https://docs.ansible.com/ansible/latest/ec2_module.html)

```
- hosts: localhost
 connection: local
 gather_facts: False

 tasks:
 - name: Setting up VPC
 ec2_vpc_net:
 region: "{{ aws_region }}"
 name: "Swarm VPC"
 cidr_block: "{{ swarm_vpc_cidr }}"
 register: swarm_vpc

 - set_fact:
 vpc: "{{ swarm_vpc.vpc }}"

 - name: Setting up the subnet tied to the VPC
 ec2_vpc_subnet:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc.id }}"
 cidr: "{{ swarm_vpc_cidr }}"
 resource_tags:
 Name: "Swarm subnet"
 register: swarm_subnet

 - name: Setting up the gateway for the VPC
 ec2_vpc_igw:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc.id }}"
 register: swarm_gateway

 - name: Setting up routing table for the VPC network
 ec2_vpc_route_table:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc.id }}"
 lookup: tag
 tags:
 Name: "Swarm Routing Table"
 subnets:
 - "{{ swarm_subnet.subnet.id }}"
 routes:
 - dest: 0.0.0.0/0
 gateway_id: "{{ swarm_gateway.gateway_id }}"

 - name: Setting up security group / firewall
 ec2_group:
 region: "{{ aws_region }}"
 name: "Swarm SG"
 description: "Security group for the swarm"
 vpc_id: "{{ vpc.id }}"
 rules:
 - cidr_ip: 0.0.0.0/0
 proto: tcp
 from_port: 22
 to_port: 22
 - cidr_ip: 0.0.0.0/0
 proto: tcp
 from_port: 80
 to_port: 80
 rules_egress:
 - cidr_ip: 0.0.0.0/0
 proto: all
 register: swarm_sg

 - name: Provisioning cluster node
 ec2:
 region: "{{ aws_region }}"
 image: "{{ cluster_node_ami }}"
 key_name: "{{ ssh_key_name }}"
 instance_type: "t2.medium"
 group_id: "{{ swarm_sg.group_id }}"
 vpc_subnet_id: "{{ swarm_subnet.subnet.id }}"
 source_dest_check: no
 assign_public_ip: yes
 monitoring: no
 instance_tags:
 Name: cluster-node
 wait: yes
 wait_timeout: 500
```

我们在这里所做的与我们之前的计划非常匹配，但是现在我们有了具体的部署代码来匹配它:

1.  我们用`ec2_vpc_net`模块设置了 VPC。

2.  我们创建我们的子网，并将其与`ec2_vpc_subnet`模块关联到 VPC。

3.  我们云的互联网虚拟网关是用`ec2_vpc_igw`创建的。
4.  然后，互联网网关将解析不在同一网络中的任何地址。
5.  `ec2_group`模块用于启用入口和出口网络，但仅允许端口`22` (SSH)和端口`80` (HTTP)进入。
6.  最后，我们的 EC2 实例是在新配置的 VPC 中用`ec2`模块创建的。

正如我们前面提到的，拆除应该非常相似，但方向相反，包含更多的`state: absent`参数。让我们将以下内容放在同一个文件夹中的`destroy.yml`中:

```
- hosts: localhost
 connection: local
 gather_facts: False

 tasks:
 - name: Finding VMs to delete
 ec2_remote_facts:
 region: "{{ aws_region }}"
 filters:
 "tag:Name": "cluster-node"
 register: deletable_instances

 - name: Deleting instances
 ec2:
 region: "{{ aws_region }}"
 instance_ids: "{{ item.id }}"
 state: absent
 wait: yes
 wait_timeout: 600
 with_items: "{{ deletable_instances.instances }}"
 when: deletable_instances is defined

 # v2.0.0.2 doesn't have ec2_vpc_net_facts so we have to fake it to get VPC info
 - name: Finding route table info
 ec2_vpc_route_table_facts:
 region: "{{ aws_region }}"
 filters:
 "tag:Name": "Swarm Routing Table"
 register: swarm_route_table

 - set_fact:
 vpc: "{{ swarm_route_table.route_tables[0].vpc_id }}"
 when: swarm_route_table.route_tables | length > 0

 - name: Removing security group
 ec2_group:
 region: "{{ aws_region }}"
 name: "Swarm SG"
 state: absent
 description: ""
 vpc_id: "{{ vpc }}"
 when: vpc is defined

 - name: Deleting gateway
 ec2_vpc_igw:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc }}"
 state: absent
 when: vpc is defined

 - name: Deleting subnet
 ec2_vpc_subnet:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc }}"
 cidr: "{{ swarm_vpc_cidr }}"
 state: absent
 when: vpc is defined

 - name: Deleting route table
 ec2_vpc_route_table:
 region: "{{ aws_region }}"
 vpc_id: "{{ vpc }}"
 state: absent
 lookup: tag
 tags:
 Name: "Swarm Routing Table"
 when: vpc is defined

 - name: Deleting VPC
 ec2_vpc_net:
 region: "{{ aws_region }}"
 name: "Swarm VPC"
 cidr_block: "{{ swarm_vpc_cidr }}"
 state: absent
```

如果部署行动手册是可读的，那么这个行动手册通常应该很容易理解，正如我们提到的，它只是反向运行相同的步骤，删除我们已经创建的任何基础架构部分。

# 运行部署和拆卸行动手册

如果您还记得，在我们的`group_vars`定义的早期，我们有一个关键变量(`ssh_key_name: swarm_key`)，在这一点上变得相对重要，因为没有工作密钥，我们既不能部署也不能启动我们的虚拟机，所以让我们现在就这样做。我们将使用`awscli`和`jq`，这是一个 JSON 解析工具，可以减少我们的工作量，但是也可以通过图形用户界面控制台不使用它:

```
$ # Create the key with AWS API and save the private key to ~/.ssh directory
$ aws ec2 create-key-pair --region us-west-1 \
 --key-name swarm_key | jq -r '.KeyMaterial' > ~/.ssh/ec2_swarm_key

$ # Check that its not empty by checking the header
$ head -1 ~/.ssh/ec2_swarm_key 
-----BEGIN RSA PRIVATE KEY-----

$ # Make sure that the permissions are correct on it
$ chmod 600 ~/.ssh/ec2_swarm_key

$ # Do a sanity check that it has the right size and permissions
$ ls -la ~/.ssh/ec2_swarm_key
-rw------- 1 sg sg 1671 Oct 31 16:52 /home/sg/.ssh/ec2_swarm_key
```

有了密钥，我们终于可以运行部署脚本了:

```
$ ansible-playbook deploy.yml 
 [WARNING]: provided hosts list is empty, only localhost is available

PLAY ***************************************************************************

TASK [Setting up VPC] **********************************************************
ok: [localhost]

TASK [set_fact] ****************************************************************
ok: [localhost]

TASK [Setting up the subnet] ***************************************************
ok: [localhost]

TASK [Setting up the gateway] **************************************************
ok: [localhost]

TASK [Setting up routing table] ************************************************
ok: [localhost]

TASK [Setting up security group] ***********************************************
ok: [localhost]

TASK [Provisioning cluster node] ***********************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost : ok=7 changed=1 unreachable=0 failed=0 

$ # Great! It looks like it deployed the machine! 
$ # Let's see what we have. First we need to figure out what the external IP is
$ aws ec2 describe-instances --region us-west-1 \
 --filters Name=instance-state-name,Values=running \
 --query 'Reservations[*].Instances[*].PublicIpAddress'
[
 [
 "52.53.240.17"
 ]
]

$ # Now let's try connecting to it
ssh -i ~/.ssh/ec2_swarm_key ubuntu@52.53.240.17 
<snip>
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '52.53.240.17' (ECDSA) to the list of known hosts.
<snip>

ubuntu@ip-172-31-182-20:~$ # Yay! Do we have Docker?
ubuntu@ip-172-31-182-20:~$ sudo docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

ubuntu@ip-172-31-182-20:~$ # Create our single-server swarm
ubuntu@ip-172-31-182-20:~$ sudo docker swarm init
Swarm initialized: current node (n2yc2tedm607rvnjs72fjgl1l) is now a manager.
<snip>

ubuntu@ip-172-31-182-20:~$ # Here we can now do anything else that's needed
ubuntu@ip-172-31-182-20:~$ # Though you would normally automate everything
```

If you see errors similar to `"No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV4Handler'] Check your credentials"`, ensure that you have your AWS credentials set properly.

看起来一切正常！在这一点上，如果我们愿意，我们完全可以部署我们以前构建的 3 层应用程序。当我们完成了我们的示例，并且我们的迷你 PaaS 开始工作后，我们可以通过运行`destroy.yml`行动手册来返回并清理问题:

```
ubuntu@ip-172-31-182-20:~$ # Get out of our remote machine
ubuntu@ip-172-31-182-20:~$ exit
logout
Connection to 52.53.240.17 closed.

$ # Let's run the cleanup script
ansible-playbook destroy.yml 
 [WARNING]: provided hosts list is empty, only localhost is available

PLAY ***************************************************************************

TASK [Finding VMs to delete] ***************************************************
ok: [localhost]

TASK [Deleting instances] ******************************************************
changed: [localhost] => <snip>

TASK [Finding route table info] ************************************************
ok: [localhost]

TASK [set_fact] ****************************************************************
ok: [localhost]

TASK [Removing security group] *************************************************
changed: [localhost]

TASK [Deleting gateway] ********************************************************
changed: [localhost]

TASK [Deleting subnet] *********************************************************
changed: [localhost]

TASK [Deleting route table] ****************************************************
changed: [localhost]

TASK [Deleting VPC] ************************************************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost : ok=9 changed=6 unreachable=0 failed=0 

```

这样，我们就可以用单个命令自动部署和拆卸我们的基础设施。虽然这个例子的范围非常有限，但是它应该会给你一些想法，告诉你如何通过自动扩展组、编排管理、注册中心部署和数据持久性来扩展这个范围，从而把它变成一个成熟的 PaaS。

# 持续集成/持续交付

随着您提供更多的服务，您会注意到手动部署源代码管理和构建中的更改会占用更多的时间，因为需要找出哪些映像依赖项属于哪里，哪些映像实际上需要重建(如果您运行的是 mono-repo)，服务是否有任何更改，以及许多其他辅助问题。为了简化和精简我们的部署过程，我们需要找到一种方法来使整个系统完全自动化，以便部署新版本的服务只需要提交对代码存储库分支的更改。

到目前为止，最受欢迎的自动化服务器詹金斯通常用于此类功能，以实现构建自动化和 Docker 映像和基础架构的部署，但其他服务器，如无人机、构建机器人、Concoure 等，也在功能强大的软件配置项/光盘工具行列中快速崛起，但迄今为止还没有一个达到业界的相同接受水平。由于 Jenkins 也相对容易使用，我们可以快速演示一下它的强大功能，虽然这个例子有点简单，但它应该清楚地表明如何将它用于更多用途。

由于詹金斯将需要`awscli`、Ansible 和`python-boto`，我们必须基于可从 Docker Hub 获得的詹金斯制作一个新的 Docker 映像。创建一个新文件夹，并添加一个包含以下内容的`Dockerfile`:

```
FROM jenkins

USER root
RUN apt-get update && \
 apt-get install -y ansible \
 awscli \
 python-boto

USER jenkins
```

现在我们构建并运行我们的服务器:

```
$ # Let's build our image
$ docker build -t jenkins_with_ansible 
Sending build context to Docker daemon 2.048kB
Step 1/4 : FROM jenkins
<snip>
Successfully tagged jenkins_with_ansible:latest

$ # Run Jenkins with a local volume for the configuration
$ mkdir jenkins_files
$ docker run -p 8080:8080 \
 -v $(pwd)/jenkins_files:/var/jenkins_home \
 jenkins_with_ansible

Running from: /usr/share/jenkins/jenkins.war
<snip>
Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

3af5d45c2bf04fffb88e97ec3e92127a

This may also be found at: /var/jenkins_home/secrets/initialAdminPassword
<snip>
INFO: Jenkins is fully up and running
```

当它还在运行的时候，让我们进入主页面，输入安装密码，在映像启动过程中，我们得到了一个警告。转到`http://localhost:8080`并输入您日志中的密码:

![](img/a4d91396-d2b1-4ce8-a9e4-10a4d9bfad90.png)

单击下一个窗口上的安装建议插件，然后在相关下载完成后，在最后一个安装程序页面上选择继续作为管理员，这将引导您进入主登录页面:

![](img/ccc437ac-87cb-4551-8e9f-1a689fbb5b47.png)

点击创造新的工作，将其命名为`redeploy_infrastructure`，并使其成为自由式项目:

![](img/a4dad089-6fd8-4422-aa17-cc4d5f855e55.png)

接下来，我们将使用我们的 Git 存储库端点配置该作业，以便它基于对主分支的任何提交:

![](img/c4cc0ce7-fdef-47a0-bd3f-ea487e9080f7.png)

作为我们的构建步骤，当存储库触发器激活时，我们将销毁并部署基础架构，用更新的版本有效地替换它。添加**执行 Shell** 类型的新构建步骤，并添加以下内容:

```
# Export needed AWS credentials
export AWS_DEFAULT_REGION="us-west-1"
export AWS_ACCESS_KEY_ID="AKIABCDEFABCDEF"
export AWS_SECRET_ACCESS_KEY="123456789ABCDEF123456789ABCDEF"

# Change to relevant directory
cd chapter_8/aws_deployment

# Redeploy the service by cleaning up the old deployment
# and deploying a new one
ansible-playbook destroy.yml
ansible-playbook deploy.yml
```

这份工作看起来应该很像这样:

![](img/7e3f36a2-8cd3-47d4-a04c-d145ed4f5366.png)

用`Save`保存更改，这将带您进入构建的主页面。在这里，点击`Build Now`按钮，一旦构建出现在左侧构建列表中，点击其进度条或其名称旁边的下拉菜单，并选择`View Log`:

![](img/0273eb37-7952-4cde-bcd5-dd41adce274b.png)

成功！正如您所看到的，通过 Jenkins 和一个小配置，我们刚刚对简单的基础架构进行了自动化部署。它很粗糙，但很有效，尽管通常你不想重新部署所有东西，只想要已经改变的部分，并让詹金斯住在集群中，但这是一些更复杂的工作，将留给读者作为可能的改进点。

# 资源考虑

由于 Jenkins 运行在 Java 虚拟机之上，它将以惊人的速度消耗可用内存，并且通常是最大的使用猪，同时也是我经历过的最频繁的内存不足的罪魁祸首。即使在最轻的用例中，也要计划为詹金斯的工作人员分配至少 1 GB 的内存，否则在构建管道的最不合适的阶段可能会出现各种故障。一般来说，此时大多数 Jenkins 安装对于分配给它们的 2gb RAM 不会有很多问题，但是由于 VM 实例中 RAM 的价格，您可以尝试向后扩展，直到达到可接受的性能水平。

最后要注意的是，相对而言，詹金斯图像也很笨重，重量约为 800 兆字节，所以请记住，移动这个容器真的不像我们一直在使用的其他工具那么容易也不那么快。

# 首先部署循环依赖关系

当在您的集群中使用 Jenkins 作为 Dockerized 服务来链接构建所有其他映像时，我必须提到一个常见的陷阱，在这个陷阱中，您不可避免地会遇到 Jenkins 最初不可用的新部署问题，因为在集群初始化阶段，注册表中通常没有可用的映像，并且没有以任何方式配置默认的 Jenkins Docker 映像。最重要的是，由于您经常需要一个已经运行的 Jenkins 实例来构建一个更新的 Jenkins 映像，因此您将处于经典的 Catch-22 情形。您可能会反射性地手动构建 Jenkins 作为后续部署步骤，但是如果您想要真正拥有基本上不干涉的基础架构，您必须抵制这样做的冲动。

解决这个问题的一般方法是在干净的集群上引导 Jenkins，如下图所示:

![](img/6de34386-4ac9-48a2-8428-5bf622dda59a.png)

首先进行集群部署，以确保我们有办法构建我们的引导映像，并且 **Docker 注册表**用于在映像构建后存储它。接下来，我们在任何可用的 Docker Engine 节点上构建 Jenkins 映像，并将其推送到注册表，这样服务就有了正确的映像。如果需要，我们随后使用相同的配置管理工具(如 Ansible)或编排工具启动上述服务，并等待自动启动作业，该作业将构建所有其他剩余的映像，这些映像应该用运行整个集群所需的所有其他映像填充注册表。这里的基本思想是通过 CM 工具进行初始引导，然后让 Jenkins 服务重建所有其他映像并(重新)启动任务。

In large-scale deployments, it is also possible to use your cluster orchestration to schedule and handle this bootstrap procedure instead of the CM tooling but due to the vast differences between each orchestration engine, these steps may vary wildly between them.

# 进一步的通用配置项/光盘用途

像詹金斯这样优秀的 CI 工具可以做的事情比我们在这里介绍的要多得多；它们都需要投入大量的时间和精力才能开始工作，但是如果您能够实施它们，好处是非常显著的:

*   **自构建**:如之前的变通方法中所述，您可以让 Jenkins 在配置发生变化时构建自己的映像，并让其自行重新部署。
*   **仅部署已更改的 Docker 映像**:如果使用 Docker 缓存，则可以检查新构建是否创建了不同的映像哈希，如果创建了，则仅进行部署。这样做将防止无意义的工作，并使您的基础架构始终运行最新的代码。
*   **定时 Docker 修剪**:您可以在 Jenkins 上运行清理作业(或任何其他类似于`cron`的作业)，这将释放或管理您的 Docker 节点，以避免手动交互。

这个列表还可以包括:自动发布、失败通知、构建跟踪，以及许多其他可以获得的东西，但是可以说，您真的希望在任何非平凡的部署中有一个工作的配置项管道。

一个经验法则是，如果你需要手动做一些可以用一些计时器和 shell 脚本自动完成的事情，大多数 CI 工具(比如 Jenkins)都在那里帮助你，所以不要害怕尝试它的不同和创造性的用途。有了我们在本章中介绍的一系列选项和其他工具，您可以睡得很香，因为您知道您的集群暂时会好起来，而不需要经常照看孩子。

# 摘要

在本章中，我们已经介绍了更多关于如何真正部署 PaaS 基础架构的内容，并深入研究了它所需的以下主题:使用 Ansible 的配置管理工具、使用 HashiCorp Packer 的云映像管理以及与 Jenkins 的持续集成。有了在这里获得的知识，您现在应该能够使用我们讨论的各种工具，并为您自己的服务部署创建您自己的小型 PaaS，通过一些额外的工作，您可以将其转变为全面的 PaaS！

在下一章中，我们将了解如何利用我们当前的 Docker 和基础架构工作，并使其变得更大。我们还将介绍该领域可能向哪个方向发展，因此，如果您想深入了解世界上最大规模的部署，请继续关注。