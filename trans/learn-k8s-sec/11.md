# *第十一章*:纵深防御

深度防御是网络安全中的一种方法，它应用多层安全控制来保护有价值的资产。在传统或单一的 IT 环境中，我们可以列出相当多的:身份验证、加密、授权、日志记录、入侵检测、防病毒、一个**虚拟专用网络** ( **VPN** )、防火墙等等。您可能会发现这些安全控制也存在于 Kubernetes 集群中(它们应该存在)。

在前几章中，我们已经讨论了身份验证、授权、准入控制器、保护 Kubernetes 组件、保护配置、强化映像和 Kubernetes 工作负载等主题。所有这些都建立了不同的安全控制层来保护您的 Kubernetes 集群。在本章中，我们将讨论建立附加安全控制层的主题，这些主题与 Kubernetes 集群中的运行时防御最为相关。这些是我们将在本章中讨论的问题:您的集群是否暴露了任何敏感数据？如果在 Kubernetes 集群中发生攻击，您能检测到攻击吗？你的Kubernetes集群能承受攻击吗？你如何应对这次袭击？

在本章中，我们将讨论 Kubernetes 审计，然后我们将介绍高可用性的概念，并讨论如何在 Kubernetes 集群中应用高可用性。接下来，我们将介绍 Vault，它是 Kubernetes 集群的一个方便的机密管理产品。然后，我们将讨论如何使用 Falco 来检测 Kubernetes 星团中的异常活动。最后但同样重要的是，我们将介绍用于取证的 Sysdig Inspect 和用户空间中的**检查点和资源**(也称为 **CRIU** )。

本章将涵盖以下主题:

*   介绍 Kubernetes 审计
*   在 Kubernetes 集群中实现高可用性
*   使用保管库管理机密
*   用法尔科探测异常
*   与 Sysdig Inspect 和 CRIU 进行取证

# 介绍Kubernetes审计

1.11 版本引入了 Kubernetes 审计。Kubernetes 审核按时间顺序记录事件，如创建部署、修补 pods、删除命名空间等。通过审计，Kubernetes 集群管理员能够回答以下问题:

*   发生了什么事？(一个Pod 被创建，它是什么样的Pod )
*   是谁干的？(来自用户/管理员)
*   什么时候发生的？(事件的时间戳)
*   在哪里发生的？(pod 是在哪个命名空间中创建的？)

从安全角度来看，审计使 DevOps 和安全团队能够通过跟踪 Kubernetes 集群内发生的事件来更好地检测和预防异常。

在 Kubernetes 集群中，是`kube-apiserver`进行审核。当一个请求(例如，创建名称空间)被发送到`kube-apiserver`时，该请求可能会经历多个阶段。每个阶段都会产生一个事件。以下是已知的阶段:

*   `RequestReceived`:审核处理程序一收到请求就生成事件，不进行处理。
*   `RequestStarted`:事件产生于响应头发送到响应体发送之间，仅适用于`watch`等长时间运行的请求。
*   `RequestComplete`:事件是响应体发送时生成的。
*   `Panic`:发生死机时生成事件。

在本节中，我们将首先介绍 Kubernetes 审计策略，然后向您展示如何启用 Kubernetes 审计以及保持审计记录的几种方法。

## 立方审计政策

由于记录Kubernetes 集群内发生的所有事情是不现实的，因此审计策略允许用户定义关于应该记录什么样的事件以及应该记录多少事件细节的规则。当`kube-apiserver`处理一个事件时，它会按顺序比较审核策略中的规则列表。第一个匹配规则还规定了事件的审核级别。让我们看看审计策略是什么样子的。这里有一个例子:

```
apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Skip generating audit events for all requests in RequestReceived stage. This can be either set at the policy level or rule level.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    verbs: ["create", "update"]
    namespace: ["ns1", "ns2", "ns3"]
    resources:
    - group: ""
# Only check access to resource "pods", not the sub-resource of pods which is consistent with the RBAC policy.
      resources: ["pods"]
# Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]
# Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs: ["/api*", "/version"]
# Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]
```

可以在审核策略中配置多个审核规则。每个审核规则将由以下字段配置:

*   `level`:定义审核事件详细程度的审核级别。
*   `resources`:Kubernetes审计对象。资源可以由****应用编程接口** ( **应用编程接口**)组和对象类型指定。**
***   `nonResourcesURL`:非资源**统一资源定位符** ( **URL** )路径，不与任何被审核资源关联。*   `namespace`:决定哪些Kubernetes对象将接受哪些命名空间的审核。空字符串将用于选择非命名空间对象，空列表表示每个命名空间。*   `verb`:决定将要审核的 Kubernetes 对象的具体操作——例如`create`、`update`或`delete`。*   `users`:决定审核规则适用的认证用户*   `userGroups`:决定审核规则适用的认证用户组。*   `omitStages`:跳过在给定阶段生成事件。这也可以在策略级别设置。**

 **通过指定`verb`、`namespace`、`resources`等，审核策略允许您在细粒度级别配置策略。该规则的审核级别定义了应该记录事件的多少细节。有四个审核级别，详细如下:

*   `None`:不记录符合审核规则的事件。
*   `Metadata`:当事件符合审核规则时，将请求的元数据(如`user`、`timestamp`、`resource`、`verb`等)记录到`kube-apiserver`。
*   `Request`:当事件与审核规则匹配时，记录元数据和请求正文。这不适用于非资源网址。
*   `RequestResponse`:当事件符合审核规则时，记录元数据、请求和响应主体。这不适用于非资源请求。

请求级事件比元数据级事件更详细，而`RequestResponse`级事件比请求级事件更详细。高冗长要求更多的**输入/输出** ( **输入/输出**)吞吐量和存储。理解审核级别之间的差异是非常必要的，这样您就可以为资源消耗和安全性正确定义审核规则。成功配置审计策略后，让我们看看审计事件是什么样子的。以下是元数据级别的审核事件:

```
{
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "05698e93-6ad7-4f4e-8ae9-046694bee469",
  "stage": "ResponseComplete",
  "requestURI": "/api/v1/namespaces/ns1/pods",
  "verb": "create",
  "user": {
    "username": "admin",
    "uid": "admin",
    "groups": [
      "system:masters",
      "system:authenticated"
    ]
  },
  "sourceIPs": [
    "98.207.36.92"
  ],
  "userAgent": "kubectl/v1.17.4 (darwin/amd64) kubernetes/8d8aa39",
  "objectRef": {
    "resource": "pods",
    "namespace": "ns1",
    "name": "pod-1",
    "apiVersion": "v1"
  },
  "responseStatus": {
    "metadata": {},
    "code": 201
  },
  "requestReceivedTimestamp": "2020-04-09T07:10:52.471720Z",
  "stageTimestamp": "2020-04-09T07:10:52.485551Z",
  "annotations": {
    "authorization.k8s.io/decision": "allow",
    "authorization.k8s.io/reason": ""
  }
}
```

前面的审计事件显示了`user`、`timestamp`、被访问的对象、授权决定等等。请求级审核事件在审核事件的`requestObject`字段中提供额外信息。您将在`requestObject`字段中找到工作量的规格，如下所示:

```
  "requestObject": {
    "kind": "Pod",
    "apiVersion": "v1",
    "metadata": {
      "name": "pod-2",
      "namespace": "ns2",
      "creationTimestamp": null,
      ...
    },
    "spec": {
      "containers": [
        {
          "name": "echo",
          "image": "busybox",
          "command": [
            "sh",
            "-c",
            "echo 'this is echo' && sleep 1h"
          ],
          ...
          "imagePullPolicy": "Always"
        }
      ],
      ...
      "securityContext": {},
    },
```

`RequestResponse`级审核事件最啰嗦。事件中的`responseObject`实例几乎与`requestObject`相同，有额外的信息，如资源版本和创建时间戳，如下面的代码块所示:

```
{
  "responseObject": {
      ...
      "selfLink": "/api/v1/namespaces/ns3/pods/pod-3",
      "uid": "3fd18de1-7a31-11ea-9e8d-0a39f00d8287",
      "resourceVersion": "217243",
      "creationTimestamp": "2020-04-09T07:10:53Z",
      "tolerations": [
        {
          "key": "node.kubernetes.io/not-ready",
          "operator": "Exists",
          "effect": "NoExecute",
          "tolerationSeconds": 300
        },
        {
          "key": "node.kubernetes.io/unreachable",
          "operator": "Exists",
          "effect": "NoExecute",
          "tolerationSeconds": 300
        }
      ],
      ...
    },
 }
```

请正确选择审核级别。更详细的日志可以更深入地了解正在执行的活动。但是，处理审计事件确实会增加存储和时间成本。值得一提的是，如果对 Kubernetes 机密对象设置一个请求或一个`RequestResponse`审核级别，则该机密内容会被记录在审核事件中。如果将包含敏感数据的 Kubernetes 对象的审核级别设置为比元数据更详细，则应使用敏感数据编校机制，以避免在审核事件中记录机密。

Kubernetes 审核功能提供了很大的灵活性，可以按对象种类、名称空间、操作、用户等来审核 Kubernetes 对象。由于默认情况下不启用 Kubernetes 审核，接下来我们来看看如何启用 Kubernetes 审核并存储 audi t 记录。

## 配置审计后端

为了启用 Kubernetes 审核，您需要在启动`kube-apiserver`时通过审核策略文件的`--audit-policy-file`标志。有两种类型的审核后端可以配置为使用流程审核事件:日志后端和 webhook 后端。让我们来看看他们 。

### 日志后端

日志后端将审核事件写入主节点上的文件。以下标志用于在`kube-apiserver`内配置日志后端:

*   `--log-audit-path`:指定主节点上的日志路径。这是打开或关闭日志后端的标志。
*   `--audit-log-maxage`:指定保存审核记录的最大天数。
*   `--audit-log-maxbackup`:指定主节点上要保留的审核文件的最大数量。
*   `--audit-log-maxsize`:指定审核日志文件旋转前的最大大小(以兆字节为单位)。

让我们来看看 w ebhook 后端。

### 网络钩子后端

网络钩子后端将审计事件写入注册到`kube-apiserver`的远程网络钩子。要启用网络钩子后端，需要用网络钩子配置文件设置`--audit-webhook-config-file`标志。启动`kube-apiserver`时也指定了该标志。下面是一个为 Falco 服务注册 webhook 后端的 webhook 配置示例，稍后将对此进行更详细的介绍:

```
apiVersion: v1
kind: Config
clusters:
- name: falco
  cluster:
    server: http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit
contexts:
- context:
    cluster: falco
    user: ""
  name: default-context
current-context: default-context
preferences: {}
users: []
```

`server`字段(`http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit`)中指定的网址是审计事件将发送到的远程端点。从 1.13 版本的 Kubernetes 开始，可以通过`AuditSink`对象动态配置 webhook 后端，目前还处于 alpha 阶段。

在本节中，我们通过介绍审计策略和审计后端来讨论 Kubernetes 审计。在下一节中，我们将讨论 Kub ernetes 集群中的高可用性。

# 在 Kubernetes 集群中实现高可用性

可用性是指用户访问服务或系统的能力。系统的高可用性确保了系统正常运行时间的约定水平。例如，如果只有一个实例来服务该服务，并且该实例已关闭，则用户无法再访问该服务。高可用性服务由多个实例提供。当一个实例关闭时，备用实例或备份实例仍然可以提供服务。下图描述了具有和不具有高可用性的服务:

![Figure 11.1 – Services with and without high availability ](img/B15566_11_001.jpg)

图 11.1–有无高可用性的服务

在 Kubernetes 集群中，通常会有多个工作节点。集群的高可用性得到保证，因为即使一个工作节点停机，也有一些其他工作节点来承载工作负载。然而，高可用性不仅仅是在集群中运行多个节点。在本节中，我们将从三个层面来看 Kubernetes 集群中的高可用性:工作负载、Kubernetes 组件和 clo ud 基础架构。

## 实现 Kubernetes 工作负载的高可用性

对于 Kubernetes 工作负载，如部署和 StatefulSet，您可以在规范中指定`replicas`字段，以了解有多少复制的单元正在为微服务运行，控制器将确保在集群中的不同工作节点上运行`x`数量的单元，如`replicas`字段中所指定的。DaemonSet 是一种特殊的工作负载；控制器将确保集群中的每个节点上都有一个 pod 在运行，假设您的 Kubernetes 集群有多个节点。因此，在部署或状态集中指定多个副本，或者使用 DaemonSet，将确保工作负载的高可用性。为了保证工作负载的高可用性，还需要保证 Kubernetes 组件的高可用性。

## 实现 Kubernetes 组件的高可用性

高可用性也适用于Kubernetes组件。让我们回顾几个关键的 Kubernetes 组件，如下所示:

*   `kube-apiserver`:Kubernetes API 服务器(`kube-apiserver`)是一个控制平面组件，用于验证和配置 pods、服务和控制器等对象的数据。它使用**代表状态转移** ( **休息**)请求与对象交互。
*   `etcd` : `etcd`是高可用性键值存储，用于存储配置、状态和元数据等数据。其`watch`功能为 Kubernetes 提供了监听配置更新并进行相应更改的能力。
*   `kube-scheduler` : `kube-scheduler`是 Kubernetes 的默认调度程序。它监视新创建的荚，并将荚分配给节点。
*   `kube-controller-manager`:Kubernetes 控制器管理器是核心控制器的组合，它们监视状态更新并相应地对集群进行更改。

如果`kube-apiserver`关闭，那么基本上您的集群也关闭了，因为用户或其他 Kubernetes 组件依赖与`kube-apiserver`的通信来执行它们的任务。如果`etcd`关闭，集群和对象的任何状态都不可用。`kube-scheduler`和`kube-controller-manager`对于确保工作负载在集群中正常运行也很重要。所有这些组件都在主节点上运行，以确保组件的高可用性。一种简单的方法是通过`kops`或`kubeadm`为 Kubernetes 集群调出多个主节点。你会发现类似这样的东西:

```
$ kubectl get pods -n kube-system
...
etcd-manager-events-ip-172-20-109-109.ec2.internal       1/1     Running   0          4h15m
etcd-manager-events-ip-172-20-43-65.ec2.internal         1/1     Running   0          4h16m
etcd-manager-events-ip-172-20-67-151.ec2.internal        1/1     Running   0          4h16m
etcd-manager-main-ip-172-20-109-109.ec2.internal         1/1     Running   0          4h15m
etcd-manager-main-ip-172-20-43-65.ec2.internal           1/1     Running   0          4h15m
etcd-manager-main-ip-172-20-67-151.ec2.internal          1/1     Running   0          4h16m
kube-apiserver-ip-172-20-109-109.ec2.internal            1/1     Running   3          4h15m
kube-apiserver-ip-172-20-43-65.ec2.internal              1/1     Running   4          4h16m
kube-apiserver-ip-172-20-67-151.ec2.internal             1/1     Running   4          4h15m
kube-controller-manager-ip-172-20-109-109.ec2.internal   1/1     Running   0          4h15m
kube-controller-manager-ip-172-20-43-65.ec2.internal     1/1     Running   0          4h16m
kube-controller-manager-ip-172-20-67-151.ec2.internal    1/1     Running   0          4h15m
kube-scheduler-ip-172-20-109-109.ec2.internal            1/1     Running   0          4h15m
kube-scheduler-ip-172-20-43-65.ec2.internal              1/1     Running   0          4h15m
kube-scheduler-ip-172-20-67-151.ec2.internal             1/1     Running   0          4h16m
```

现在您的有多个`kube-apiserver`豆荚、`etcd`豆荚、`kube-controller-manager`豆荚和`kube-scheduler`豆荚在`kube-system`命名空间中运行，它们运行在不同的主节点上。还有一些其他组件，如`kubelet`和`kube-proxy`在每个节点上运行，因此它们的可用性由节点的可用性来保证，而`kube-dns`默认与多个 pod 一起旋转，因此它们的高可用性得到了保证。无论您的 Kubernetes 集群是运行在公共云还是私有数据中心，基础架构都是支持 Kubernetes 集群可用性的支柱。接下来，我们将讨论云基础设施的高可用性，并以云提供商为例。

## 实现云基础设施的高可用性

云提供商通过位于不同地区的多个数据中心在全球范围内提供云服务。云用户可以选择在哪个地区和区域(实际的数据中心)托管他们的服务。区域和分区提供了与大多数类型的物理基础架构和基础架构软件服务故障的隔离。请注意，如果集群托管在云中，云基础架构的可用性也会影响 Kubernetes 集群上运行的服务。您应该利用云的高可用性，并最终确保运行在 Kubernetes 集群上的服务的高可用性。以下代码块提供了一个使用`kops`指定区域的示例，以利用云基础架构的高可用性:

```
export NODE_SIZE=${NODE_SIZE:-t2.large}
export MASTER_SIZE=${MASTER_SIZE:-t2.medium}
export ZONES=${ZONES:-"us-east-1a,us-east-1b,us-east-1c"}
export KOPS_STATE_STORE="s3://my-k8s-state-store2/"
kops create cluster k8s-clusters.k8s-demo-zone.com \
  --cloud aws \
  --node-count 3 \
  --zones $ZONES \
  --node-size $NODE_SIZE \
  --master-size $MASTER_SIZE \
  --master-zones $ZONES \
  --networking calico \
  --kubernetes-version 1.14.3 \
  --yes \
```

Kubernetes 集群的节点如下所示:

```
$ kops validate cluster
...
INSTANCE GROUPS
NAME			ROLE	MACHINETYPE	MIN	MAX	SUBNETS
master-us-east-1a	Master	t2.medium	1	1	us-east-1a
master-us-east-1b	Master	t2.medium	1	1	us-east-1b
master-us-east-1c	Master	t2.medium	1	1	us-east-1c
nodes			Node	t2.large	3	3	us-east-1a,us-east-1b,us-east-1c
```

前面的代码块显示了分别在`us-east-1a`、`us-east-1b`和`us-east-1c`可用性区域上运行的三个主节点。因此，作为工作节点，即使其中一个数据中心出现故障或处于维护状态，主节点和工作节点仍然可以在其他数据中心运行。

在本节中，我们讨论了 Kubernetes 工作负载、Kubernetes 组件和云基础架构的高可用性。让我们使用下图来回顾一下 Kubernetes 集群的高可用性:

![Figure 11.2 – High availability of Kubernetes cluster in the cloud ](img/B15566_11_002.jpg)

图 11.2–云中 Kubernetes 集群的高可用性

现在，让我们进入下一个主题，关于管理 Kubernetes 集群中的 secretts。

# 用金库管理机密

机密管理是一个大话题，为了帮助解决不同平台上的机密管理问题，已经开发了许多开源和专有的解决方案。所以，在 Kubernetes 中，它内置的`Secret`对象用来存储机密数据，实际数据和其他 Kubernetes 对象一起存储在`etcd`中。默认情况下，机密数据以明文(编码格式)存储在`etcd`中。`etcd`可以配置为静态加密机密。同样，如果`etcd`未配置为使用**传输层安全性** ( **TLS** )加密通信，机密数据也会以明文传输。除非安全要求很低，否则建议使用第三方解决方案来管理 Kubernetes 集群中的机密。

在本节中，我们将介绍 Vault，一个**云原生计算基金会**(**【CNCF】**)机密管理项目。Vault 支持机密的安全存储、动态机密的生成、数据加密、密钥撤销等。在本节中，我们将重点介绍如何使用 Vault 为 Kubernetes 集群中的应用存储和提供机密的使用案例。现在，让我们看看如何为Kubernetes集群设置保险库。

## 设置保险库

您可以使用`helm`在 Kubernetes 集群中部署 Vault，如下所示:

```
helm install vault --set='server.dev.enabled=true' https://github.com/hashicorp/vault-helm/archive/v0.4.0.tar.gz
```

注意`server.dev.enabled=true`已设置。这对于开发环境很好，但不建议在生产环境中设置。您应该看到两个Pod 正在运行，如下所示:

```
$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
vault-0                                 1/1     Running   0          80s
vault-agent-injector-7fd6b9588b-fgsnj   1/1     Running   0          80s
```

`vault-0`舱是管理和存储机密的舱，而`vault-agent-injector-7fd6b9588b-fgsnj`舱负责将机密注入带有特殊拱顶注释的舱，我们将在*供应和旋转机密*部分更详细地展示。接下来，让我们为`postgres`数据库连接创建一个示例机密，如下所示:

```
vault kv put secret/postgres username=alice password=pass
```

请注意，前面的命令需要在`vault-0`Pod 中执行。由于您希望只限制 Kubernetes 集群中的相关应用访问该机密，因此您可能希望定义一个策略来实现这一点，如下所示:

```
cat <<EOF > /home/vault/app-policy.hcl
path "secret*" {
  capabilities = ["read"]
}
EOF
vault policy write app /home/vault/app-policy.hcl
```

现在，您有一个策略定义了在`secret`路径下读取机密的特权，例如`secret` / `postgres`。接下来，您希望将策略与允许的实体相关联，例如 Kubernetes 中的服务帐户。这可以通过执行以下命令来完成:

```
vault auth enable kubernetes
vault write auth/kubernetes/config \
   token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
   kubernetes_host=https://${KUBERNETES_PORT_443_TCP_ADDR}:443 \
   kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
vault write auth/kubernetes/role/myapp \
   bound_service_account_names=app \
   bound_service_account_namespaces=demo \
   policies=app \
   ttl=24h
```

Vault 可以利用 Kubernetes 的简单身份验证，然后将机密访问策略绑定到服务帐户。现在，名称空间演示中的服务帐户应用可以访问`postgres`机密。现在，让我们在`vault-app.yaml`文件中部署一个演示应用，如下所示:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    app: vault-agent-demo
spec:
  selector:
    matchLabels:
      app: vault-agent-demo
  replicas: 1
  template:
    metadata:
      annotations:
      labels:
        app: vault-agent-demo
    spec:
      serviceAccountName: app
      containers:
      - name: app
        image: jweissig/app:0.0.1
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app
  labels:
    app: vault-agent-demo
```

请注意，在前面的`.yaml`文件中，还没有添加注释，因此不会注入机密，也不会在应用创建时添加侧车容器。代码可以在下面的代码片段中看到:

```
$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
app-668b8bcdb9-js9mm                    1/1     Running   0          3m23s
```

接下来，我们将展示机密注射是如何工作的。

## 供应和旋转机密

我们在部署应用时不显示机密注入的原因是我们想向您展示注入到演示应用 pod 之前和之后的详细区别。现在，让我们用以下 Vault 注释来修补部署:

```
$ cat patch-template-annotation.yaml
spec:
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-status: "update"
        vault.hashicorp.com/agent-inject-secret-postgres: "secret/postgres"
        vault.hashicorp.com/agent-inject-template-postgres: |
          {{- with secret "secret/postgres" -}}
          postgresql://{{ .Data.data.username }}:{{ .Data.data.password }}@postgres:5432/wizard
          {{- end }}
        vault.hashicorp.com/role: "myapp"
```

前面的注释规定了将注入哪个机密，以什么格式和使用哪个角色。一旦我们更新了演示应用部署，我们会发现机密已经被注入，如下所示:

```
$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
app-68d47bb844-2hlrb                    2/2     Running   0          13s
$ kubectl -n demo exec -it app-68d47bb844-2hlrb -c app -- cat /vault/secrets/postgres
postgresql://alice:pass@postgres:5432/wizard
```

让我们看看 pod 的规范(不是修补的部署)—与修补的部署规范相比，您会发现添加了以下内容(用粗体标记):

```
  containers:
  - image: jweissig/app:0.0.1
    ...
    volumeMounts:
    - mountPath: /vault/secrets
      name: vault-secrets
  - args:
    - echo ${VAULT_CONFIG?} | base64 -d > /tmp/config.json && vault agent -config=/tmp/config.json
    command:
    - /bin/sh
    - -ec
    image: vault:1.3.2
    name: vault-agent
    volumeMounts:
    - mountPath: /vault/secrets
      name: vault-secrets
 initContainers:
  - args:
    - echo ${VAULT_CONFIG?} | base64 -d > /tmp/config.json && vault agent -config=/tmp/config.json
    command:
    - /bin/sh
    - -ec
    image: vault:1.3.2
    name: vault-agent-init
    volumeMounts:
    - mountPath: /vault/secrets
      name: vault-secrets
  volumes:
   - emptyDir:
      medium: Memory
    name: vault-secrets
```

从前面列举的改动中值得一提的是:注入了一个名为`vault-agent-init`的`init`容器和一个名为`vault-agent`的边车容器，以及一个名为`vault-secrets`的`emptyDir`型容积。这就是为什么在补丁发布后，您会看到两个容器在演示应用窗格中运行。此外，`vault-secrets`卷安装在带有`/vault/secrets/`目录的`init`容器、`sidecar`容器和`app`容器中。机密存储在`vault-secrets`卷中。Pod 规格修改由`vault-agent-injector`Pod 通过预定义的突变网钩配置(通过`helm`安装)完成，如下所示:

```
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  ...
  name: vault-agent-injector-cfg
webhooks:
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    caBundle: <CA_BUNDLE>
    service:
      name: vault-agent-injector-svc
      namespace: demo
      path: /mutate
  failurePolicy: Ignore
  name: vault.hashicorp.com
  namespaceSelector: {}
  rules:
  - apiGroups:
    - ""
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - pods
    scope: '*'
```

在`kube-apiserver`注册的变异网钩配置基本上告诉`kube-apiserver`重定向任何豆荚，创建或更新请求到`demo`命名空间中的`vault-agent-injector-svc`服务。服务背后是`vault-agent-injector`号Pod 。然后，`vault-agent-injector`Pod 将查找相关注释，并根据请求将`init`容器和`sidecar`容器以及存储机密的卷注入Pod 的规范中。为什么我们需要一个`init`容器和一个`sidecar`容器？`init`容器用于预填充我们的机密，而`sidecar`容器用于在应用的整个生命周期中保持机密数据的同步。

现在，让我们通过运行以下代码来更新这个机密，看看会发生什么:

```
vault kv put secret/postgres username=alice password=changeme
```

现在，密码已经从`vault`舱的`pass`更新为`changeme`。并且，在`demo`应用端，我们可以从下面的代码块中看到，在等待几秒钟之后，它也被更新了:

```
$ kubectl -n demo exec -it app-68d47bb844-2hlrb -c app -- cat /vault/secrets/postgres
postgresql://alice:changeme@postgres:5432/wizard
```

Vault 是一个强大的机密管理解决方案，它的许多功能无法在单个部分中涵盖。我鼓励您阅读文档并尝试来更好地理解 Vault。接下来，让我们和法尔科谈谈 Kubernetes 中的 runt ime 威胁检测。

# 用法尔科探测异常

Falco 是 CNCF 的一个开源项目，它在云原生环境中检测异常行为或运行时威胁，比如 Kubernetes 集群。它是一个基于规则的运行时检测引擎，大约有 100 个现成的检测规则。在本节中，我们将首先概述 Falco，然后向您展示如何编写 Falco 规则，以便您可以构建自己的 Falco 规则来保护您的 Kubernetes 集群。

## 法尔科概述

Falco 被广泛用于检测云原生环境中的异常行为，尤其是在 Kubernetes 集群中。那么，什么是异常检测呢？基本上，它使用行为信号来检测安全异常，例如泄露的凭据或异常活动，并且行为信号可以从您对正常行为是什么的实体的了解中导出。

### 面临的挑战

识别Kubernetes集群中的正常行为并不容易。从运行应用的角度来看，我们可以将它们分为三类，如下所示:

*   **Kubernetes 组件** : `kube-apiserver`、`kube-proxy`、`kubelet`、**容器运行时接口** ( **CRI** )插件、**容器联网接口** ( **CNI** )插件等等
*   **自托管应用** : Java、Node.js、Golang、Python 等
*   **供应商服务** : Cassandra、Redis、MySQL、nginx、Tomcat 等

或者，从系统的角度来看，我们有以下类型的活动:

*   文件活动，如打开、读取和写入
*   过程活动，如`execve`和`clone`系统调用
*   接受、连接和发送等网络活动

或者，从 Kubernetes 对象的角度来看:`pod`、`secret`、`deployment`、`namespace`、`serviceaccount`、`configmap`等等

为了涵盖所有这些活动或在 Kubernetes 集群中发生的行为，我们将需要丰富的信息来源。接下来，我们来谈谈 Falco 用来进行异常检测的事件源，以及 s 源是如何覆盖前面的活动和行为的。

### 异常检测的事件源

Falco 依靠两个事件源进行异常检测。一个是系统调用，另一个是 Kubernetes 审计事件。对于系统调用事件，Falco 使用内核模块来挖掘机器上的系统调用流，然后将这些系统调用传递给用户空间(最近也支持`ebpf`)。在用户空间中，Falco 还用更多的上下文来丰富原始系统调用事件，例如进程名、容器 ID、容器名、图像名等等。对于 Kubernetes 审计事件，用户需要启用 Kubernetes 审计策略，并向 Falco 服务端点注册 Kubernetes 审计 webhook 后端。然后，Falco 引擎检查与引擎中加载的任何 Falco 规则匹配的任何系统调用事件或 Kubernetes 审核事件。

谈论使用系统调用和 Kubernetes 审计事件作为事件源来进行异常检测的基本原理也很重要。系统调用是应用与操作系统交互以访问文件、设备、网络等资源的编程方式。考虑到容器是一堆具有自己专用名称空间的进程，并且它们在节点上共享相同的操作系统，系统调用是一个统一的事件源，可用于监控容器的活动。应用用什么编程语言编写并不重要；最终，所有的功能都将被转换成系统调用来与操作系统交互。请看下图:

![Figure 11.3 – Containers and system calls ](img/B15566_11_003.jpg)

图 11.3–容器和系统调用

在上图中，有四个运行不同应用的容器。这些应用可能用不同的编程语言编写，它们都调用一个函数来打开具有不同函数名的文件(例如，`fopen`、`open`、`os.Open`)。然而，从操作系统的角度来看，所有这些应用都调用同一个系统调用，`open`，但可能使用不同的参数。Falco 能够从系统调用中检索事件，因此它们是什么样的应用或者使用什么样的编程语言并不重要。

另一方面，在 Kubernetes 审计事件的帮助下，Falco 对 Kubernetes 对象的生命周期具有完全的可见性。这对于异常检测也很重要。例如，在生产环境中有一个带有`busybox`图像的 pod 作为特权 pod 启动，这可能是不正常的。

总的来说，两个事件源——系统调用和 Kubernetes 审计事件——足以覆盖 Kubernetes 集群中发生的所有有意义的活动。现在，了解了法尔科事件源，让我们用一个高级架构图来总结一下我们对法尔科的概述。

### 高级架构

法尔科主要由一个几个组件组成，如下:

*   **法尔科规则**:定义为的规则，用于检测事件是否为异常。
*   **法尔科引擎**:用法尔科规则评估一个传入事件，如果一个事件符合任何规则，则抛出一个输出。
*   **内核模块/Sysdig 库**:标记系统调用事件，并在发送到 Falco 引擎进行评估之前进行丰富。
*   **Web 服务器**:收听 Kubernetes 审核事件，并传递给 Falco 引擎进行评估。

下图显示了法尔科的内部架构:

![Figure 11.4 – Falco's internal architecture ](img/B15566_11_004.jpg)

图 11.4–法尔科的内部架构

现在，我们已经结束了对 Falco 的概述。接下来，让我们尝试使用一些法尔科规则并检测任何异常行为。

## 创建法尔科规则来检测异常

在我们深入探讨 Falco 规则之前，请通过运行以下命令确保您已经安装了 Falco:

```
helm install --name falco stable/falco
```

Falco DaemonSet 应该在 Kubernetes 集群中运行，如下面的代码块所示:

```
$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
falco-9h8tg   1/1     Running   10         62m
falco-cnt47   1/1     Running   5          3m45s
falco-mz6jg   1/1     Running   0          6s
falco-t4cpw   1/1     Running   0          10s
```

要启用 Kubernetes 审核并将 Falco 注册为 webhook 后端，请按照 Falco 存储库中的说明操作([https://github . com/falcosecurity/evolution/tree/master/examples/k8s _ audit _ config](https://github.com/falcosecurity/evolution/tree/master/examples/k8s_audit_config))。

法尔科规则有三种类型的元素，如下所示:

*   **规则**:在下将触发警报的一种情况。规则具有以下属性:规则名称、描述、条件、优先级、来源、标签和输出。当事件符合任何规则的条件时，会根据规则的输出定义生成警报。
*   **宏**:可以被其他规则或宏重用的规则条件片段。
*   **列表**:可以被宏和规则使用的项目集合。

为了方便 Falco 用户建立自己的规则，Falco 提供了一些默认列表和宏。

### 创建系统调用规则

Falco 系统调用规则评估系统调用事件——更准确地说，是丰富的系统调用。系统调用事件字段由内核模块提供，与 Sysdig(由 Sysdig 公司构建的开源工具)过滤器字段相同。策略引擎使用 Sysdig 的过滤器从系统调用事件中提取进程名、容器映像和文件路径等信息，并用 Falco 规则对其进行评估。

以下是最常见的 Sysdig 过滤器字段，可用于构建 Falco 规则:

*   **工艺名称**:工艺名称
*   **文件名**:写入或读取的文件名
*   **容器号**:容器号
*   **Container . image . repository**:无标签的容器图像名称
*   **fd.sip 和 fd.sport** :服务器**互联网协议** ( **IP** )地址和服务器端口
*   **fd.cip 和 fd.cport** :客户端 ip 和客户端端口
*   **evt.type** :系统调用事件(`open`、`connect`、`accept`、`execve`等)

让我们试着建立一个简单的法尔科规则。假设您有一个`nginx` pod，仅服务于`/usr/share/nginx/html/`目录中的静态文件。因此，您可以创建一个 Falco 规则来检测任何异常文件读取活动，如下所示:

```
    - rule: Anomalous read in nginx pod
      desc: Detect any anomalous file read activities in Nginx pod.
      condition: (open_read and container and container.image.repository="kaizheh/insecure-nginx" and fd.directory != "/usr/share/nginx/html")
      output: Anomalous file read activity in Nginx pod (user=%user.name process=%proc.name file=%fd.name container_id=%container.id image=%container.image.repository)
      priority: WARNING
```

前面的规则使用了两个默认宏:`open_read`和`container`。`open_read`宏检查系统调用事件是否以只读模式打开，而`container`宏检查系统调用事件是否发生在容器内。然后，该规则仅适用于运行`kaizheh/insecure-nginx`图像的容器，并且`fd.directory`过滤器从系统调用事件中检索文件目录信息。在该规则中，它检查在`/usr/share/nginx/html/`目录之外是否有任何文件被读取。那么，如果`nginx`配置错误导致文件路径遍历(读取任意目录下的文件)怎么办？这方面的一个例子显示在下面的代码块中:

```
# curl insecure-nginx.insecure-nginx.svc.cluster.local/files../etc/passwd
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/bin/false
```

同时，Falco 检测到指定目录之外的文件访问，输出如下:

```
08:22:19.484698397: Warning Anomalous file read activity in Nginx pod (user=<NA> process=nginx file=/etc/passwd container_id=439e2e739868 image=kaizheh/insecure-nginx) k8s.ns=insecure-nginx k8s.pod=insecure-nginx-7c99fdf44b-gffp4 container=439e2e739868 k8s.ns=insecure-nginx k8s.pod=insecure-nginx-7c99fdf44b-gffp4 container=439e2e739868
```

接下来我们来看看如何使用 K8s 审核规则。

### 创建 K8s 审核规则

K8s 审核规则评估 Kubernetes 审核事件。在本章的前面，我们已经展示了 Kubernetes 审计事件记录的样子。与 Sysdig 过滤器类似，有两种方法可以从 Kubernetes 审核事件中检索信息。一种是使用 **JavaScript 对象符号** ( **JSON** )指针；另一种是使用 Falco 内置滤镜。以下是一些常用的 Falco 内置过滤器，用于检索 Kubernetes 审计事件的信息:

*   `ka.verb`:Kubernetes 审核事件的动词字段。`jevt.value[/verb]`是其对应的 JSON 指针。
*   `ka.target.resource`:Kubernetes 审核事件的资源字段。`jevt.value[/objectRef/resource]`是其对应的 JSON 指针。
*   `ka.user.name`:Kubernetes 审核事件的用户名字段。`jevt.value[/user/username]`是其对应的 JSON 指针。
*   `ka.uri`:Kubernetes 审核事件的`requestURI`字段。`jet.value[/requestURI]`是它对应的 JSON 指针。

让我们尝试构建一个简单的 K8s 审计规则。假设您不想在`kube-system`名称空间中部署映像，除了少数用于服务的可信映像，如`kube-apiserver`、`etcd-manager`等。因此，您可以创建一个法尔科规则，如下所示:

```
- list: trusted_images
  items: [calico/node, kopeio/etcd-manager, k8s.gcr.io/kube-apiserver, k8s.gcr.io/kube-controller-manager, k8s.gcr.io/kube-proxy, k8s.gcr.io/kube-scheduler]
- rule: Untrusted Image Deployed in kube-system Namespace
  desc: >
    Detect an untrusted image deployed in kube-system namespace
  condition: >
    kevt and pod
    and kcreate
    and ka.target.namespace=kube-system
    and not ka.req.pod.containers.image.repository in (trusted_images)
  output: Untrusted image deployed in kube-system namespace (user=%ka.user.name image=%ka.req.pod.containers.image.repository resource=%ka.target.name)
  priority: WARNING
  source: k8s_audit
  tags: [k8s]
```

首先，我们定义一个允许在`kube-system`命名空间中部署的可信映像列表。在规则中，我们使用两个默认宏:`pod`和`kcreate`。`pod`宏检查目标资源是否是 pod，而`kcreate`检查动词是否是`create`。我们还会检查目标名称空间是否为`kube-system`，部署映像是否不在`trusted_images`列表中。该规则的`source`字段中的`k8s_audit`值表示该规则评估 Kubernetes 审核事件。然后，如果我们尝试在`kube-system`命名空间中部署一个`busybox`映像包，我们将会看到来自 Falco 的以下警报:

```
21:47:15.063915008: Warning Untrusted image deployed in kube-system namespace (user=admin image=busybox resource=pod-1)
```

请注意，为了使该规则起作用，pod 创建的审核级别至少需要在`Request`级别，审核事件包括 pod 的规范信息，如图像。

在本节中，我们介绍了 Falco，并向您展示了如何从两个事件源创建 Falco 规则:系统调用和 Kubernetes 审计事件。这两个规则都用于根据工作负载或集群的已知良性活动来检测异常活动。接下来，我们来谈谈如何在 Kubernetes 集群中进行取证。

# 与 Sysdig Inspect 和 CRIU 进行取证

网络安全中的取证意味着收集、处理和分析信息，以支持漏洞缓解和/或欺诈、反情报或法律实施调查。您可以保留的数据越多，对收集的数据进行分析的速度越快，您就能越快地追踪攻击并更好地响应事件。在这一节中，我们将向您展示如何使用 CRIU 和 Sysdig 开源工具来收集数据，然后介绍 Sy sdig Inspect，一个用于分析 Sysdig 收集的数据的开源工具。

## 利用 CRIU 收集数据

**CRIU** 是**用户空间检查点和恢复**的缩写。它是一个可以冻结正在运行的容器并在磁盘上捕获容器状态的工具。稍后，保存在磁盘上的容器和应用的数据可以恢复到冻结时的状态。它对于容器快照、迁移和远程调试非常有用。从安全角度来看，捕获容器中正在运行的恶意活动(这样您就可以在检查点之后立即终止容器)并在沙盒环境中恢复状态以供进一步分析，这一点尤其有用。

CRIU 作为 Docker 插件工作，目前仍处于实验模式，存在一个已知问题，即 CRIU 在最近几个版本([https://github.com/moby/moby/issues/37344](https://github.com/moby/moby/issues/37344))中工作不正常。出于演示的目的，我使用了一个旧的 Docker 版本(Docker CE 17.03)，并将展示如何使用 CRIU 检查正在运行的容器，并将状态恢复为新容器。

要启用 CRIU，您需要在 Docker 守护程序中启用`experimental`模式，如下所示:

```
echo "{\"experimental\":true}" >> /etc/docker/daemon.json
```

然后，重新启动 Docker 守护程序后，您应该能够成功执行`docker checkpoint`命令，如下所示:

```
# docker checkpoint
Usage:	docker checkpoint COMMAND
Manage checkpoints
Options:
      --help   Print usage
Commands:
  create      Create a checkpoint from a running container
  ls          List checkpoints for a container
  rm          Remove a checkpoint
```

然后，按照说明安装 CRIU([https://criu.org/Installation](https://criu.org/Installation))。接下来，让我们看一个简单的例子来展示 CRIU 有多强大。我有一个简单的`busybox`容器，它每秒钟运行一次`1`来增加计数器，如下面的代码片段所示:

```
# docker run -d --name looper --security-opt seccomp:unconfined busybox /bin/sh -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done'
91d68fafec8fcf11e7699539dec0b037220b1fcc856fb7050c58ab90ae8cbd13
```

睡了几秒钟后，我看到计数器的输出增加，如下所示:

```
# sleep 5
# docker logs looper
0
1
2
3
4
5
```

接下来，我想检查容器并将状态存储到本地文件系统，如下所示:

```
# docker checkpoint create --checkpoint-dir=/tmp looper checkpoint
checkpoint
```

现在`checkpoint`状态已经保存在`/tmp`目录下。请注意，容器活套将在检查点之后被杀死，除非您在创建检查点时指定了`--leave-running`标志。

然后，创建一个镜像容器，但不运行它，如下所示:

```
# docker create --name looper-clone --security-opt seccomp:unconfined busybox /bin/sh -c 'i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done'
49b9ade200e7da6bbb07057da02570347ad6fefbfc1499652ed286b874b59f2b
```

现在，我们可以以存储状态启动新的`looper-clone`容器。让我们再等几秒钟，看看会发生什么。结果可以在下面的代码片段中看到:

```
# docker start --checkpoint-dir=/tmp --checkpoint=checkpoint looper-clone
# sleep 5
# docker logs looper-clone
6
7
8
9
10
```

新`looper-clone`容器从`6`开始计数，表示状态(计数器为`5`)成功恢复使用。

CRIU 对于容器取证非常有用，尤其是当容器内发生一些可疑活动时。您可以检查容器(假设集群中运行多个副本)，让 CRIU 杀死可疑容器，然后在沙箱环境中恢复容器的可疑状态，以便进一步分析。接下来，让我们讨论另一种捕获数据进行取证的方法。

## 使用系统挖掘和系统挖掘检查

Sysdig 是 Linux 系统探索和故障排除的开源工具，支持容器。Sysdig 还可以通过检测 Linux 内核并捕获系统调用和其他操作系统事件来创建系统活动的跟踪文件。捕获功能使其成为容器化环境中令人敬畏的取证工具。为了支持 Kubernetes 集群中的捕获系统调用，Sysdig 提供了一个`kubectl`插件`kubectl-capture`，它使你能够像使用其他一些`kubectl`命令一样简单地捕获目标Pod 的系统调用。捕获完成后，可以使用功能强大的开源工具 Sysdig Inspect 进行故障排除和安全调查。

让我们继续以`insecure-nginx`为例，因为我们得到了一个 Falco 警报，如下面的代码片段所示:

```
08:22:19.484698397: Warning Anomalous file read activity in Nginx pod (user=<NA> process=nginx file=/etc/passwd container_id=439e2e739868 image=kaizheh/insecure-nginx) k8s.ns=insecure-nginx k8s.pod=insecure-nginx-7c99fdf44b-gffp4 container=439e2e739868 k8s.ns=insecure-nginx k8s.pod=insecure-nginx-7c99fdf44b-gffp4 container=439e2e739868
```

当警报被触发时，仍然有可能`nginx`Pod 正在遭受攻击。有几件事可以做回应。开始捕捉然后分析法尔科警报的更多背景是其中之一。

要触发捕获，请从[https://github.com/sysdiglabs/kubectl-capture](https://github.com/sysdiglabs/kubectl-capture)下载`kubectl-capture`，并将其与其他`kubectl`插件放在一起，如下所示:

```
$ kubectl plugin list
The following compatible plugins are available:
/Users/kaizhehuang/.krew/bin/kubectl-advise_psp
/Users/kaizhehuang/.krew/bin/kubectl-capture
/Users/kaizhehuang/.krew/bin/kubectl-ctx
/Users/kaizhehuang/.krew/bin/kubectl-krew
/Users/kaizhehuang/.krew/bin/kubectl-ns
/Users/kaizhehuang/.krew/bin/kubectl-sniff
```

然后，在`nginx`Pod 上开始捕获，如下所示:

```
$ kubectl capture insecure-nginx-7c99fdf44b-4fl5s -ns insecure-nginx
Sysdig is starting to capture system calls:
Node: ip-172-20-42-49.ec2.internal
Pod: insecure-nginx-7c99fdf44b-4fl5s
Duration: 120 seconds
Parameters for Sysdig: -S -M 120 -pk -z -w /capture-insecure-nginx-7c99fdf44b-4fl5s-1587337260.scap.gz
The capture has been downloaded to your hard disk at:
/Users/kaizhehuang/demo/chapter11/sysdig/capture-insecure-nginx-7c99fdf44b-4fl5s-1587337260.scap.gz
```

在引擎盖下，`kubectl-capture`启动一个新的Pod ，在疑似受害者Pod 运行的主机上进行捕获，捕获持续时间为`120`-秒，这样我们就可以看到此时此刻和下一个`120`秒在该主机上发生的一切。一旦捕获完成，压缩的捕获文件将被创建在当前工作的目录中。您可以引入 Sysdig Inspect 作为 Docker 容器来启动安全调查，如下所示:

```
$ docker run -d -v /Users/kaizhehuang/demo/chapter11/sysdig:/captures -p3000:3000 sysdig/sysdig-inspect:latest
17533f98a947668814ac6189908ff003475b10f340d8f3239cd3627fa9747769
```

现在登录`http://localhost:3000`，应该会看到登录**用户界面** ( **UI** )。请记住解压缩`scap`文件，这样您应该能够看到捕获文件的概览页面，如下所示:

![Figure 11.5 – Sysdig Inspect overview ](img/B15566_11_005.jpg)

图 11.5–系统挖掘检查概述

Sysdig Inspect 从以下角度提供了对容器内部活动的全面了解:

*   执行的命令
*   文件存取
*   网络连接
*   系统调用

让我们比法尔科警报做更多的挖掘。从警报中，我们可能会怀疑这是文件路径遍历问题，因为它是访问`/etc/passwd`文件的`nginx`进程，我们知道这个 pod 只服务静态文件，因此`nginx`进程永远不会访问`/usr/share/nginx/html/`目录之外的任何文件。现在，让我们看看下面的截图，看看发送到`nginx`Pod 的网络请求是什么:

![Figure 11.6 – Sysdig Inspect investigating network connections to nginx ](img/B15566_11_006.jpg)

图 11.6–系统挖掘检查调查与 nginx 的网络连接

查看连接后，我们看到请求来自单个 IP，`100.123.226.66`，看起来像一个 pod IP。会不会是来自同一个集群？单击左侧面板上的**容器**视图，并在过滤器中指定`fd.cip=100.123.226.66`。然后，你会发现它来自`anchore-cli`容器，如下图所示:

![Figure 11.7 – Sysdig Inspect investigating a container sending a request to nginx ](img/B15566_11_007.jpg)

图 11.7–Sysdig Inspect 调查向 nginx 发送请求的容器

`anchore-cli` pod 实际上恰好与运行在与`nginx` pod 相同的节点上，如下面的代码块所示:

```
$ kubectl get pods -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP               NODE                           NOMINATED NODE   READINESS GATES
anchore-cli   1/1     Running   1          77m   100.123.226.66   ip-172-20-42-49.ec2.internal   <none>           <none>
$ kubectl get pods -n insecure-nginx -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE                           NOMINATED NODE   READINESS GATES
insecure-nginx-7c99fdf44b-4fl5s   1/1     Running   0          78m   100.123.226.65   ip-172-20-42-49.ec2.internal   <none>           <none>
```

现在我们知道可能有一些从`anchore-cli`Pod 发起的文件路径遍历攻击，让我们看看是什么(只需双击前面 **Sysdig Inspect** 页面中的条目)，如下所示:

![Figure 11.8 – Sysdig Inspect investigating path traversal attack commands ](img/B15566_11_008.jpg)

图 11.8–Sysdig Inspect 调查路径遍历攻击命令

我们发现在`anchore-cli`窗格中有执行的文件路径遍历命令列表，详细如下:

*   `curl 100.71.138.95/files../etc/`
*   `curl 100.71.138.95/files../`
*   `curl 100.71.138.95/files../etc/passwd`
*   `curl 100.71.138.95/files../etc/shadow`

我们现在能够离攻击者更近一步，下一步是尝试更多地调查攻击者是如何降落在`anchore-cli`Pod 中的。

CRIU 和 Sysdig 都是在容器化环境中进行取证的强大工具。CRIU 问题有望很快得到解决。请注意，CRIU 还要求 Docker 守护程序在`experimental`模式下运行，而 Sysdig 和 Sysdig Inspect 在 Kubernetes 级别工作得更多。Sysdig Inspect 为提供了一个很好的用户界面，帮助浏览在Pod 和容器中发生的不同活动。

# 总结

在这漫长的一章中，我们介绍了 Kubernetes 审计、Kubernetes 集群的高可用性、使用 Vault 管理机密、使用 Falco 检测异常活动以及使用 CRIU 和 Sysdig 进行取证。虽然您可能会发现熟悉所有的实践和工具需要相当长的时间，但是深度防御是一个巨大的主题，值得深入挖掘安全性，以便为您的 Kubernetes 集群建立更牢固的防护。

我们谈到的大多数工具都易于安装和部署。我鼓励您尝试一下:添加您自己的 Kubernetes 审核规则，使用 Vault 管理 Kubernetes 集群中的机密，构建您自己的 Falco 规则来检测异常行为，因为您比任何人都更了解您的集群，并使用 Sysdig 收集所有取证数据。一旦您熟悉了所有这些工具，您应该相信您的 Kubernetes 集群会受到更多的控制。

在下一章中，我们将讨论一些已知的攻击，例如针对 Kubernetes 集群的加密挖掘黑客攻击，并了解如何使用我们在本书中学习的技术来减轻这些攻击。

# 问题

1.  为什么不应该将保密对象的审核级别设置为`Request`或`RequestResponse`？
2.  在`kops`中设置多个主节点使用什么标志？
3.  当一个机密在 Vault 中更新时，边车容器会做什么？
4.  法尔科使用的事件来源有哪些？
5.  Falco 使用哪个过滤器从系统调用 l 事件中检索进程名？
6.  CRIU 能对正在运行的容器做什么？
7.  你能用 Sysdig Inspect 做什么？

# 进一步参考

*   Kubernetes 审核:[https://kubernetes . io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)
*   通过`kubeadm` : [实现高可用性](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)
*   金库:[https://www.vaultproject.io/docs/internals/architecture](https://www.vaultproject.io/docs/internals/architecture)
*   法尔科:https://falco.org/docs/
*   Sysdig 过滤:[https://github . com/DRA IOs/Sysdig/wiki/Sysdig-用户-指南#用户-内容-过滤](https://github.com/draios/sysdig/wiki/Sysdig-User-Guide#user-content-filtering)
*   大喊:https://criu . org/dock
*   sys dig`kubectl-capture`:[https://sys dig . com/blog/tracing-in-kubi treble-capture 插件/](https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/)
*   Sysdig 检查:https://github . com/draos/sys dig 检查
*   sys dig:https://github . com/draos/sys dig**