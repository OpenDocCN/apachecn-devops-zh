# *第八章*:吊舱放置控制

本章描述了在 Kubernetes 中控制 Pod 放置的各种方法，并解释了为什么首先实现这些控制可能是一个好主意。Pod 放置意味着控制 Pod 在 Kubernetes 中被调度到哪个节点。我们从像节点选择器这样的简单控件开始，然后转向像污点和容忍这样的更复杂的工具，最后是两个 beta 特性，节点相似性和 Pod 间相似性/反相似性。

在过去的章节中，我们已经了解了如何在 Kubernetes 上最好地运行应用程序 Pods–从使用部署来协调和扩展它们，使用配置映射和机密注入配置，到使用持久卷来添加存储。

然而，在所有这些过程中，我们一直依赖于 Kubernetes 调度程序来将 Pods 放在最佳节点上，而没有给调度程序很多关于所讨论的 Pods 的信息。到目前为止，我们已经在 Pod 中添加了资源限制和请求(`resource.requests`和`resource.limits`)。资源请求指定了 Pod 需要的节点上的最小空闲资源级别，以便进行调度，而资源限制则指定了 Pod 允许使用的最大资源量。但是，我们没有对 Pod 必须运行的节点或节点集提出任何具体要求。

对于许多应用程序和集群来说，这很好。然而，正如我们将在第一节中看到的，在许多情况下，使用更细粒度的 Pod 放置控件是一种有用的策略。

在本章中，我们将涵盖以下主题:

*   确定 Pod 放置的用例
*   使用节点选择器
*   实施污点和宽容
*   用节点相似性控制 Pods
*   使用 Pod 间亲和力和反亲和力

# 技术要求

为了运行本章中详细介绍的命令，您将需要一台支持`kubectl`命令行工具的计算机以及一个工作正常的 Kubernetes 集群。参见 [*第一章*](01.html#_idTextAnchor016)*与库本内特斯*通讯，了解几种快速与库本内特斯一起起床跑步的方法，以及如何安装`kubectl`工具的说明。

本章使用的代码可以在本书的 GitHub 资源库中找到[https://GitHub . com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/chapter 8](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8)。

# 确定 Pod 放置的用例

Pod 放置控件是 Kubernetes 提供给我们的工具，用于决定在哪个节点上调度 Pod，或者何时由于缺少我们想要的节点而完全阻止 Pod 调度。这可以用于几种不同的模式，但是我们将回顾几种主要的模式。首先，Kubernetes 本身在默认情况下完全实现了 Pod 放置控件——让我们看看如何实现。

## Kubernetes 节点健康放置控件

Kubernetes 使用一些默认的放置控件来指定哪些节点在某种程度上是不健康的。这些通常是使用污点和容忍来定义的，我们将在本章后面详细讨论。

Kubernetes 使用的一些默认污点(我们将在下一节中讨论)如下:

*   `memory-pressure`
*   `disk-pressure`
*   `unreachable`
*   `not-ready`
*   `out-of-disk`
*   `network-unavailable`
*   `unschedulable`
*   `uninitialized`(仅针对云提供商创建的节点)

这些条件可以将节点标记为无法接收新的 Pods，尽管调度程序处理这些污点的方式有一定的灵活性，我们将在后面看到。这些系统创建的放置控制的目的是防止不健康的节点接收可能无法正常工作的工作负载。

除了系统为节点运行状况创建的放置控制之外，还有几个用例，作为用户，您可能希望实现微调调度，我们将在下一节中看到。

## 需要不同节点类型的应用

在异构的 Kubernetes 集群中，每个节点不是平等创建的。您可能有一些功能更强大的虚拟机(或裸机)和一些功能更弱的虚拟机，或者有不同的专用节点集。

例如，在运行数据科学管道的集群中，您可能有具有 GPU 加速功能的节点来运行深度学习算法，有常规计算节点来服务于应用程序，有大量内存的节点来根据已完成的模型进行推理，等等。

使用 Pod 放置控件，您可以确保平台的各个部分在最适合手头任务的硬件上运行。

## 需要特定数据合规性的应用

类似于前面的示例，其中应用程序需求可能要求不同类型的计算，某些数据合规性需求可能要求特定类型的节点。

例如，AWS 和 Azure 等云提供商通常允许您购买具有专用租赁的虚拟机，这意味着底层硬件和虚拟机管理程序上不会运行其他应用程序。这不同于其他典型的云提供商虚拟机，在这些虚拟机中，多个客户可能共享一台物理机。

对于某些数据法规，需要此级别的专用租赁来保持合规性。为了满足这一需求，您可以使用 Pod 放置控制来确保相关应用程序仅在具有专用租赁的节点上运行，同时通过在没有控制平面的更典型虚拟机上运行控制平面来降低成本。

## 多租户集群

如果您正在运行一个具有多个租户的集群(例如，由名称空间分隔)，您可以使用 Pod 放置控件为一个租户保留特定的节点或节点组，以物理方式或其他方式将它们与集群中的其他租户分开。这类似于 AWS 或 Azure 中专用硬件的概念。

## 多个故障域

虽然 Kubernetes 已经通过允许您调度在多个节点上运行的工作负载来提供高可用性，但是扩展这种模式也是可能的。我们可以创建自己的 Pod 调度策略，解决跨多个节点的故障域。处理这个问题的一个很好的方法是通过 Pod 或节点相似性或反相似性特性，我们将在本章后面讨论。

现在，让我们将一个案例概念化，在这个案例中，我们的集群位于裸机上，每个物理机架有 20 个节点。如果每个机架都有自己的专用电源连接和备份，则可以将其视为故障域。当电源连接出现故障时，机架上的所有机器都会出现故障。因此，我们可能希望鼓励 Kubernetes 在单独的机架/故障域上运行两个实例或 Pods。下图显示了应用程序如何跨故障域运行:

![Figure 8.1 – Failure domains](img/B14790_08_001.jpg)

图 8.1–故障域

如图所示，由于应用程序单元分布在多个故障域中，而不仅仅是同一故障域中的多个节点，因此即使*故障域 1* 发生故障，我们也可以保持正常运行时间。*App-A-Pod 1*和*App-B- Pod 1*处于同一个(红色)故障域。但是，如果该故障域(*机架 1* )关闭，我们仍将在*机架 2* 上拥有每个应用程序的副本。

我们在这里使用“鼓励”这个词，因为在 Kubernetes 调度程序中，可以将某些功能配置为硬要求或尽力而为。

这些示例应该让您对高级放置控件的一些潜在用例有一个坚实的理解。

现在让我们逐一讨论每个放置工具集的实际实现。我们将从最简单的节点选择器开始。

# 使用节点选择器和节点名称

节点选择器是 Kubernetes 中一种非常简单的布局控件。每个 Kubernetes 节点都可以在元数据块中用一个或多个标签进行标记，Pods 可以指定一个节点选择器。

要标记现有节点，可以使用`kubectl label`命令:

```
> kubectl label nodes node1 cpu_speed=fast
```

在这个例子中，我们用标签`cpu_speed`和值`fast`来标记我们的`node1`节点。

现在，让我们假设我们有一个真正需要快速 CPU 周期来有效执行的应用程序。我们可以在工作负载中添加一个`nodeSelector`，以确保它只安排在具有快速 CPU 速度标签的节点上，如下面的代码片段所示:

带有节点选择器的 pod . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: speedy-app
spec:
  containers:
  - name: speedy-app
    image: speedy-app:latest
    imagePullPolicy: IfNotPresent
  nodeSelector:
    cpu_speed: fast
```

部署时，作为部署的一部分或单独部署，我们的`speedy-app` Pod 将仅在带有`cpu_speed`标签的节点上进行调度。

请记住，与我们稍后将回顾的一些其他更高级的 Pod 放置选项不同，节点选择器中没有回旋余地。如果没有具有所需标签的节点，则根本不会调度应用程序。

对于更简单(但更脆弱)的选择器，您可以使用`nodeName`，它指定了 Pod 应该被调度的确切节点。你可以这样使用它:

带有节点名的 pod . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: speedy-app
spec:
  containers:
  - name: speedy-app
    image: speedy-app:latest
    imagePullPolicy: IfNotPresent
  nodeName: node1
```

正如你所看到的，这个选择器将只允许吊舱被安排在`node1`上，所以如果它当前由于任何原因没有接受吊舱，吊舱将不会被安排。

对于稍微细致的位置控制，让我们继续讨论污点和容忍。

# 实施污染和容忍

Kubernetes 中的污点和容忍就像反向节点选择器一样工作。节点不会因为有适当的标签而吸引 Pods，这些标签会被选择器使用，我们会污染节点，这会阻止所有 Pods 在节点上被调度，然后用容忍标记我们的 Pods，这允许它们在被污染的节点上被调度。

正如本章开头提到的，Kubernetes 使用系统创建的污点将节点标记为不健康，并阻止在它们上面安排新的工作负载。例如，`out-of-disk`污点将阻止任何新的豆荚被安排到带有该污点的节点。

让我们使用与节点选择器相同的用例，并使用污点和容忍来应用它。由于这基本上与我们之前的设置相反，让我们首先使用`kubectl taint`命令给节点一个污点:

```
> kubectl taint nodes node2 cpu_speed=slow:NoSchedule
```

让我们把这个命令拆开。我们给`node2`一个名为`cpu_speed`的污点和一个值`slow`。我们也用一个效果来标记这个污点——在这个例子中是`NoSchedule`。

一旦我们完成了我们的示例(如果您正在跟随命令，请不要这样做)，我们可以使用减运算符删除`taint`:

```
> kubectl taint nodes node2 cpu_speed=slow:NoSchedule-
```

`taint`效果让我们在调度器处理污点的方式上增加了一些粒度。有三种可能的影响值:

*   `NoSchedule`
*   `NoExecute`
*   `PreferNoSchedule`

前两个效果`NoSchedule`和`NoExecute`提供了硬效果——也就是说，像节点选择器一样，只有两种可能性，要么在吊舱上存在容忍(我们稍后会看到)，要么吊舱没有被调度。`NoExecute`通过驱逐节点上所有有容忍能力的豆荚来增加这个基本功能，而`NoSchedule`让现有豆荚留在原地，同时防止任何没有容忍能力的新豆荚加入。

`PreferNoSchedule`另一方面，为 Kubernetes 调度器提供了一些回旋余地。它告诉调度程序尝试为 Pod 找到一个没有不可容忍的污点的节点，但是如果不存在污点，继续进行调度。它实现了柔和的效果。

在我们的例子中，我们选择了`NoSchedule`，所以不会给节点分配新的 Pods 当然，除非我们提供了一个宽容。我们现在就开始吧。假设我们有第二个不关心 CPU 时钟速度的应用程序。它很高兴生活在我们较慢的节点上。这是吊舱清单:

无速度要求的吊舱

```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
```

现在，我们的`slow-app` Pod 不会在任何有污点的节点上运行。我们需要为这个 Pod 提供一个容忍度，以便将其安排在有污点的节点上——我们可以这样做:

宽容的豆荚

```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Equal"
  value: "slow"
  effect: "NoSchedule"
```

让我们分离我们的`tolerations`条目，它是一个值数组。每个值都有一个`key`，这和我们的污点名称是一样的。然后还有一个`operator`值。这个`operator`可以是`Equal`也可以是`Exists`。对于`Equal`，您可以像前面的代码一样使用`value`键来配置一个污点必须相等的值，以便 Pod 能够容忍。对于`Exists`，污点名称必须在节点上，但值是什么并不重要，如本 Pod 规范中所示:

宽容的豆荚 2.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Exists"
  effect: "NoSchedule"
```

如你所见，我们已经使用`Exists` `operator`值来允许我们的吊舱容忍任何`cpu_speed`污染。

最后，我们有了我们的`effect`，它的工作方式与污点本身的`effect`相同。它可以包含与污点效应完全相同的值–`NoSchedule`、`NoExecute`和`PreferNoSchedule`。

一个具有`NoExecute`耐受性的豆荚将无限期地忍受与之相关的污染。但是，您可以添加一个名为`tolerationSeconds`的字段，以便让 Pod 在规定的时间过后离开受感染的节点。这允许您指定一段时间后生效的容忍。让我们看一个例子:

宽容的豆荚 3.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Equal"
  Value: "slow"
  effect: "NoExecute"
  tolerationSeconds: 60
```

在这种情况下，当执行污点和容忍时，已经在带有污点`slow`的节点上运行的 Pod 将在节点上保留`60`秒，然后被重新调度到不同的节点。

## 多重污染和耐受性

当一个 Pod 和节点上存在多个污点或容忍时，调度程序将检查所有这些污点或容忍。这里没有`OR`逻辑运算符——如果节点上的任何污点在 Pod 上没有匹配的容忍性，它将不会在节点上被调度(除了`PreferNoSchedule`，在这种情况下，如前所述，如果可能，调度器将尝试不在节点上调度)。即使节点上的六个污点中，豆荚可以容忍其中的五个，它仍然不会被安排`NoSchedule`污点，并且它仍然会因为`NoExecute`污点而被驱逐。

对于一个给我们一个更微妙的方法来控制放置的工具，让我们看看节点亲和力。

# 用节点亲和性控制荚果

正如你所知可能会说的，污点和容忍——虽然比节点选择器灵活得多——仍然没有解决一些用例，通常只允许一个*过滤器*模式，你可以使用`Exists`或`Equals`在特定污点上进行匹配。可能会有更高级的用例，在这些用例中，您需要更灵活的选择节点的方法，而*亲缘关系*是 Kubernetes 解决这个问题的一个特性。

有两种类型的相似性:

*   **节点亲和力**
*   **荚果间亲和性**

节点相似性是一个类似于节点选择器的概念，除了它允许一组更健壮的选择特征。让我们看一些 YAML 的例子，然后挑选出不同的部分:

pod-with-node-affinity.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: In
            values:
            - fast
            - medium_fast
  containers:
  - name: speedy-app
    image: speedy-app:latest
```

如您所见，我们的`Pod` `spec`有一个`affinity`键，我们已经指定了一个`nodeAffinity`设置。有两种可能的节点亲缘关系类型:

*   `requiredDuringSchedulingIgnoredDuringExecution`
*   `preferredDuringSchedulingIgnoredDuringExecution`

这两种类型的功能分别直接映射到`NoSchedule`和`PreferNoSchedule`的工作方式。

## 使用 requireduringschedulingnoredduringeexecution 节点关联性

对于`requiredDuringSchedulingIgnoredDuringExecution`，如果没有与节点匹配的术语，库本内斯将永远不会调度 Pod 。

对于`preferredDuringSchedulingIgnoredDuringExecution`，它将尝试满足软要求，但如果不能，它仍将调度 Pod。

节点相似性超过节点选择器、污点和容忍的真正能力来自于实际的表达式和逻辑，当涉及到选择器时，您可以实现它们。

`requiredDuringSchedulingIgnoredDuringExecution`和`preferredDuringSchedulingIgnoredDuringExecution`相似性的功能有很大的不同，所以我们将分别回顾它们。

对于我们的`required`相似性，我们有能力指定`nodeSelectorTerms`-它可以是一个或多个包含`matchExpressions`的块。对于`matchExpressions`的每个区块，可以有多个表达式。

在我们上一节看到的代码块中，我们有一个单节点选择器项，即`matchExpressions`块，它本身只有一个表达式。这个表达式寻找`key`，就像节点选择器一样，它代表一个节点标签。接下来，它有一个`operator`，这给了我们一些灵活的方法来识别匹配。以下是运算符的可能值:

*   `In`
*   `NotIn`
*   `Exists`
*   `DoesNotExist`
*   `Gt`(注:大于)
*   `Lt`(注:小于)

在我们的例子中，我们正在使用`In`运算符，该运算符将检查该值是否是我们指定的几个值之一。最后，在我们的`values`部分，我们可以根据运算符列出一个或多个必须匹配的值，然后表达式为真。

如您所见，这为我们指定选择器提供了更大的粒度。让我们看看我们使用不同运算符的例子`cpu_speed`:

带有节点的 pod-affinity 2 . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```

如你所见，我们正在使用一个非常精细的`matchExpressions`选择器。现在，这种使用更高级操作员匹配的能力使我们能够确保我们的`speedy-app`仅在具有足够高时钟速度(在本例中为 5 GHz)的节点上调度。不需要将我们的节点分成像`slow`和`fast`这样的大组，我们可以更加细化我们的规范。

接下来，让我们看看另一个节点关联类型–`preferredDuringSchedulingIgnoredDuringExecution`。

## 使用 preferredduringschedulingnoredduringeexecution 节点关联性

这个的语法略有不同，给了我们更多的粒度来影响这个`soft`需求。让我们看看实现这一点的 Pod 规范 YAML:

带有节点的 pod-affinity 3 . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: slow-app-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Lt
            values:
            - "3"
  containers:
  - name: slow-app
    image: slow-app:latest
```

这个看起来有点不同于我们的`required`语法。

对于`preferredDuringSchedulingIgnoredDuringExecution`，我们有能力为每个条目分配一个`weight`，以及一个相关的偏好，它也可以是一个`matchExpressions`块，具有多个使用相同`key-operator-values`语法的内部表达式。

`weight`值是这里的关键区别。由于`preferredDuringSchedulingIgnoredDuringExecution`是一个**软**需求，我们可以列出几个不同的偏好和相关的权重，让调度器尽力满足它们。这种方法的原理是，调度器将检查所有的首选项，并根据每个首选项的权重以及它是否被满足来计算节点的分数。假设所有硬性要求都得到满足，调度器将选择计算出的最高分数的节点。在前面的例子中，我们有一个权重为 1 的偏好，但是权重可以是 1 到 100 之间的任何值，所以让我们看看我们的`speedy-app`用例的更复杂的设置:

带有节点的 pod-affinity 4 . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: speedy-app-prefers-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "3"
      - weight: 10
        preference:
          matchExpressions:
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```

在我们确保我们的`speedy-app`在尽可能好的节点上运行的旅程中，我们在这里决定只实现`soft`的要求。如果没有快速节点存在，我们仍然希望我们的应用程序被调度和运行。为此，我们指定了两个首选项——T2 超过 3 千兆赫的节点和超过 4 千兆赫的内存速度。

由于我们的应用程序受 CPU 的限制远多于受内存的限制，我们决定适当地权衡我们的偏好。在这种情况下，`cpu_speed`携带`90`的`weight`，而`memory_speed`携带`10`的`weight`。

因此，任何满足我们的`cpu_speed`要求的节点将比只满足`memory_speed`要求的节点具有高得多的计算分数，但仍然低于同时满足这两个要求的节点。当我们试图为这个应用程序安排 10 或 100 个新的 Pods 时，你可以看到这个计算是多么有价值。

## 多节点亲和力

当我们处理与多个节点的亲缘关系时，有几个关键的逻辑需要记住。首先，即使有单个节点相似性，如果它与同一 Pod 规范上的节点选择器相结合(这确实是可能的)，在任何节点相似性逻辑发挥作用之前，必须满足节点选择器。这是因为节点选择器只实现硬需求，两者之间没有`OR`逻辑运算符。一个`OR`逻辑运算符将检查这两个需求，并确保其中至少有一个是真的——但是节点选择器不让我们这样做。

其次，对于一个`requiredDuringSchedulingIgnoredDuringExecution`节点亲缘关系，`nodeSelectorTerms`下的多个条目在一个`OR`逻辑运算符中处理。如果满足一个，但不是全部，Pod 仍将被调度。

最后，对于任何在`matchExpressions`下有多个条目的`nodeSelectorTerm`，都必须满足——这是一个`AND`逻辑运算符。我们来看一个 YAML 的例子:

带节点的 pod-affinity 5 . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest
```

在这种情况下，如果一个节点的中央处理器速度为`5`，但不满足内存速度要求(反之亦然)，Pod 将不会被调度。

关于节点关联性，最后要注意的一点是，正如您可能已经注意到的，这两种关联性类型都不允许我们在污点和容忍设置中可以使用的相同的`NoExecute`功能。

一个额外的节点相似性类型–`requiredDuringSchedulingRequiredDuring execution`–将在未来版本中添加此功能。截止到 Kubernetes 1.19，这还不存在。

接下来，我们将研究 pod 之间的亲缘关系和反亲缘关系，后者提供 pod 之间的亲缘关系定义，而不是为节点定义规则。

# 利用荚果间亲和性和抗亲和性

荚间相似性和反相似性允许您根据节点上已经存在的其他荚来决定荚应该如何运行。由于集群中的 pods 数量通常比节点数量大得多，并且一些 Pods 相似性和反相似性规则可能有些复杂，因此如果您在多个节点上运行多个 Pods，该功能会给集群控制平面带来很大的负载。因此，Kubernetes 文档不建议在集群中的大量节点上使用这些功能。

荚果亲和力和反亲和力的工作原理完全不同——在讨论它们如何结合之前，让我们先单独看看它们。

## 荚果亲和力

与节点相似性一样，让我们深入 YAML，讨论 Pod 相似性规范的组成部分:

pod-with-pod-affinity.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: hunger
            operator: In
            values:
            - "1"
            - "2"
        topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest
```

就像节点关联性一样，Pod 关联性允许我们在两种类型之间进行选择:

*   `preferredDuringSchedulingIgnoredDuringExecution`
*   `requiredDuringSchedulingIgnoredDuringExecution`

同样，类似于节点相似性，我们可以有一个或多个选择器——称为`labelSelector`，因为我们选择的是 Pods，而不是节点。`matchExpressions`功能与节点相似性相同，但 Pod 相似性增加了一个名为`topologyKey`的全新密钥。

`topologyKey`本质上是一个选择器，它限制了调度程序应该查看同一选择器的其他 Pods 是否正在运行的范围。这意味着 Pod 亲缘关系不仅仅意味着同一节点上的其他相同类型的 Pod(选择器)；它可以表示多个节点的组。

让我们回到本章开头的失败域示例。在该示例中，每个机架都是自己的故障域，每个机架有多个节点。为了将这个概念扩展到`topologyKey`，我们可以用`rack=1`或`rack=2`标记机架上的每个节点。然后我们可以使用`topologyKey`机架，就像我们在 YAML 一样，指定调度程序应该检查在具有相同`topologyKey`的节点上运行的所有 Pods(在这种情况下，这意味着在相同机架中的`Node 1`和`Node 2`上的所有 Pods)，以便应用 Pods 相似性或反相似性规则。

综上所述，我们的例子 YAML 告诉调度程序的是:

*   此 Pod *必须在标签为`rack`的节点上进行调度，标签的值`rack`将节点分成组。*
*   然后，该 Pod 将被安排在一个组中，该组中已经存在一个运行标签为`hunger`且值为 1 或 2 的 Pod。

本质上，我们将集群划分为拓扑域(在本例中是机架)，并规定调度程序只在共享相同拓扑域的节点上一起调度相似的单元。这与我们的第一个故障域示例相反，在该示例中，如果可能的话，我们不希望 pods 共享同一个域，但也有一些原因可能会让您希望将类似的 pods 保留在同一个域中。例如，在多租户环境中，租户希望在一个域上拥有专用硬件租户，您可以确保属于某个租户的每个 Pod 都被调度到完全相同的拓扑域。

同样可以使用`preferredDuringSchedulingIgnoredDuringExecution`。在我们讨论的反亲和力之前，这里有一个关于荚果亲和力和`preferred`类型的例子:

pod-with-pod-affinity2.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: Lt
              values:
              - "3"
          topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest
```

和以前一样，在这个代码块中，我们使用小于(`Lt`)运算符将`weight`(在本例中为`50`)与表达式匹配。这种相似性将促使调度程序尽最大努力在它所在的节点上调度 Pod，或者在同一机架上的另一个节点上调度 Pod，该节点运行的 Pod 的`hunger`小于 3。调度程序使用`weight`来比较节点，如节点关联性一节中所述–*用节点关联性*控制 Pods(参见`pod-with-node-affinity4.yaml`)。具体在这个场景中，`50`的权重没有任何区别，因为亲缘关系列表中只有一个条目。

Pod 反亲和使用相同的选择器和拓扑扩展了这一范式——让我们详细看看它们。

## 荚果抗亲和性

吊舱反关联性允许您防止吊舱在与匹配选择器的吊舱相同的拓扑域上运行。它们实现了与 Pod 相似性相反的逻辑。让我们深入一些 YAML，并解释这是如何工作的:

pod-with-pod-anti-affinity . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
  containers:
  - name: hungry-app
    image: hungry-app
```

类似于 Pod 亲和，我们使用`affinity`键作为位置来指定我们在`podAntiAffinity`下的反亲和。此外，与 Pod 相似，我们有能力使用`preferredDuringSchedulingIgnoredDuringExecution`或`requireDuringSchedulingIgnoredDuringExecution`。我们甚至对选择器使用与 Pod 相似性相同的语法。

语法上唯一的实际区别是在`affinity`键下使用了`podAntiAffinity`。

那么，这个 YAML 是做什么的？在这种情况下，我们向调度程序推荐(一个`soft`要求)，它应该尝试在一个节点上调度这个 Pod，在这个节点上，它或任何其他具有相同`rack`标签值的节点没有任何运行有`hunger`标签值为 4 或 5 的 Pod。我们告诉调度程序*尽量不要将这个豆荚和任何额外的饥饿豆荚*放在同一个域中。

这一特性为我们提供了一种按故障域分离机架的好方法——我们可以将每个机架指定为一个域，并通过一个自己的选择器赋予它一个反关联性。这将使调度程序安排 Pod 的克隆(或者尝试以首选的相似性)到不在同一故障域中的节点，从而在域故障的情况下为应用程序提供更高的可用性。

我们甚至可以选择结合荚果亲和力和反亲和力。让我们看看这是如何工作的。

## 结合亲和力和反亲和力

这是一种情况，在这种情况下，您可以真正给集群控制平面施加不适当的负载。将 Pod 相似性与反相似性相结合，可以允许将难以置信的细微差别的规则传递给 Kubernetes 调度器，该调度器承担着实现这些规则的艰巨任务。

让我们看一些结合了这两个概念的 YAML 部署规范。请记住，相似性和反相似性是应用于 Pods 的概念，但是我们通常不指定没有控制器(如部署或复制集)的 Pods。因此，这些规则应用于 YAML 部署中的 Pod 规范级别。为了简明起见，我们只显示了这个部署的 Pod 规范部分，但是您可以在 GitHub 存储库中找到完整的文件:

抗肥胖和亲和力兼备的豆荚

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hungry-app-deployment
# SECTION REMOVED FOR CONCISENESS  
     spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - other-hungry-app
            topologyKey: "rack"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - hungry-app-cache
            topologyKey: "rack"
      containers:
      - name: hungry-app
        image: hungry-app:latest
```

在这个代码块中，我们告诉调度程序这样对待我们部署中的 Pod:Pod 必须被调度到带有`rack`标签的节点上，这样它或任何其他带有`rack`标签且相同值的节点就有一个带有`app=hungry-label-cache`的 Pod。

其次，如果可能的话，调度器必须尝试将 Pod 调度到具有`rack`标签的节点，使得它或具有`rack`标签和相同值的任何其他节点没有运行具有`app=other-hungry-app`标签的 Pod。

简而言之，我们希望`hungry-app`的吊舱运行在与 `hungry-app-cache`相同的拓扑中，如果可能的话，我们不希望它们运行在与`other-hungry-app`相同的拓扑中。

因为权力越大，责任越大，我们的 Pod 亲和和反亲和工具是强大和降低性能的同等部分，所以 Kubernetes 确保对您可以使用它们的可能方式设置一些限制，以防止奇怪的行为或重大的性能问题。

## Pod 亲和力和抗亲和力限制

亲和和反亲和最大的限制是不允许使用空白`topologyKey`。如果不限制调度程序将什么视为单个拓扑类型，可能会发生一些意想不到的行为。

第二个限制是，默认情况下，如果你使用的是反亲和的硬版本–`requiredOnSchedulingIgnoredDuringExecution`，你不能只使用任何标签作为`topologyKey`。

Kubernetes 只会让你使用`kubernetes.io/hostname`标签，这实质上意味着如果你使用`required`反亲和，每个节点只能有一个拓扑。这种限制对于`prefer`反亲和力或亲和力都不存在，甚至对于`required`也不存在。可以更改此功能，但需要编写自定义准入控制器–我们将在 [*第 12 章*](12.html#_idTextAnchor269)*Kubernetes 安全与合规*和 [*第 13 章*](13.html#_idTextAnchor289)*使用 CRDs 扩展 Kubernetes*中讨论。

到目前为止，我们关于放置控件的工作还没有讨论名称空间。然而，对于荚果亲和力和反亲和力，它们确实具有相关性。

## Pod 相似性和反相似性名称空间

由于 Pod 亲和力和反亲和力基于其他 Pod 的位置导致行为的改变，所以命名空间是决定哪个 Pod 支持或反对亲和力或反亲和力的相关部分。

默认情况下，调度程序将只查看创建具有相似性或反相似性的 Pod 的命名空间。对于前面的所有示例，我们没有指定名称空间，因此将使用默认名称空间。

如果您想添加一个或多个 Pods 将影响相似性或反相似性的命名空间，您可以使用以下 YAML:

pod-with-anti-affinity-namespace . YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
          namespaces: ["frontend", "backend", "logging"]
  containers:
  - name: hungry-app
    image: hungry-app
```

在这个代码块中，调度程序将在尝试匹配反关联性时查看前端、后端和日志名称空间(如您在`podAffinityTerm`块中的`namespaces`键上所见)。这允许我们在验证调度程序的规则时约束调度程序对哪些名称空间进行操作。

# 总结

在这一章中，我们了解了 Kubernetes 提供的一些不同的控件，以便通过调度程序强制执行某些 Pod 放置规则。我们了解到，既有“硬”要求，也有“软”规则，后者是调度程序尽了最大努力，但不一定能阻止违反规则的 Pods 被放置。我们还了解了您可能希望实施计划控制的几个原因，例如现实中的故障域和多租户。

我们了解到有一些简单的方法可以影响 Pod 的放置，比如节点选择器和节点名称——此外还有一些更高级的方法，比如污点和容忍，Kubernetes 本身也默认使用这些方法。最后，我们发现 Kubernetes 提供了一些高级工具，用于节点和 Pod 关联以及反关联，这允许我们创建复杂的规则集供调度程序遵循。

在下一章中，我们将讨论 Kubernetes 上的可观测性。我们将学习如何查看应用程序日志，我们还将使用一些优秀的工具来实时查看集群中发生的情况。

# 问题

1.  节点选择器和节点名称字段有什么区别？
2.  Kubernetes 如何使用系统提供的污点和容忍？什么原因？
3.  为什么在使用多种类型的 Pod 亲和力或反亲和力时要小心？
4.  对于三层 web 应用程序，出于性能原因，您如何平衡跨多个故障区域的可用性和托管？给出一个使用节点或 Pod 亲和力和反亲和力的例子。

# 进一步阅读

*   有关默认系统污点和容忍的更深入的解释，请前往[https://kubernetes . io/docs/concepts/scheduling-驱逐/污点和容忍/#基于污点的驱逐](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions)。