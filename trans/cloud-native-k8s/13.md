# *第十三章*:用 CRDs 扩展库本内特

本章解释了扩展 Kubernetes 功能的许多可能性。它从开始讨论**自定义资源定义** ( **CRD** )，这是一种 Kubernetes-native 方式，用于指定 Kubernetes API 可以使用熟悉的`kubectl`命令(如`get`、`create`、`describe`和`apply`)操作的自定义资源。接下来是对运营商模式的讨论，这是 CRD 的延伸。然后，它详细介绍了云提供商附加到其 Kubernetes 实现上的一些钩子，并以对更大的云原生生态系统的简单介绍结束。使用本章中学习的概念，您将能够构建和开发 Kubernetes 集群的扩展，从而解锁高级使用模式。

本章的案例研究将包括创建两个简单的 CRD 来支持一个示例应用程序。我们将从 CRDs 开始，这将使您对扩展如何建立在 Kubernetes API 上有一个很好的基础了解。

在本章中，我们将涵盖以下主题:

*   如何用**自定义资源定义** ( **CRDs** )扩展 Kubernetes
*   Kubernetes 操作员的自我管理功能
*   使用特定于云的 Kubernetes 扩展
*   与生态系统整合

# 技术要求

为了运行本章中详细介绍的命令，您将需要一台支持`kubectl`命令行工具的计算机以及一个工作正常的 Kubernetes 集群。参见 [*第一章*](01.html#_idTextAnchor016)*与库本内特斯*通讯，了解几种快速与库本内特斯一起起床跑步的方法，以及如何安装`kubectl`工具的说明。

本章使用的代码可以在本书的 GitHub 资源库中找到[https://GitHub . com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/chapter 13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13)。

# 如何用自定义资源定义扩展 Kubernetes

让我们从基础开始。什么是 CRD？我们知道 Kubernetes 有一个 API 模型，我们可以在其中对资源执行操作。Kubernetes 资源的一些例子(你现在应该很熟悉了)是 Pods、PersistentVolumes、Secrets 和其他。

现在，如果我们想在集群中实现一些自定义功能，编写自己的控制器，并将控制器的状态存储在某个地方，该怎么办？当然，我们可以将自定义功能的状态存储在运行于 Kubernetes 或其他地方的 SQL 或 NoSQL 数据库中(这实际上是扩展 Kubernetes 的策略之一)——但是，如果我们的自定义功能更多地作为 Kubernetes 功能的扩展，而不是一个完全独立的应用程序呢？

在这种情况下，我们有两种选择:

*   自定义资源定义
*   API 聚合

API 聚合允许高级用户在 Kubernetes API 服务器之外构建自己的资源 API，并使用自己的存储——然后在 API 层聚合这些资源，以便可以使用 Kubernetes API 查询它们。这显然是高度可扩展的，本质上只是使用 Kubernetes API 作为您自己的定制功能的代理，它可能会也可能不会与 Kubernetes 真正集成。

另一个选项是 CRDs，在这里我们可以使用 Kubernetes API 和底层数据存储(`etcd`)而不是构建我们自己的。我们可以使用已知的`kubectl`和`kube api`方法与我们自己的定制功能进行交互。

在本书中，我们将不讨论 API 聚合。虽然肯定比 CRDs 更灵活，但这是一个高级的主题，值得深入了解 Kubernetes API，并仔细阅读 Kubernetes 文档，以确保正确。您可以在[https://Kubernetes . io/docs/concepts/extend-Kubernetes/API-extend/API server-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)的 Kubernetes 文档中了解更多关于 API 聚合的信息。

因此，现在我们知道我们正在使用 Kubernetes 控制平面作为我们自己的状态存储来实现新的定制功能，我们需要一个模式。类似于中的 Pod 资源规格，Kubernetes 期望某些字段和配置，我们可以告诉 Kubernetes 我们对新的定制资源的期望。让我们现在来看看 CRD 的规格。

## 编写自定义资源定义

对于 CRDs，Kubernetes 使用 OpenAPI V3 规范。有关 OpenAPI V3 的更多信息，您可以查看官方文档，网址为[。](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md)

让我们看一个 CRD 规范的例子。现在让我们弄清楚，这不是这个 CRD 的任何具体记录的 YAMLs 会是什么样子。相反，这只是我们定义库本内 CRD 需求的地方。一旦创建，Kubernetes 将接受符合规格的资源，我们可以开始制作我们自己的这种类型的记录。

这是一个 CRD 规范的 YAML 例子，我们称之为`delayedjob`。这个高度简化的 CRD 旨在延迟启动容器映像作业，这可以防止用户不得不为他们的容器延迟启动脚本。这个 CRD 相当脆，我们不建议任何人实际使用它，但它很好地突出了建设 CRD 的过程。让我们从一个完整的 CRD 规格 YAML 开始，然后分解它:

自定义资源定义 1.yaml

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: delayedjobs.delayedresources.mydomain.com
spec:
  group: delayedresources.mydomain.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                delaySeconds:
                  type: integer
                image:
                  type: string
  scope: Namespaced
  conversion:
    strategy: None
  names:
    plural: delayedjobs
    singular: delayedjob
    kind: DelayedJob
    shortNames:
    - dj
```

让我们回顾一下这个文件的各个部分。乍一看，它看起来像你典型的库本内特斯 YAML 规格-这是因为它是！在`apiVersion`领域，我们有`apiextensions.k8s.io/v1`，这是自库本内斯`1.16`以来的标准(在此之前是`apiextensions.k8s.io/v1beta1`)。我们的`kind`永远是`CustomResourceDefinition`。

`metadata`字段是事情开始具体到我们的资源的时候。我们需要将`name`元数据字段构造为我们资源的`plural`形式，然后是一个周期，然后是它的组。让我们从我们的 YAML 文件中转移一下，讨论一下在 Kubernetes API 中组是如何工作的。

### 了解库本内斯应用编程接口组

组是 Kubernetes 在其应用编程接口中分割资源的一种方式。每个组对应于 Kubernetes API 服务器的不同子路径。

默认情况下，有一个名为核心组的遗留组，它对应于在 Kubernetes REST API 的`/api/v1`端点上访问的资源。推而广之，这些遗留组资源的 YAML 规格中有`apiVersion: v1`。核心组中的资源之一的一个例子是 Pod。

接下来是一组命名组，它们对应于可以在形成为`/apis/<GROUP NAME>/<VERSION>`的`REST`网址上访问的资源。这些命名组构成了 Kubernetes 资源的主体。但是，最古老和最基本的资源，如 Pod、Service、Secret 和 Volume，都在核心组中。位于命名组中的资源的一个例子是位于`storage.k8s.io`组中的`StorageClass`资源。

重要说明

要查看哪个资源在哪个组中，您可以查看官方的 Kubernetes API 文档，了解您正在使用的 Kubernetes 版本。例如，版本`1.18`文档将位于[https://kubernetes . io/docs/reference/generated/kubernetes-API/v 1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18)。

CRDs 可以指定自己的命名组，这意味着特定的 CRD 将在 Kubernetes API 服务器可以监听的`REST`端点上可用。考虑到这一点，让我们回到我们的 YAML 文件，这样我们就可以谈论 CRD 的主要部分-版本规格。

### 了解自定义资源定义版本

如你所见，我们选择了`delayedresources.mydomain.com`组。理论上，这个群体将持有任何其他延期的 CRDs 例如`DelayedDaemonSet`或`DelayedDeployment`。

接下来，我们有我们的 CRD 的主要部分。在`versions`下，我们可以定义一个或多个 CRD 版本(在`name`字段中)，以及该版本 CRD 的 API 规范。然后，当您创建 CRD 的实例时，您可以在 YAML 的`apiVersion`键中定义您将用于版本参数的版本，例如`apps/v1`，或者在本例中为`delayedresources.mydomain.com/v1`。

每个版本项还有一个`served`属性，本质上是定义给定版本是启用还是禁用的一种方式。如果`served`是`false`，该版本将不会由库本内特斯应用编程接口创建，该版本的应用编程接口请求(或`kubectl`命令)将失败。

此外，可以在特定版本上定义`deprecated`键，这将导致 Kubernetes 在使用不推荐使用的版本向 API 发出请求时返回警告消息。这是一个怎样的 CRD。`yaml`文件有一个不推荐使用的版本外观-我们已经删除了一些规格，以保持 YAML 短:

自定义资源定义 2.yaml

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: delayedjob.delayedresources.mydomain.com
spec:
  group: delayedresources.mydomain.com
  versions:
    - name: v1
      served: true
      storage: false
      deprecated: true
      deprecationWarning: "DelayedJob v1 is deprecated!"
      schema:
        openAPIV3Schema:
		…
    - name: v2
      served: true
      storage: true
      schema:
        openAPIV3Schema:
		...
  scope: Namespaced
  conversion:
    strategy: None
  names:
    plural: delayedjobs
    singular: delayedjob
    kind: DelayedJob
    shortNames:
    - dj
```

如您所见，我们已经将`v1`标记为弃用，并且还包含了一个弃用警告，供 Kubernetes 发送作为响应。如果我们不包括一个折旧警告，将使用默认消息。

再往下，我们有`storage`键，它与`served`键交互。这样做的原因是，虽然 Kubernetes 同时支持一个资源的多个活动(也就是`served`)版本，但是这些版本中只有一个可以存储在控制平面中。然而，`served`属性意味着一个资源的多个版本可以由应用编程接口提供服务。那这到底是怎么回事？

答案是 Kubernetes 会将 CRD 对象从存储的版本转换为您要求的版本(或者在创建资源时，反之亦然)。

这种转换是如何处理的？让我们跳过剩余的版本属性，转到`conversion`键，看看如何操作。

`conversion`键让你指定一个策略，库本内特斯将如何在之间转换 CRD 对象，无论你的服务版本是什么，也无论存储版本是什么。如果两个版本相同，例如，如果您请求一个`v1`资源，并且存储的版本是`v1`，那么不会发生转换。

这里截止到 Kubernetes 1.13 的默认值是`none`。使用`none`设置，Kubernetes 不会在字段之间进行任何转换。它将简单地包括应该出现在`served`(或存储，如果创建一个资源)版本的字段。

另一个可能的转换策略是`Webhook`，它允许你定义一个定制的网络钩子，它将接受一个版本，并正确地转换到你想要的版本。下面是我们的 CRD 的一个例子，它有一个`Webhook`转换策略——为了简明起见，我们去掉了一些版本模式:

自定义资源定义 3.yaml

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: delayedjob.delayedresources.mydomain.com
spec:
  group: delayedresources.mydomain.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
		...
  scope: Namespaced
  conversion:
    strategy: Webhook
    webhook:
      clientConfig:
        url: "https://webhook-conversion.com/delayedjob"
  names:
    plural: delayedjobs
    singular: delayedjob
    kind: DelayedJob
    shortNames:
    - dj
```

正如您所看到的，策略`Webhook`让我们定义了一个请求的网址，该网址包含关于传入资源对象的信息、它的当前版本以及它需要转换到的版本。

这个想法是我们的`Webhook`服务器将处理转换，并传回修正后的 Kubernetes 资源对象。`Webhook`策略比较复杂，可以有很多可能的配置，本书就不深入探讨了。

重要说明

要了解如何配置转换 Webhooks，请查看官方 Kubernetes 文档，网址为[https://Kubernetes . io/docs/tasks/extend-Kubernetes/custom-resources/custom-resource-definition-version/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)。

现在，回到我们在 YAML 的入口！在`served`和`storage`键下，我们看到了`schema`对象，它包含了我们资源的实际规格。如前所述，这遵循 OpenAPI Spec v3 模式。

由于空间原因从前面的代码块中删除的`schema`对象如下:

自定义资源定义 3.yaml(续)

```
     schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                delaySeconds:
                  type: integer
                image:
                  type: string
```

如您所见，我们支持一个字段`delaySeconds`，它将是一个整数，`image`，它是一个字符串，对应于我们的容器图像。如果我们真的想让`DelayedJob`做好生产准备，我们会想加入各种其他选项，让它更接近最初的库本内斯工作资源——但这不是我们的意图。

在原始代码块中，在版本列表之外，我们看到了一些其他属性。首先是`scope`属性，可以是`Cluster`也可以是`Namespaced`。这告诉 Kubernetes 是将 CRD 对象的实例视为特定于命名空间的资源(如 Pods、Deployments 等)，还是将其视为集群范围的资源(如命名空间本身)，因为在命名空间中获取命名空间对象没有任何意义！

最后，我们有`names`块，它允许您定义资源名称的复数和单数形式，用于各种情况(例如，`kubectl get pods`和`kubectl get pod`都有效)。

`names`块还允许您定义骆驼大小写的`kind`值，该值将用于资源 YAML，以及一个或多个`shortNames`，该值可用于引用 API 或`kubectl`中的资源–例如`kubectl get po`。

YAML 解释了我们的 CRD 规格，让我们看一下我们的 CRD 的一个实例，正如我们刚刚回顾的规格所定义的，YAML 将是这样的:

延迟作业

```
apiVersion: delayedresources.mydomain.com/v1
kind: DelayedJob
metadata:
  name: my-instance-of-delayed-job
spec:
  delaySeconds: 6000
  image: "busybox"
```

如你所见，这就像我们的 CRD 定义了这个物体。现在，我们所有的零件都就位了，让我们测试一下我们的 CRD！

### 测试自定义资源定义

让我们继续在 Kubernetes 上测试我们的 CRD 概念:

1.  First, let's create the CRD spec in Kubernetes – the same way we would create any other object:

    ```
    kubectl apply -f delayedjob-crd-spec.yaml
    ```

    这将导致以下输出:

    ```
    customresourcedefinition "delayedjob.delayedresources.mydomain.com" has been created
    ```

2.  现在，Kubernetes 将接受对我们`DelayedJob`资源的请求。我们可以通过使用前面的资源 YAML 最终创建一个来测试这个问题:

    ```
    kubectl apply -f my-delayed-job.yaml
    ```

如果我们正确定义了我们的 CRD，我们将看到以下输出:

```
delayedjob "my-instance-of-delayed-job" has been created
```

可以看到，Kubernetes API 服务器已经成功创建了我们的`DelayedJob`实例！

现在，你可能会问一个非常相关的问题——现在怎么办？这是一个很好的问题，因为事实是，到目前为止，我们只完成了向库本内特斯应用编程接口数据库添加一个新的`table`的工作。

仅仅因为我们给了我们的`DelayedJob`资源一个应用程序映像和一个`delaySeconds`字段，并不意味着像我们想要的任何功能将会实际发生。通过创建我们的`DelayedJob`实例，我们刚刚在`table`中添加了一个条目。我们可以使用 Kubernetes API 或`kubectl`命令获取、编辑或删除它，但是还没有实现任何应用程序功能。

为了让我们的`DelayedJob`资源实际上做一些事情，我们需要一个定制的控制器，它将采用我们的`DelayedJob`实例，并用它做一些事情。最后，我们仍然需要使用官方的 Kubernetes 资源——Pods 等来实现实际的容器功能。

这就是我们现在要讨论的。有很多方法可以为 Kubernetes 构建定制控制器，但是一种流行的方法是**操作模式**。让我们进入下一部分，看看我们如何给我们的`DelayedJob`资源一个自己的生命。

# 【Kubernetes 操作员的自我管理功能

如果不首先讨论**操作员框架**，就不可能讨论 Kubernetes 操作员。一个常见的误解是，运算符是通过运算符框架专门构建的。Operator Framework 是一个开源框架，最初由 Red Hat 创建，以便于编写 Kubernetes 运算符。

实际上，操作员只是一个自定义控制器，与 Kubernetes 接口并对资源进行操作。Operator Framework 是制作 Kubernetes 操作符的一种固执己见的方法，但是还有许多其他开源框架可以使用——或者，您可以从头开始制作一个！

使用框架构建操作员时，两个最受欢迎的选项是前面提到的**操作员框架**和**kuebuilder**。

这两个项目有很多共同点。它们都使用`controller-tools`和`controller-runtime`，这两个库用于构建 Kubernetes 项目官方支持的 Kubernetes 控制器。如果你是从零开始构建一个操作员，使用这些官方支持的控制器库将使事情变得容易得多。

与 Operator Framework 不同，Kubebuilder 是 Kubernetes 项目的一个官方部分，很像`controller-tools`和`controller-runtime`库——但这两个项目各有利弊。重要的是，无论是这些选项，还是操作员模式，都让控制器在集群上运行。看起来很明显，这是最好的选择，但是您可以在集群之外运行您的控制器，并让它同样工作。要开始使用运营商框架，请查看位于[https://github.com/operator-framework](https://github.com/operator-framework)的官方 GitHub。对于 Kubebuilder，可以查看[https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)。

大多数操作者，不管框架如何，都遵循控制环范例——让我们看看这个想法是如何工作的。

## 绘制操作员控制回路

控制回路是系统设计和编程中的一种控制方案，由一个永无止境的逻辑流程回路组成。典型地，控制回路实施测量-分析-调整方法，其中它测量系统的当前状态，分析使其符合预期状态所需的改变，然后调整系统组件以使其符合(或至少更接近)预期状态。

特别是在 Kubernetes 操作员或控制器中，该操作通常是这样工作的:

1.  首先，一个`watch`步骤——即，观察存储在`etcd`中的预期状态的变化。
2.  然后是`analyze`步骤，控制器决定如何使集群状态符合预期状态。
3.  最后，一个`update`步骤——更新集群状态以实现集群变化的意图。

为了帮助理解控制回路，这里有一个图表，显示了各个部分是如何配合在一起的:

![Figure 13.1 – Measure Analyze Update Loop](img/B14790_13_01.jpg)

图 13.1–测量分析更新循环

让我们使用 Kubernetes 调度程序——它本身就是一个控制循环过程——来说明这一点:

1.  让我们从一个处于稳定状态的假设集群开始:所有 Pods 都已安排好，节点运行正常，一切都在正常运行。
2.  然后，用户创建一个新的 Pod。

我们之前讨论过 kubelet 是在`pull`的基础上工作的。这意味着，当 kubelet 在其节点上创建一个 Pod 时，该 Pod 已经通过调度程序分配给该节点。然而，当第一次通过`kubectl create`或`kubectl apply`命令创建吊舱时，吊舱没有被安排或分配到任何地方。这就是我们的调度程序控制循环开始的地方:

1.  第一步是**测量**，调度器读取 Kubernetes API 的状态。当从应用编程接口中列出吊舱时，它发现其中一个吊舱没有被分配给节点。现在进入下一步。
2.  接下来，调度器对集群状态和 Pod 要求进行分析，以决定 Pod 应该分配到哪个节点。正如我们在前面几章中所讨论的，这考虑了 Pod 资源限制和请求、节点状态、放置控制等，这使得它成为一个相当复杂的过程。一旦这个处理完成，更新步骤就可以开始了。
3.  最后，**更新**–调度程序通过将 Pod 分配给从*步骤 2* 分析中获得的节点来更新集群状态。此时，kubelet 接管自己的控制循环，并在其节点上为 Pod 创建相关的容器。

接下来，让我们把从调度器控制循环中学到的知识应用到我们自己的`DelayedJob`资源中。

## 为自定义资源定义设计运算符

实际上，为我们的“T0”CRD 编码一个操作符不在我们的书的范围之内，因为它需要编程语言的知识。如果您要选择一种编程语言来构建一个操作器，Go 提供了与 Kubernetes SDK、**控制器-工具**和**控制器-运行时**的最大互操作性，但是任何可以编写 HTTP 请求的编程语言都可以，因为这是所有 SDK 的基础。

然而，我们仍将使用一些伪代码来完成为我们的`DelayedJob` CRD 实现一个运算符的步骤。让我们一步一步来。

### 第一步:测量

首先是**测量**步骤，我们将在伪代码中将其实现为永远运行的`while`循环。在生产实现中，会有去抖、错误处理和一系列其他问题，但是对于这个示例，我们将保持简单。

看看这个循环的伪代码，它本质上是我们应用程序的主要功能:

主函数

```
// The main function of our controller
function main() {
  // While loop which runs forever
  while() {
     // fetch the full list of delayed job objects from the cluster
	var currentDelayedJobs = kubeAPIConnector.list("delayedjobs");
     // Call the Analysis step function on the list
     var jobsToSchedule = analyzeDelayedJobs(currentDelayedJobs);
     // Schedule our Jobs with added delay
     scheduleDelayedJobs(jobsToSchedule);
     wait(5000);
  }
}
```

如您所见，我们的`main`函数中的循环调用 Kubernetes API 来查找存储在`etcd`中的`delayedjobs`CRD 列表。这是`measure`步。然后它调用分析步骤，并根据结果调用更新步骤来调度任何需要调度的`DelayedJobs`。

重要说明

请记住，在本例中，Kubernetes 调度程序仍将执行实际的容器调度——但是我们需要首先将我们的`DelayedJob`归结为官方的 Kubernetes 资源。

在更新步骤之后，我们的循环会等待整整 5 秒钟，然后再次执行循环。这设定了控制循环的节奏。接下来，让我们进入分析步骤。

### 第二步:分析

接下来，让我们回顾一下我们的运算符的**分析**步骤，这是我们的控制器伪代码中的`analyzeDelayedJobs`函数:

分析函数

```
// The analysis function
function analyzeDelayedJobs(listOfDelayedJobs) {
  var listOfJobsToSchedule = [];
  foreach(dj in listOfDelayedJobs) {
    // Check if dj has been scheduled, if not, add a Job object with
    // added delay command to the to schedule array
    if(dj.annotations["is-scheduled"] != "true") {
      listOfJobsToSchedule.push({
        Image: dj.image,
        Command: "sleep " + dj.delaySeconds + "s",
        originalDjName: dj.name
      });
    }
  }
  return listOfJobsToSchedule;  
}
```

如您所见，前面的功能循环通过从**测量**循环传递的集群中的`DelayedJob`对象列表。然后，它通过检查对象注释之一的值来检查`DelayedJob`是否已经被调度。如果它还没有被调度，它将一个对象添加到一个名为`listOfJobsToSchedule`的数组中，该数组包含在`DelayedJob`对象中指定的图像、`DelayedJob`对象中指定的睡眠秒数命令以及`DelayedJob`的原始名称，我们将使用该名称在**更新**步骤中标记为调度。

最后，在**分析**步骤中，`analyzeDelayedJobs`函数将我们新创建的`listOfJobsToSchedule`数组返回到主函数。让我们用最后的更新步骤来结束我们的操作员设计，这是我们主循环中的`scheduleDelayedJobs`功能。

### 步骤 3:更新

最后，控制回路的**更新**部分将从我们的分析中获取输出，并根据需要更新集群以创建预期状态。伪代码如下:

更新函数

```
// The update function
function scheduleDelayedJobs(listOfJobs) {
  foreach(job in listOfDelayedJobs) {
    // First, go ahead and schedule a regular Kubernetes Job
    // which the Kube scheduler can pick up on.
    // The delay seconds have already been added to the job spec
    // in the analysis step
    kubeAPIConnector.create("job", job.image, job.command);
    // Finally, mark our original DelayedJob with a "scheduled"
    // attribute so our controller doesn't try to schedule it again
    kubeAPIConnector.update("delayedjob", job.originalDjName,
    annotations: {
      "is-scheduled": "true"
    });
  } 
}
```

在这种情况下，我们将从我们的`DelayedJob`对象中派生出我们的常规 Kubernetes 对象，并在 Kubernetes 中创建它，以便`Kube`调度器可以拾取它，创建相关的 Pod，并管理它。一旦我们创建了具有延迟的常规作业对象，我们也用一个注释更新我们的`DelayedJob` CRD 实例，该注释将`is-scheduled`注释设置为`true`，防止它被重新计划。

这就完成了我们的控制循环——从这一点开始，`Kube`调度程序接管了，我们的 CRD 被赋予了生命，成为了一个 Kubernetes Job 对象，它控制着一个 Pod，Pod 最终被分配给一个 Node，一个容器被调度来运行我们的代码！

这个例子当然是高度简化的，但是您会惊讶于有多少 Kubernetes 操作者执行一个简单的控制循环来协调 CRD，并将它们归结为基本的 Kubernetes 资源。操作员可能会变得非常复杂，并执行特定于应用程序的功能，如备份数据库、清空持久卷等，但该功能通常与被控制的内容紧密耦合。

现在我们已经讨论了 Kubernetes 控制器中的 Operator 模式，我们可以讨论一些针对云特定的 Kubernetes 控制器的开源选项。

# 使用特定于云的 Kubernetes 扩展

通常可用默认情况下，在亚马逊 EKS、Azure AKS 和谷歌云的 GKE 等托管 Kubernetes 服务中，特定于云的 Kubernetes 扩展和控制器可以与所讨论的云平台紧密集成，并可以轻松控制来自 Kubernetes 的其他云资源。

即使没有添加任何额外的第三方组件，许多这种特定于云的功能也可以通过**云控制器管理器** ( **CCM** )组件在上游的 Kubernetes 中获得，该组件包含许多与主要云提供商集成的选项。这是默认情况下在每个公共云的托管 Kubernetes 服务中通常启用的功能，但它们可以与在特定云平台上运行的任何集群集成，无论是否托管。

在本节中，我们将回顾 Kubernetes 的一些更常见的云扩展，包括**云控制器管理器(CCM)** 中的和需要安装其他控制器的功能，如**外部 dns** 和**集群自动缩放器**。让我们从一些大量使用的 CCM 功能开始。

## 了解云控制器管理器组件

正如在 [*第 1 章*](01.html#_idTextAnchor016)*与库本内特斯通信*中所述，CCM 是一个官方支持的库本内特斯控制器，它提供了几种公共云服务功能的挂钩。为了运行，CCM 组件需要以对所讨论的云服务的访问权限启动——例如，AWS 中的 IAM 角色。

对于官方支持的云，如 AWS、Azure 和 Google Cloud，CCM 可以简单地作为集群内的 DaemonSet 运行。我们使用 DaemonSet，因为 CCM 可以执行任务，例如在云提供商中创建持久存储，并且它需要能够将存储连接到特定的节点。如果您使用的云不受官方支持，您可以为该特定云运行 CCM，并且您应该遵循该项目中的特定说明。这些替代类型的 CCM 通常是开源的，可以在 GitHub 上找到。关于安装 CCM 的细节，让我们进入下一部分。

## 安装云控制器管理器

通常，在创建集群时配置 CCM。如前一节所述，托管服务(如 EKS、AKS 和 GKE)已经启用了该组件，但即使是 Kops 和 Kubeadm 也在安装过程中将 CCM 组件公开为一个标志。

假设您没有以任何其他方式安装 CCM，并且计划使用上游版本的官方支持的公共云之一，则可以将 CCM 安装为 DaemonSet。

首先，你需要一个`ServiceAccount`:

服务帐户. yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cloud-controller-manager
  namespace: kube-system
```

该`ServiceAccount`将用于对 CCM 进行必要的访问。

接下来，我们需要一个`ClusterRoleBinding`:

群集绑定. yaml

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:cloud-controller-manager
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
```

如您所见，我们需要让`cluster-admin`角色访问我们的 CCM 服务帐户。除了其他功能之外，CCM 还需要能够编辑节点。

最后，我们可以部署 CCM `DaemonSet`本身。您需要使用特定云提供商的正确设置填写此 YAML 文件，请查看您的云提供商在 Kubernetes 上的文档以获取此信息。

`DaemonSet`规格比较长，我们分两部分来复习。首先，我们有了带有所需标签和名称的`DaemonSet`模板:

daemmonset . YAML

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cloud-controller-manager
  name: cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: cloud-controller-manager
```

如您所见，为了匹配我们的`ServiceAccount`，我们正在`kube-system`名称空间中运行 CCM。我们也给`DaemonSet`贴上了`k8s-app`的标签，以此来区分它是一个库本内特斯控制平面组件。

接下来，我们有`DaemonSet`的规格:

Daemonset.yaml(续)

```
    spec:
      serviceAccountName: cloud-controller-manager
      containers:
      - name: cloud-controller-manager
        image: k8s.gcr.io/cloud-controller-manager:<current ccm version for your version of k8s>
        command:
        - /usr/local/bin/cloud-controller-manager
        - --cloud-provider=<cloud provider name>
        - --leader-elect=true
        - --use-service-account-credentials
        - --allocate-node-cidrs=true
        - --configure-cloud-routes=true
        - --cluster-cidr=<CIDR of the cluster based on Cloud Provider>
      tolerations:
      - key: node.cloudprovider.kubernetes.io/uninitialized
        value: "true"
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      nodeSelector:
        node-role.kubernetes.io/master: ""
```

如您所见，本规范中有几个地方需要您查看所选云提供商的文档或集群网络设置，以找到合适的值。尤其是像`--cluster-cidr`和`--configure-cloud-routes`这样的网络标志，它们的值可能会根据您如何设置集群而变化，甚至在单个云提供商上也是如此。

现在，我们已经以某种方式在集群上运行了 CCM，让我们深入了解它提供的一些功能。

## 了解云控制器管理器的功能

默认的 CCM 在几个关键领域提供功能。首先，CCM 包含节点、路由和服务的辅助控制器。让我们从节点/节点生命周期控制器开始，依次回顾每一个，看看它能为我们提供什么。

### 节点/节点生命周期控制器

CCM 节点控制器确保集群状态，就集群中的哪些节点而言，等同于云提供商系统中的状态。一个简单的例子是自动缩放 AWS 中的组。当使用 AWS EKS(或仅在 AWS EC2 上使用 Kubernetes，尽管这需要额外的配置)时，可以在 AWS 自动缩放组中配置工作节点组，该组将根据节点的 CPU 或内存使用情况进行缩放。当云提供商添加和初始化这些节点时，CCM 节点控制器将确保集群为云提供商提供的每个节点拥有一个节点资源。

接下来，让我们继续讨论路由控制器。

### 通信控制模块路由控制器

CCM 路由控制器负责以支持 Kubernetes 集群的方式配置云提供商的网络设置。这可以包括 IP 的分配和在节点之间设置路由。服务控制器还处理网络，但外部方面。

### CCM 服务控制器

CCM 服务控制器提供了许多在公共云提供商上运行 Kubernetes 的“魔力”。我们在 [*第 5 章*](05.html#_idTextAnchor127)*服务和入口–与外界交流*中回顾的一个方面是`LoadBalancer`服务。例如，在配置了 AWS CCM 的集群上，`LoadBalancer`类型的服务将自动配置匹配的 AWS 负载平衡器资源，提供了一种简单的方法来公开集群中的服务，而无需处理`NodePort`设置甚至入口。

现在我们了解了 CCM 提供的内容，我们可以进一步讨论在公共云上运行 Kubernetes 时经常使用的其他几个云提供商扩展。首先来看看`external-dns`。

## 使用外部域名系统

`external-dns`库是官方支持的 Kubernetes 插件，允许集群配置外部 DNS 提供商，以自动方式为服务和入口提供 DNS 解析。`external-dns`插件支持广泛的云提供商，如 AWS 和 Azure，以及其他域名服务，如 Cloudflare。

重要说明

要安装`external-dns`，可以查看[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)官方 GitHub 库。

一旦`external-dns`在您的集群上实现，以自动化的方式创建新的域名系统记录就很简单了。要用服务测试`external-dns`，我们只需要在库本内特斯用适当的注释创建服务。

让我们看看这是什么样子:

服务，yaml

```
apiVersion: v1
kind: Service
metadata:
  name: my-service-with-dns
  annotations:
    external-dns.alpha.kubernetes.io/hostname: myapp.mydomain.com
spec:
  type: LoadBalancer
  ports:
  - port: 80
    name: http
    targetPort: 80
  selector:
    app: my-app
```

如您所见，我们只需要为`external-dns`控制器添加一个注释进行检查，域记录将在 DNS 中创建。当然，您的`external-dns`控制器必须可以访问域和托管区域，例如，在 AWS 路由或 Azure 域名系统上。有关详细信息，请查看`external-dns` GitHub 存储库中的特定文档。

一旦服务启动并运行，`external-dns`将提取注释并创建新的 DNS 记录。这种模式非常适合多租户或按版本部署，因为通过类似 Helm 图表的方式，变量可用于根据应用程序部署的版本或分支来更改域，例如`v1.myapp.mydomain.com`。

对于入口，这就更简单了–您只需要在入口记录中指定一台主机，如下所示:

入口，yaml

```
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: my-domain-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx".
spec:
  rules:
  - host: myapp.mydomain.com
    http:
      paths:
      - backend:
          serviceName: my-app-service
          servicePort: 80
```

该主机值将自动创建一个域名系统记录，指向您的入口使用的任何方法，例如 AWS 上的负载平衡器。

接下来，我们来谈谈**集群自动缩放器**库是如何工作的。

## 使用集群自动缩放器插件

与`external-dns`类似，`cluster-autoscaler`是官方支持的 Kubernetes 的附加组件，支持一些具有特定功能的主要云提供商。`cluster-autoscaler`的目的是触发集群中节点数量的缩放。它通过控制云提供商自己的缩放资源(如 AWS 自动缩放组)来执行这个过程。

当任何单个 Pod 由于节点上的资源限制而无法调度时，群集自动缩放器将执行向上缩放操作，但前提是现有节点大小的节点(例如，AWS 中`t3.medium`大小的节点)允许调度 Pod。

同样，群集自动缩放器将在任何节点清空 Pods 时执行向下缩放操作，而不会对任何其他节点造成内存或 CPU 压力。

要安装`cluster-autoscaler`，只需按照云提供商针对集群类型和预期版本的`cluster-autoscaler`的正确说明进行操作。例如，EKS`cluster-autoscaler`的 AWS 安装说明可以在[中找到。](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/)

接下来，让我们看看如何通过检查 Kubernetes 生态系统来找到 Kubernetes 的开源和开源扩展。

# 与生态系统融合

Kubernetes(以及更一般的云原生)生态系统是庞大的，由数百个流行的开源软件库和数千个新的开源软件库组成。这可能很难驾驭，因为每个月都会有新的技术来审查，收购、合并和公司倒闭可能会把你最喜欢的开源库变成一个未维护的烂摊子。

谢天谢地，这个生态系统中有一些结构，为了帮助解决云原生开源中缺乏选项的问题，了解它是值得的。第一个大的结构组件是**云原生计算基金会**或 **CNCF** 。

## 介绍云原生计算基金会

CNCF 是 Linux 基金会的一个子基金会，该基金会是一个非盈利实体，主持开源项目，并协调不断变化的贡献和使用开源软件的公司名单。

CNCF 的建立几乎完全是为了引导库本内特项目的未来。它是与 1.0 版本的 Kubernetes 一起宣布的，并且从那时起已经发展到包含云原生领域的数百个项目——从普罗米修斯到特使到赫尔姆，还有更多。

查看 CNCF 组成项目概况的最佳方式是查看 CNCF 云原生景观，该景观可在[https://landscape.cncf.io/](https://landscape.cncf.io/)找到。

如果您对使用 Kubernetes 或云原生解决方案可能遇到的问题感兴趣，CNCF 景观是一个很好的起点。对于每个类别(监控、日志、无服务器、服务网格等)，都有几个开源选项可供审查和选择。

这是当前云原生技术生态系统的优势和劣势。有相当多的选项可用，这使得正确的路径往往不明确，但也意味着您可能能够找到接近您确切需求的解决方案。

CNCF 还运营着一个官方的 Kubernetes 论坛，可以从 Kubernetes 官方网站 [kubernetes.io](http://kubernetes.io) 加入。库本内特论坛的网址是[https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)。

最后，值得一提的是*Kubernetes*/*CloudNativeCon*，这是一个由 CNCF 运营的大型会议，包括 Kubernetes 本身和许多生态系统项目。 *KubeCon* 每年都会变大，2019 年几乎有 12，000 人参加 *KubeCon* *北美*。

# 总结

在本章中，我们学习了如何扩展 Kubernetes。首先，我们讨论了 CRDs——它们是什么，一些相关的用例，以及如何在集群中实现它们。接下来，我们回顾了 Kubernetes 中操作员的概念，并讨论了如何使用操作员或自定义控制器来赋予您的 CRD 以生命。

然后，我们讨论了针对云提供商的 Kubernetes 扩展，包括`cloud-controller-manager`、`external-dns`和`cluster-autoscaler`。最后，我们介绍了云原生开源生态系统，以及一些发现用例项目的好方法。

您在本章中使用的技能将帮助您扩展 Kubernetes 集群，以便与您的云提供商以及您自己的定制功能进行交互。

在下一章中，我们将讨论两种应用于 Kubernetes 的新生架构模式——无服务器和服务网格。

# 问题

1.  CRD 的服务版本和存储版本有什么区别？
2.  自定义控制器或操作员控制回路的三个典型部分是什么？
3.  `cluster-autoscaler`如何与 AWS 自动缩放组等现有云提供商缩放解决方案进行交互？

# 进一步阅读

*   CNCF 山水:[https://landscape.cncf.io/](https://landscape.cncf.io/)
*   官场久别论坛:[https://discuss . kubrintes . io/](https://discuss.kubernetes.io/)