# 第十一章。码头工人网络故障排除

在本章中，我们将介绍以下食谱:

*   使用 tcpdump 验证网络路径
*   验证 VETH 对
*   验证发布的端口和出站伪装
*   验证名称解析
*   构建测试容器
*   重置本地 Docker 网络数据库

# 简介

正如我们在前面几章中看到的，Docker 利用相对知名的 Linux 网络结构的组合来交付容器网络。在本书中，我们已经了解了许多不同的配置、使用和验证 Docker 网络配置的方法。我们还没有做的是概述一个故障排除和验证方法，您可以在遇到问题时使用。排除容器网络故障时，了解并能够排除用于提供端到端连接的每个特定网络组件的故障非常重要。本章的目的是提供当您需要验证或排除 Docker 网络问题时可以采取的具体步骤。

# 使用 tcpdump 验证网络路径

虽然我们在前面的章节中浏览了它的用法，但是任何在基于 Linux 的系统上工作的人都应该对`tcpdump`感到满意。`tcpdump`允许您捕获主机上一个或多个接口上的网络流量。在本食谱中，我们将介绍如何在多种不同的 Docker 网络场景中使用`tcpdump`来验证集装箱网络流量。

## 做好准备

在本食谱中，我们将使用单个 Docker 主机。假设 Docker 已安装并处于默认配置。您还需要根级访问，以便检查和更改主机网络和防火墙配置。您还需要安装`tcpdump`实用程序。如果您的系统上没有它，您可以使用以下命令安装它:

```
sudo apt-get install tcpdump
```

## 怎么做…

`tcpdump`是一个惊人的故障排除工具。如果使用得当，它可以为您提供数据包在 Linux 主机上通过接口时的详细视图。为了演示，让我们在 Docker 主机上启动一个容器:

```
user@docker1:~$ docker run -dP --name web1 jonlangemak/web_server_1
ea32565ece0c0c22eace935113b6697bebe837f0b5ddf31724f371220792fb15
user@docker1:~$
```

因为我们没有指定任何网络参数，所以这个容器将在`docker0`桥上运行，并将任何公开的端口发布到主机接口。当流量流向外部网络时，从容器生成的流量也将隐藏在主机的 IP 接口后面。使用`tcpdump`，我们可以看到每个阶段的这个流量。

让我们首先检查进入主机的入站流量:

```
user@docker1:~$ docker port web1
80/tcp -> 0.0.0.0:32768
user@docker1:~$
```

在我们的例子中，这个容器暴露了端口`80`，该端口现在已经发布到端口`32768`上的主机接口。让我们首先确保流量通过正确的端口进入主机。为此，我们可以在主机`eth0`接口上捕获去往端口`32768`的流量:

```
user@docker1:~$ sudo tcpdump -qnn -i eth0 dst port 32768
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
15:46:07.629747 IP 10.20.30.41.55939 > 10.10.10.101.32768: tcp 0
15:46:07.629997 IP 10.20.30.41.55940 > 10.10.10.101.32768: tcp 0
15:46:07.630257 IP 10.20.30.41.55939 > 10.10.10.101.32768: tcp 0

```

为了使用`tcpdump`来捕获这个入站流量，我们使用了几个不同的参数:

*   `q`:这告诉`tcpdump`安静，或者不要产生那么多输出。因为我们真的只想看到第 3 层和第 4 层的信息，这就很好地清理了输出
*   `nn`:这告诉`tcpdump`不要尝试将 IP 解析为 DNS 名称。同样，我们想在这里看到 IP 地址
*   `i`:指定我们要捕捉的界面，这里是`eth0`
*   `src port`:告诉`tcpdump`过滤目的港为`32768`的流量

### 注

`dst`参数可以从该命令中删除。这样做将过滤掉端口为`32768`的任何流量，从而向您显示包括返回流量在内的整个流量。

如前面代码中的所示，我们可以看到主机在端口`32768`上的物理接口(`10.10.10.101`)上接收来自远程来源`10.20.30.41`的流量。在这种情况下，`10.20.30.41`是一个测试服务器，它向容器的发布端口发起流量。

现在我们已经看到了到达主机的流量，让我们看看它穿过`docker0`桥的情况:

```
user@docker1:~$ sudo tcpdump -qnn -i docker0
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes
16:34:54.193822 IP 10.20.30.41.53846 > 172.17.0.2.80: tcp 0
16:34:54.193848 IP 10.20.30.41.53847 > 172.17.0.2.80: tcp 0
16:34:54.193913 IP 172.17.0.2.80 > 10.20.30.41.53846: tcp 0
16:34:54.193940 IP 172.17.0.2.80 > 10.20.30.41.53847: tcp 0

```

在这种情况下，我们只需在`docker0`桥接口上过滤就可以看到流量。不出所料，我们看到了相同的流量、相同的源，但由于发布的端口功能，现在反映了在容器中运行的服务的准确目的地 IP 和端口。

虽然这肯定是捕获流量最简单的方法，但如果您有多个容器在`docker0`桥上运行，效果就不是很好。当前的过滤将为您提供所有通过网桥的流量，而不仅仅是您正在寻找的特定容器。在这些情况下，您也可以在过滤器中指定 IP 地址，如下所示:

```
user@docker1:~$ sudo tcpdump -qnn -i docker0 dst 172.17.0.2
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes
16:42:22.332555 IP 10.20.30.41.53878 > 172.17.0.2.80: tcp 0
16:42:22.332940 IP 10.20.30.41.53878 > 172.17.0.2.80: tcp 0

```

### 注

我们在这里指定目的地 IP 作为过滤器。如果我们希望看到流量来源和目的地都是该 IP 地址，我们可以将`dst`替换为`host`。

这种数据包捕获对于验证像端口发布这样的功能是否如预期那样工作至关重要。可以对大多数接口类型进行捕获，包括那些没有关联 IP 地址的接口类型。这种接口的一个很好的例子是用于将容器命名空间连接回默认命名空间的 VETH 对的主机端。排除容器连接故障时，将到达`docker0`桥的流量与特定的主机端 VETH 接口相关联可能会很方便。我们可以通过关联来自多个地方的数据来做到这一点。例如，假设我们执行以下`tcpdump`:

```
user@docker1:~$ sudo tcpdump -qnne -i docker0 host 172.17.0.2
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes
16:59:33.334941 02:42:ab:27:0e:3e > 02:42:ac:11:00:02, IPv4, length 66: 10.20.30.41.57260 > 172.17.0.2.80: tcp 0
16:59:33.335012 02:42:ac:11:00:02 > 02:42:ab:27:0e:3e, IPv4, length 66: 172.17.0.2.80 > 10.20.30.41.57260: tcp 0
```

请注意，在这种情况下，我们将`e`参数传递给了`tcpdump`。这告诉`tcpdump`显示每帧的源和目的 MAC 地址。在这种情况下，我们可以看到我们有两个 MAC 地址。其中一个是与`docker0`桥相关联的媒体访问控制地址，另一个是与容器相关联的媒体访问控制地址。我们可以查看`docker0`网桥信息来确定它的 MAC 地址是什么:

```
user@docker1:~$ ip link show dev docker0
4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ab:27:0e:3e brd ff:ff:ff:ff:ff:ff
user@docker1:~$
```

这就留下了地址`02:42:ac:11:00:02`。使用作为`iproute2`工具集一部分的桥接命令，我们可以确定该 MAC 地址位于哪个接口上:

```
user@docker1:~$ bridge fdb show | grep 02:42:ac:11:00:02
02:42:ac:11:00:02 dev vetha431055
user@docker1:~$
```

在这里，我们可以看到容器的 MAC 地址是通过名为`vetha431055`的接口访问的。在该界面上进行捕获将确认我们看到的是正确的界面:

```
user@docker1:~$ sudo tcpdump -qnn -i vetha431055
tcpdump: WARNING: vetha431055: no IPv4 address assigned
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on vetha431055, link-type EN10MB (Ethernet), capture size 65535 bytes
21:01:24.503939 IP 10.20.30.41.58035 > 172.17.0.2.80: tcp 0
21:01:24.503990 IP 172.17.0.2.80 > 10.20.30.41.58035: tcp 0
```

`tcpdump`可以是验证容器通信的重要工具。明智的做法是花些时间了解该工具，以及使用其不同参数过滤流量的不同方式。

# 验证 VETH 对

在我们在本书中回顾的所有 Linux 网络结构中，VETH 对可能是最重要的。由于知道名称空间，它们允许您将唯一名称空间中的容器连接到任何其他名称空间，包括默认名称空间。虽然 Docker 为您处理了所有这些，但是能够确定 VETH 对的端点在哪里，并将它们关联起来，以确定 VETH 对的目的是什么，这是非常有用的。在这个食谱中，我们将深入回顾如何找到和关联 VETH 对的末端。

## 做好准备

在本食谱中，我们将使用单个 Docker 主机。假设 Docker 已安装并处于默认配置。您还需要根级访问，以便检查和更改主机网络和防火墙配置。

## 怎么做…

Docker 中 VETH 对的主要用例是将容器网络命名空间连接回默认网络命名空间。它通过将一对 VETH 放在`docker0`桥上，另一端放在容器中来实现。VETH 对的容器端获得一个分配给它的 IP 地址，然后重命名为`eth0`。

当寻找匹配容器的 VETH 对的末端时，有两种情况。第一种是从默认命名空间的末尾开始，第二种是从容器命名空间的末尾开始。让我们浏览一下每个案例，以及如何将它们关联在一起。

让我们首先了解接口的主机端。例如，假设我们正在寻找这个接口的容器端:

```
user@docker1:~$ ip -d link show
…<Additional output removed for brevity>… 
4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ab:27:0e:3e brd ff:ff:ff:ff:ff:ff promiscuity 0
    bridge
6: vetha431055@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 82:69:cb:b6:9a:db brd ff:ff:ff:ff:ff:ff promiscuity 1
    veth
user@docker1:~$
```

这里有几件事需要指出。首先，将`-d`参数传递给`ip link`子命令，显示关于界面的额外细节。在这种情况下，它确认接口是 VETH 对。第二，VETH 配对命名一般遵循`<end1>@<end2>`命名惯例。在这种情况下，我们可以看到终端`vetha431055`是本地接口，`if5`是另一端。`if5`代表接口 5 或主机上第 5 个接口的索引 ID。因为 VETH 接口总是成对创建的，所以可以假设索引为 6 的 VETH 对的末尾很可能是索引 5 或 7。在这种情况下，命名表示它是 5，但是我们可以使用`ethtool`命令来确认:

```
user@docker1:~$ sudo ethtool -S vetha431055
NIC statistics:
 peer_ifindex: 5
user@docker1:~$
```

正如您所看到的，这个 VETH 对的另一端的接口索引为 5，如名称所示。现在找到哪个容器有 5 个是困难的部分。为此，我们需要检查每个容器的特定接口号。如果你运行很多容器，这可能是一个挑战。不用手动检查每个容器，你可以使用 Linux `xargs`循环检查它们。例如，看看这个命令:

```
docker ps -q | xargs --verb -I {} docker exec {} ip link | grep ^5:
```

我们在这里做的是返回所有运行容器的容器标识列表，然后将该列表传递给`xargs`。反过来，`xargs`使用这些容器标识来运行带有`docker exec`的容器内的命令。该命令恰好是`ip link`命令，它将返回所有接口及其相关索引号的列表。如果返回的任何信息以`5:`开头，表示界面索引为 5，我们将把它打印到屏幕上。为了查看哪个容器有问题的接口，我们必须以详细模式(`--verb`)运行`xargs`命令，这将在每个命令运行时显示给我们。输出如下所示:

```
user@docker1:~$ docker ps -q | xargs --verb -I {} docker exec {} ip link | grep ^5:
docker exec 4b521df22184 ip link
docker exec 772e12b15c92 ip link
docker exec d8f3e7936690 ip link
docker exec a2e3201278e2 ip link
docker exec f9216233ba56 ip link
docker exec ea32565ece0c ip link
5: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP
user@docker1:~$
```

正如你看到的，这台主机上运行着六个容器。直到最后一个容器，我们才找到我们要找的接口 ID。给定容器 ID，我们可以知道哪个容器有 VETH 接口的另一端。

### 注

您可以通过运行`docker exec -it` `ea32565ece0c ip link`命令来确认这一点。

现在，让我们尝试另一个从 VETH 对的容器端开始的例子。这稍微容易一些，因为接口的命名告诉我们主机端匹配接口的索引:

```
user@docker1:~$ docker exec web1 ip -d link show dev eth0
5: eth0@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    veth
user@docker1:~$
```

然后，我们可以再次使用`ethtool`来验证主机上索引为 6 的接口与容器中索引为 5 的接口是否匹配:

```
user@docker1:~$ ip -d link show | grep ^6:
6: vetha431055@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
user@docker1:~$ sudo ethtool -S vetha431055
[sudo] password for user:
NIC statistics:
 peer_ifindex: 5
user@docker1:~$
```

# 验证发布的端口和出站伪装

在 Docker 网络中，更困难的部分之一是`iptables`。`iptables` /netfilter 集成在提供功能(如端口发布和出站伪装)方面发挥了关键作用。然而，`iptables`如果你还不熟悉，可能很难理解和排除故障。在本食谱中，我们将回顾如何详细检查`iptables`配置，并验证连接是否按预期工作。

## 做好准备

在本食谱中，我们将使用单个 Docker 主机。假设 Docker 已安装并处于默认配置。为了检查`iptables`规则集，您还需要根级访问。

## 怎么做…

正如我们在前面几章中看到的，Docker 代表您出色地管理了主机防火墙规则。您可能很少需要查看或修改与 Docker 相关的`iptables`规则。然而，在排除容器网络故障时，能够验证配置以排除`iptables`是一个可能的问题总是一个好主意。

为了演示遍历`iptables`规则集，我们将检查一个发布端口的示例容器。我们执行的步骤可以很容易地转移到检查任何其他 Docker 集成的`iptables`用例的规则。为此，我们将运行一个简单的容器，公开用于发布的端口`80`:

```
user@docker1:~$ docker run -dP --name web1 jonlangemak/web_server_1
```

既然我们告诉 Docker 发布任何暴露的端口，我们知道这个容器应该将其暴露的端口`80`发布给主机。为了验证端口实际上是被发布的，我们可以检查`iptables`规则集。我们要做的第一件事是确保端口发布所需的目标 NAT 已经到位。要检查`iptables`表，我们可以使用`iptables`命令并传递以下参数:

*   `n`:告诉`iptables`在输出中使用数字信息，例如地址和端口
*   `L`:告诉`iptables`您想要输出一个规则列表
*   `v`:告诉`iptables`提供详细的输出，这样我们就可以看到所有的规则信息以及规则计数器
*   `t`:告诉`iptables`只显示特定表格的信息

将放在一起，我们可以使用命令`sudo iptables –nL –t nat`查看主机 NAT 表中的规则:

![How to do it…](graphics/B05453_11_01.jpg)

### 注

请注意，我们将在本食谱中检查的所有默认表和链策略都是`ACCEPT`。如果默认的链策略是`ACCEPT`，这意味着即使我们没有得到规则匹配，流量仍然会被允许。不管默认策略设置为什么，Docker 都会创建规则。

如果你对`iptables`感到不舒服，解释这个输出可能会有点令人生畏。即使我们正在查看 NAT 表，我们也需要知道主机的入站通信正在处理哪个链。在我们的例子中，由于流量进入主机，我们感兴趣的链是`PREROUTING`链。让我们浏览一下表是如何处理的:

*   `PREROUTING`链中的第一行查找去往`LOCAL`或主机本身的流量。由于流量的目的地是主机接口上的一个 IP 地址，因此我们匹配此规则，并执行引用跳转到名为`DOCKER`的新链的操作。
*   在`DOCKER`链中，我们碰到的第一个规则是寻找进入`docker0`桥的流量。由于这些交通没有进入`docker0`大桥，规则被忽略，我们进入下一个规则。
*   `DOCKER`链中的第二个规则是寻找没有进入`docker0`桥并且有一个目的地端口为 TCP `32768`的流量。我们匹配此规则，并执行操作来执行到`172.17.0.2`端口`80`的目的 NAT。

表中的处理如下所示:

![How to do it…](graphics/B05453_11_02.jpg)

上图中的箭头表示流量通过 NAT 表时的流量。在这个例子中，我们只有一个容器在主机上运行，所以很容易看到哪些规则正在被处理。

### 注

您可以将这种输出与`watch`命令相结合，以获得计数器的实时输出，例如:

```
sudo watch --interval 0 iptables -vnL -t nat
```

现在我们已经遍历了 NAT 表，接下来我们需要担心的是过滤表。我们可以像查看 NAT 表一样查看过滤器表:

![How to do it…](graphics/B05453_11_03.jpg)

乍一看，我们可以看到这张表的布局与 NAT 表略有不同。例如，我们在这个表中有不同于 NAT 表的链。在我们的例子中，我们对入站发布端口通信感兴趣的链是前向链。这是因为主机正在将流量转发或路由到容器。流量将按如下方式遍历该表:

*   前向链中的第一条线将流量直接发送到`DOCKER-ISOLATION`链。
*   在这种情况下，`DOCKER-ISOLATION`链中唯一的规则是将流量发回的规则，因此我们继续查看`FORWARD`表中的规则。
*   前向表中的第二条规则是，如果流量流出`docker0`桥，将流量发送到`DOCKER`链。由于我们的目的地(`172.17.0.20`)住在`docker0`桥外，我们按照这条规则匹配并跳到`DOCKER`链。
*   在`DOCKER`链中，我们检查第一个规则，并确定它正在寻找目的地为 TCP 端口`80`上的容器 IP 地址的流量，该流量正在流出而不是流入`docker0`桥。我们匹配这个规则，流程被接受。

表中的处理如下所示:

![How to do it…](graphics/B05453_11_04.jpg)

通过过滤表是发布的端口流量到达容器必须采取的最后一步。然而，我们现在只到达了集装箱。我们仍然需要考虑从容器返回到与发布端口对话的主机的返回流量。所以现在，我们需要谈谈起源于集装箱的流量是如何被`iptables`处理的。

我们将遇到的出站流量的第一个表是过滤器表。来自容器的流量将再次使用过滤表的前向链。流程如下所示:

*   前向链中的第一条线将流量直接发送到`DOCKER-ISOLATION`链。
*   在这种情况下，`DOCKER-ISOLATION`链中唯一的规则是将流量发回的规则，因此我们继续检查 FORWARD 表中的规则。
*   前向表中的第二条规则是，如果流量来自`docker0`桥，则将流量发送到`DOCKER`链。由于我们的交通是进入`docker0`桥而不是出去，这条规则被忽略了，我们进入下一条规则。
*   前向表中的第三条规则是，如果流量从`docker0`桥流出，并且其连接状态为`RELATED`或`ESTABLISHED`，则流量应该被接受。这个交通要进入`docker0`桥，所以我们也不符合这个规则。但是，值得指出的是，该规则用于允许从容器发起的流的返回流量。它只是没有作为初始出站连接的一部分被命中，因为这代表了一个新的流。
*   前向表中的第四条规则是，如果交通是从`docker0`桥进去的，而不是从`docker0`桥出来的，要接受。因为我们的交通将进入`docker0`大桥，我们遵守这条规则，交通被接受。

表中的处理如下所示:

![How to do it…](graphics/B05453_11_05.jpg)

出站流量的下一个表是 NAT 表。这一次，我们要看`POSTROUTING`链。在这种情况下，我们匹配链的第一个规则，该规则寻找的流量不是从`docker0`桥发出的，而是来自`docker0`桥子网(`172.17.0.0/16`)的:

![How to do it…](graphics/B05453_11_06.jpg)

此规则的操作是`MASQUERADE`，它将根据主机路由表隐藏其中一个主机接口后面的流量。

采用同样的方法，您可以轻松验证与 Docker 相关的其他`iptables`流。诚然，随着集装箱数量的增加，这将成为一项更加艰巨的任务。但是，由于大多数规则是基于每个容器编写的，命中计数器对于每个容器都是唯一的，因此更容易缩小范围。

### 注

更多关于`iptables`表和链的处理顺序的信息，请看这个`iptables`网页和[http://www.iptables.info/en/structure-of-iptables.html](http://www.iptables.info/en/structure-of-iptables.html)的相关流程图。

# 验证名称解析

容器的 DNS 解析一直相当简单。容器接收到与主机相同的 DNS 配置。然而，随着用户定义的网络和嵌入式域名系统服务器的出现，这现在变得有点棘手。我见过的许多域名系统问题中的一个常见问题是不理解嵌入式域名系统服务器是如何工作的，以及如何验证它是否正常工作。在这个食谱中，我们将逐步通过一个容器 DNS 配置来验证它正在使用哪个 DNS 服务器来解析特定的名称空间。

## 做好准备

在本食谱中，我们将使用单个 Docker 主机。假设 Docker 已安装并处于默认配置。您还需要根级访问权限，以便检查和更改主机的网络和防火墙配置。

## 怎么做…

没有用户定义网络的 Docker 的标准 DNS 配置是简单地将 DNS 配置从主机复制到容器中。在这些情况下，域名解析非常简单:

```
user@docker1:~$ docker run -dP --name web1 jonlangemak/web_server_1
e5735b30ce675d40de8c62fffe28e338a14b03560ce29622f0bb46edf639375f
user@docker1:~$
user@docker1:~$ docker exec web1 more /etc/resolv.conf
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
nameserver <your local DNS server>
search lab.lab
user@docker1:~$
user@docker1:~$ more /etc/resolv.conf
nameserver <your local DNS server>
search lab.lab
user@docker1:~$
```

在这种情况下，所有的域名系统请求将直接转到定义的域名系统服务器。这意味着我们的容器可以解析任何 DNS 记录，我们的主机可以:

```
user@docker1:~$ docker exec -it web1 ping docker2.lab.lab -c 2
PING docker2.lab.lab (10.10.10.102): 48 data bytes
56 bytes from 10.10.10.102: icmp_seq=0 ttl=63 time=0.471 ms
56 bytes from 10.10.10.102: icmp_seq=1 ttl=63 time=0.453 ms
--- docker2.lab.lab ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.453/0.462/0.471/0.000 ms
user@docker1:~$
```

再加上 Docker 会将这些流量伪装成主机本身的 IP 地址，使得这成为一个简单且易于维护的解决方案。

然而，当我们开始使用用户定义的网络时，这就变得有点棘手了。这是因为用户定义的网络提供了容器名称解析。也就是说，一个容器可以解析另一个容器的名称，而无需使用静态或手动主机文件条目和链接。这是一个很好的特性，但是如果您不理解容器是如何接收其 DNS 配置的，它会引起一些混乱。例如，现在让我们创建一个新的用户定义网络:

```
user@docker1:~$ docker network create -d bridge mybridge1
e8afb0e506298e558baf5408053c64c329b8e605d6ad12efbf10e81f538df7b9
user@docker1:~$
```

现在让我们在这个网络上开始一个名为`web2`的新容器:

```
user@docker1:~$ docker run -dP --name web2 --net \
mybridge1 jonlangemak/web_server_2
1b38ad04c3c1be7b0f1af28550bf402dcde1515899234e4b09e482da0a560a0a
user@docker1:~$
```

现在，如果我们将现有的`web1`容器连接到该桥，我们应该会发现`web1`可以通过名称解析容器`web2`:

```
user@docker1:~$ docker network connect mybridge1 web1
user@docker1:~$ docker exec -it web1 ping web2 -c 2
PING web2 (172.18.0.2): 48 data bytes
56 bytes from 172.18.0.2: icmp_seq=0 ttl=64 time=0.100 ms
56 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.086 ms
--- web2 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.086/0.093/0.100/0.000 ms
user@docker1:~$
```

这里的问题是，为了促进这一点，Docker 不得不更改`web1`容器的 DNS 配置。这样做时，它会将嵌入式域名系统服务器注入到容器域名系统请求的中间。以前，当我们直接与主机 DNS 服务器对话时，我们现在与嵌入式 DNS 服务器对话:

```
user@docker1:~$ docker exec -t  web1 more /etc/resolv.conf
search lab.lab
nameserver 127.0.0.11
options ndots:0
user@docker1:~$
```

这是容器的域名解析工作所需的，但它有一个有趣的副作用。嵌入式域名系统服务器读取主机的`/etc/resolv.conf`文件，并使用该文件中定义的任何名称服务器作为嵌入式域名系统服务器的转发器。这样做的净效果是，您不会注意到嵌入式域名系统服务器，因为它仍在转发无法应答主机域名系统服务器的请求。然而，它只对这些转发器进行编程，如果它们被定义的话。如果它们不存在或设置为`127.0.0.1`，那么 Docker 会将转发器编程为谷歌的公共域名系统服务器(`8.8.8.8`和`8.4.4.4`)。

虽然这很有道理，但很少有你的本地 DNS 服务器恰好是`127.0.0.1`的情况。例如，您碰巧在同一台主机上运行某种类型的本地 DNS 解析器，或者使用 DNS 转发器应用程序，如**DNS sq**。在这些情况下，Docker 会将容器的 DNS 请求转发到前面提到的外部 DNS 服务器，而不是本地定义的服务器，这可能会导致一些复杂情况。也就是说，内部域名系统区域将不再是可解析的:

```
user@docker1:~$ docker exec -it web1 ping docker2.lab.lab
ping: unknown host
user@docker1:~$
```

### 注

这也可能导致一般的解决问题，因为通常会阻止到外部 DNS 服务器的 DNS 流量，而宁愿强制内部端点使用内部 DNS 服务器。

在这些场景中，有几种方法可以解决这个问题。您可以通过在容器运行时传递 DNS 标志，使用特定的 DNS 服务器运行容器:

```
user@docker1:~$ docker run -dP --name web2 --net mybridge1 \
--dns <your local DNS server> jonlangemak/web_server_2
```

否则，您可以将 DNS 服务器设置为 Docker 服务级别，然后嵌入式 DNS 服务器会将其用作转发器:

```
ExecStart=/usr/bin/dockerd --dns=<your local DNS server>
```

无论是哪种情况，如果您有容器解析问题，请总是检查并查看容器在其`/etc/resolv.conf`文件中配置了什么。如果是`127.0.0.11`，说明你使用的是 Docker 嵌入式 DNS 服务器。如果是，并且仍然有问题，请确保验证主机 DNS 配置，以确定嵌入式 DNS 服务器正在为转发器消耗什么。如果没有定义一个或者它是`127.0.0.1`，那么确保你告诉 Docker 服务它应该以前面定义的两种方式之一传递什么 DNS 服务器到容器。

# 构建测试容器

建造码头工人集装箱的租户之一是保持集装箱的小而瘦。在某些情况下，这可能会限制您的故障排除选项，因为容器的映像中不会包含许多常见的 Linux 网络工具。虽然不理想，但有时安装这些工具的容器映像会很好，这样您就可以从容器的角度对网络进行故障排除。在本章中，我们将回顾如何专门为此目的构建 Docker 映像。

## 做好准备

在本食谱中，我们将使用单个 Docker 网络主机。假设 Docker 已安装并处于默认配置。您还需要根级访问，以便检查和更改主机网络和防火墙配置。

## 怎么做…

Docker 映像是通过定义 Docker 文件构建的。Dockerfile 定义了要使用的基本图像以及要在容器内部运行的命令。在我的示例中，我将如下定义 Dockerfile:

```
FROM ubuntu:16.04
MAINTAINER Jon Langemak jon@interubernet.com
RUN apt-get update && apt-get install -y apache2 net-tools \
inetutils-ping curl dnsutils vim ethtool tcpdump
ADD index.html /var/www/html/index.html
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_LOG_DIR /var/log/apache2
ENV APACHE_PID_FILE /var/run/apache2/apache2.pid
ENV APACHE_LOCK_DIR /var/run/apache2
RUN mkdir -p /var/run/apache2
RUN chown www-data:www-data /var/run/apache2
EXPOSE 80
CMD ["/usr/sbin/apache2", "-D", "FOREGROUND"]
```

这个形象的目标是双重的。首先，我希望能够以分离模式运行容器，并让它提供服务。这将允许我定义容器，并验证诸如端口发布之类的事情是否在主机上运行。这个容器图像为我提供了一个已知良好的容器，它将在端口`80`上发布服务。为此，我们使用 Apache 来托管一个简单的索引页面。

索引文件在构建时被拉入图像中，并且可以由您自定义。我使用一个简单的 HTML 页面，`index.html`，它显示大红色字体，如下所示:

```
<body>
  <html>
    <h1><span style="color:#FF0000;font-size:72px;">Test Web Server - Running on port 80</span>
    </h1>
</body>
  </html>
```

第二，映像中安装了许多网络工具。您会注意到我正在安装以下软件包:

*   `net-tools`:这提供了查看和配置接口的网络工具
*   `inetutils-ping`:这提供了 ping 功能
*   `curl`:这是从其他网络端点拉文件
*   `dnsutils`:这是解析 DNS 名称和其他 DNS 追踪
*   `ethtool`:这是从界面获取信息和统计
*   `tcpdump`:这是从容器内部进行数据包捕获

如果您定义了这个 Dockerfile，以及它所需要的支持文件(一个索引页)，您可以如下构建图像:

```
sudo docker build -t <tag name for image> <path files ('.' If local)>
```

### 注

构建图像时，您可以定义许多选项。更多信息请看`docker build --help`。

Docker 随后将处理 Dockerfile，如果成功，它将生成一个`docker image`文件，然后您可以将该文件推送到您选择的容器注册表，以便在具有`docker pull`的其他主机上使用。

一旦构建完成，您就可以运行它并验证工具是否如预期的那样工作。容器中有`ethtool`意味着我们可以很容易地确定 VETH 对的主机端 VETH 端:

```
user@docker1:~$ docker run -dP --name nettest jonlangemak/net_tools
user@docker1:~$ docker exec -it nettest /bin/bash
root@2ef59fcc0f60:/# ethtool -S eth0
NIC statistics:
 peer_ifindex: 5
root@2ef59fcc0f60:/#
```

我们还可以执行本地`tcpdump` 操作来验证到达集装箱的流量:

```
root@2ef59fcc0f60:/# tcpdump -qnn -i eth0
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes
15:17:43.442243 IP 10.20.30.41.54974 > 172.17.0.3.80: tcp 0
15:17:43.442286 IP 172.17.0.3.80 > 10.20.30.41.54974: tcp 0

```

当你的用例改变时，你可以修改 Dockerfile，使其更具体到你自己的用例。在诊断连接问题时，能够从容器内部进行故障排除可能会有很大帮助。

### 注

这张图片只是一个例子。有很多方法可以让它变得更轻。我决定用 Ubuntu 作为基础图片，只是为了熟悉。因此，前面描述的图像相当重。

# 重置本地 Docker 网络数据库

随着用户定义网络的出现，用户能够为他们的容器定义自定义网络类型。一旦定义，这些网络将在系统重新启动后持续存在，直到被管理员删除。为了使这种持久性发挥作用，Docker 需要一些地方来存储与用户定义的网络相关的信息。答案是主机本地的数据库文件。在极少数情况下，该数据库可能会与主机上容器的当前状态不同步或损坏。这可能会导致与删除容器、删除网络和启动 Docker 服务相关的问题。在本食谱中，我们将向您展示如何删除数据库，以将 Docker 恢复到其默认网络配置。

## 做好准备

在本食谱中，我们将使用单个 Docker 网络主机。假设 Docker 已安装并处于默认配置。您还需要根级访问权限，以便检查和更改主机的网络和防火墙配置。

## 怎么做…

Docker 将与用户定义的网络相关的信息存储在本地主机上存储的数据库中。该数据库在定义网络时写入，在服务启动时读取。在此数据库不同步或损坏的极少数情况下，您可以删除数据库并重新启动 Docker 服务，以便重置 Docker 用户定义的网络并恢复三种默认网络类型(网桥、主机和无)。

### 注

警告:删除此数据库会删除主机上的所有 Docker 用户定义的网络。只有在万不得已的情况下，并且如果您有能力重新创建以前定义的网络，这样做才是明智的。在尝试此操作之前，应使用所有其他故障排除选项，并且应在删除文件之前创建文件备份。

数据库名为`local-kv.db`，存储在路径`/var/lib/network/files/`中。访问和/或删除文件需要根级访问权限。在本例中，我们将切换到 root 用户，以便更轻松地浏览这个受保护的目录:

```
user@docker1:~$ sudo su
[sudo] password for user:
root@docker1:/home/user# cd /var/lib/docker/network/files
root@docker1:/var/lib/docker/network/files# ls -al
total 72
drwxr-x--- 2 root root 32768 Aug  9 21:27 .
drwxr-x--- 3 root root  4096 Apr  3 21:04 ..
-rw-r--r-- 1 root root 65536 Aug  9 21:27 local-kv.db
root@docker1:/var/lib/docker/network/files#
```

为了演示当我们删除这个文件时会发生什么，让我们首先创建一个新的用户定义的网络，并为其附加一个容器:

```
root@docker1:~# docker network create -d bridge mybridge
c765f1d24345e4652b137383839aabdd3b01b1441d1d81ad4b4e17229ddca7ac
root@docker1:~# docker run -d --name web1 --net mybridge jonlangemak/web_server_1
24a6497e99de9e114b617b65673a8a50492655e9869dbf7f7930dd7f9f930b5e
root@docker1:~#
```

现在我们删除文件`local-db.kv`:

```
root@docker1:/var/lib/docker/network/files# rm local-kv.db
```

虽然这对正在运行的容器没有直接影响，但它确实阻止我们添加、删除或启动与此用户定义网络相关联的新容器:

```
root@docker1:/~# docker run -d --name web2 --net mybridge \
jonlangemak/web_server_2
2ef7e52f44c93412ea7eaa413f523020a65f1a9fa6fd6761ffa6edea157c2623
docker: Error response from daemon: failed to update store for object type *libnetwork.endpointCnt: Key not found in store.
root@docker1:~#
```

删除`boltdb`数据库文件`local-kv.db`后，您需要重新启动 Docker 服务，以便 Docker 使用默认设置重新创建它:

```
root@docker1:/var/lib/docker/network/files# cd
root@docker1:~# systemctl restart docker
root@docker1:~# ls /var/lib/docker/network/files
local-kv.db
root@docker1:~# docker network ls
NETWORK ID          NAME                DRIVER
bfd1ba1175a9        none                null
0740840aef37        host                host
97cbc0e116d7        bridge              bridge
root@docker1:/var/lib/docker/network/files#
```

现在文件被重新创建，您将再次能够创建用户定义的网络。但是，连接到以前配置的用户定义网络的任何容器现在都将无法启动:

```
root@docker1:~# docker start web1
Error response from daemon: network mybridge not found
Error: failed to start containers: web1
root@docker1:~#
```

这是预期的行为，因为 Docker 仍然认为容器应该在网络上有一个接口:

```
root@docker1:~# docker inspect web1
…<Additional output removed for brevity>…
            "Networks": {
                "mybridge": {
                    "IPAMConfig": null,
                    "Links": null,
                    "Aliases": null,
                    "NetworkID": "c765f1d24345e4652b137383839aabdd3b01b1441d1d81ad4b4e17229ddca7ac",
…<Additional output removed for brevity>…
root@docker1:~#
```

要解决这个问题，你有两个选择。首先，您可以使用与最初配置时相同的配置选项重新创建名为`mybridge`的用户定义网络。如果这不起作用，您唯一的选择是删除容器并重新启动引用新创建或默认网络的新实例。

### 注

在使用`docker network disconnect`子命令时，关于支持`--force`标志的新版 Docker 的 GitHub 有过一些讨论。在 1.10 版本中，这个参数是存在的，但是仍然不喜欢用户定义的网络不存在。如果你正在运行一个更新的版本，这可能也值得一试。