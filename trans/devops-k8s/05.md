# 网络与安全

我们已经在[第 3 章](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)、*中学习了如何在 Kubernetes 中部署具有不同资源的容器，并知道如何使用卷来保存数据、动态资源调配和不同的存储类。接下来，我们将学习 Kubernetes 如何路由流量来实现这一切。网络在软件世界中一直扮演着重要的角色。我们将描述从单个主机上的容器到多个主机，最后到 Kubernetes 的网络。*

*   码头工人网络
*   不可思议的网络
*   进入
*   网络策略

# 不可思议的网络

在 Kubernetes 中有很多选择可以用来实现网络。Kubernetes 本身并不关心您如何实现它，但是您必须满足它的三个基本要求:

*   所有容器都应该可以在没有 NAT 的情况下相互访问，无论它们位于哪个节点上
*   所有节点都应该与所有容器通信
*   IP 容器应该像其他容器一样看待自己

在深入讨论之前，我们将首先回顾默认容器网络是如何工作的。这是使这一切成为可能的网络支柱。

# 码头工人网络

在进入 Kubernetes 网络之前，让我们回顾一下 Docker 网络是如何工作的。在[第二章](02.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e)、 *DevOps 带容器*中，我们学习了容器组网的三种模式，桥接、无、主机。

网桥是默认的网络模式。Docker 创建并附加虚拟以太网设备(也称为 veth)，并为每个容器分配网络名称空间。

The **network namespace** is a feature in Linux, which is logically another copy of a network stack. It has its own routing tables, arp tables, and network devices. It's a fundamental concept of container networking.

Veth 总是成对出现，一个在网络名称空间中，另一个在网桥中。当流量进入主机网络时，它将被路由到网桥。数据包将被发送到它的 veth，并进入容器内部的命名空间，如下图所示:

![](../images/00088.jpeg)

让我们仔细看看。在下面的例子中，我们将使用 minikube 节点作为 docker 主机。首先，我们必须使用`minikube ssh`来 ssh 到节点中，因为我们还没有使用 Kubernetes。进入 minikube 节点后，让我们启动一个容器与我们进行交互:

```
// launch a busybox container with `top` command, also, expose container port 8080 to host port 8000.
# docker run -d -p 8000:8080 --name=busybox busybox top
737e4d87ba86633f39b4e541f15cd077d688a1c8bfb83156d38566fc5c81f469 
```

让我们看看容器内出站流量的实现。`docker exec <container_name or container_id>`可以在运行容器中运行命令。让我们使用`ip link list`列出所有接口:

```
// show all the network interfaces in busybox container
// docker exec <container_name> <command>
# docker exec busybox ip link list
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: sit0@NONE: <NOARP> mtu 1480 qdisc noop qlen 1
 link/sit 0.0.0.0 brd 0.0.0.0
**53**: **eth0@if54**: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> 
    mtu 1500 qdisc noqueue
 link/ether 02:42:ac:11:00:07 brd ff:ff:ff:ff:ff:ff  
```

我们可以看到`busybox`容器内部有三个接口。一个是身份证`53`和名字`eth0@if54`。`if`后的数字是该对中的另一个接口标识。在这种情况下，配对 ID 是`54`。如果我们在主机上运行相同的命令，我们可以看到主机中的 veth 指向容器内的`eth0`:

```
// show all the network interfaces from the host
# ip link list
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue  
   state UNKNOWN mode DEFAULT group default qlen 1
 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc 
   pfifo_fast state UP mode DEFAULT group default qlen  
   1000
 link/ether 08:00:27:ca:fd:37 brd ff:ff:ff:ff:ff:ff
...
**54**: **vethfeec36a@if53**: <BROADCAST,MULTICAST,UP,LOWER_UP> 
    mtu 1500 qdisc noqueue master docker0 state UP mode  
    DEFAULT group default
 link/ether ce:25:25:9e:6c:07 brd ff:ff:ff:ff:ff:ff link-netnsid 5  
```

我们在主机上有一个名为`vethfeec36a@if53` **的 veth。**它与容器网络命名空间中的`eth0@if54`配对。veth 54 连接到`docker0`桥，最终通过 eth0 接入互联网。如果我们看一下 iptables 规则，我们可以在 Docker 为出站流量创建的主机上找到一个伪装规则(也称为 SNAT)，这将使容器可以访问互联网:

```
// list iptables nat rules. Showing only POSTROUTING rules which allows packets to be altered before they leave the host.
# sudo iptables -t nat -nL POSTROUTING
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
...
MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0
...  
```

另一方面，对于入站流量，Docker 在预路由上创建自定义过滤链，并在`DOCKER`过滤链中动态创建转发规则。如果我们公开一个集装箱端口`8080`并将其映射到一个主机端口`8000`，我们可以看到我们正在监听任何 IP 地址(`0.0.0.0/0`)上的端口`8000`，然后该端口将被路由到集装箱端口`8080`:

```
// list iptables nat rules
# sudo iptables -t nat -nL
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
...
DOCKER     all  --  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
...
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL
...
Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  0.0.0.0/0            0.0.0.0/0
...
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:8000 to:172.17.0.7:8080
...  
```

现在我们知道包裹是如何进出集装箱的了。让我们来看看容器是如何相互通信的。

# 集装箱到集装箱的通信

Kubernetes 中的 Pods 有自己的真实 IP 地址。pod 中的容器共享网络名称空间，因此它们将对方视为*本地主机*。默认情况下，这是由**网络容器**实现的，它充当一个桥，为 pod 中的每个容器调度流量。让我们在下面的例子中看看这是如何工作的。我们用[第三章](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)*中的第一个例子开始使用 Kubernetes* ，它包括两个容器，`nginx`和`centos`在一个容器内:

```
#cat 5-1-1_pod.yaml
apiVersion: v1
kind: Pod
metadata:
 name: example
spec:
 containers:
 - name: web
 image: nginx
 - name: centos
 image: centos
 command: ["/bin/sh", "-c", "while : ;do curl http://localhost:80/; sleep 10; done"]

// create the Pod
#kubectl create -f 5-1-1_pod.yaml
pod "example" created  
```

然后，我们将描述容器并查看其容器标识:

```
# kubectl describe pods example
Name:       example
Node:       minikube/192.168.99.100
...
Containers:
 web:
 Container ID: docker:// **d9bd923572ab186870284535044e7f3132d5cac11ecb18576078b9c7bae86c73**
 Image:        nginx
...
centos:
 Container ID: docker: **//f4c019d289d4b958cd17ecbe9fe22a5ce5952cb380c8ca4f9299e10bf5e94a0f**
 Image:        centos
...  
```

在本例中，`web`带有容器标识`d9bd923572ab`，`centos`带有容器标识`f4c019d289d4`。如果我们使用`docker ps`进入节点`minikube/192.168.99.100`，我们可以检查 Kubernetes 实际启动了多少个容器，因为我们在 minikube 中，minikube 启动了许多其他集群容器。通过`CREATED`栏查看最新的投放时间，在这里我们会发现有三个刚刚投放的容器:

```
# docker ps
CONTAINER ID        IMAGE                                      COMMAND                  CREATED             STATUS              PORTS                                      NAMES
f4c019d289d4        36540f359ca3                               "/bin/sh -c 'while : "   2 minutes ago        Up 2 minutes k8s_centos_example_default_9843fc27-677b-11e7-9a8c-080027cafd37_1
d9bd923572ab        e4e6d42c70b3                               "nginx -g 'daemon off"   2 minutes ago        Up 2 minutes k8s_web_example_default_9843fc27-677b-11e7-9a8c-080027cafd37_1
4ddd3221cc47        gcr.io/google_containers/pause-amd64:3.0   "/pause"                 2 minutes ago        Up 2 minutes  
```

还有一个额外的容器`4ddd3221cc47`被启动。在深究是哪个集装箱之前，我们先来检查一下我们的`web`集装箱的网络模式。我们会发现示例 pod 中的容器运行在具有映射容器模式的容器中:

```
# docker inspect d9bd923572ab | grep NetworkMode
"NetworkMode": "container:4ddd3221cc4792207ce0a2b3bac5d758a5c7ae321634436fa3e6dd627a31ca76",  
```

`4ddd3221cc47`容器在这种情况下就是所谓的网络容器，它持有网络命名空间让`web`和`centos`容器加入。同一网络命名空间中的容器共享相同的 IP 地址和网络配置。这是 Kubernetes 中实现容器到容器通信的默认实现，映射到第一个需求。

# 吊舱间通信

无论在哪个节点上，都可以从其他豆荚访问豆荚的 IP 地址。这符合第二个要求。在下一节中，我们将描述 pods 在同一个节点内和跨节点的通信。

# 同一节点内的 Pod 通信

默认情况下，同一节点内的 Pod 到 pod 通信通过网桥进行。假设我们有两个豆荚，它们有自己的网络名称空间。当 pod1 想和 pod2 通话时，数据包通过 pod1 的命名空间到达相应的 veth 对 **vethXXXX** ，最终到达网桥。网桥然后广播目的地 IP，以帮助数据包找到路径，**vethyyy**响应。数据包随后到达 pod2:

![](../images/00089.jpeg)

然而，Kubernetes 是关于集群的。当 pod 位于不同的节点时，流量是如何路由的？

# 跨节点的 Pod 通信

根据第二个要求，所有节点必须与所有容器通信。Kubernetes 将实现委托给**容器网络接口** ( **CNI** )。用户可以选择不同的实现，通过 L2、L3 或叠加。覆盖网络是常见的解决方案之一，被称为**数据包封装**。它在离开源之前包装消息，得到传递，并在目的地打开消息。这导致覆盖增加了网络延迟和复杂性。只要所有容器可以跨节点相互访问，您就可以自由使用任何技术，例如 L2 邻接或 L3 网关。有关 CNI 的更多信息，请参考其规范(https://github . com/container networking/CNI/blob/master/spec . MD):

![](../images/00090.jpeg)

假设我们有一个从 pod1 到 pod4 的数据包。数据包从容器接口离开，到达 veth 对，然后通过网桥和节点的网络接口。网络实现在第 4 步开始发挥作用。只要数据包可以被路由到目标节点，您就可以自由使用任何选项。在下面的例子中，我们将使用`--network-plugin=cni`选项启动 minikube。启用 CNI 后，参数将通过节点中的 kubelet 传递。Kubelet 有一个默认的网络插件，但是你可以在它启动时探测任何支持的插件。在启动 minikube 之前，如果已经启动，您可以先使用`minikube stop`或`minikube delete`彻底删除整个集群，然后再做任何进一步的操作。虽然 minikube 是一个单节点环境，它可能不完全代表我们将遇到的生产场景，但这只是让您对所有这些工作有了一个基本的了解。我们将在[第 9 章](09.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)、 *Kubernetes 在 AWS* 和[第 10 章](10.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e)、 *Kubernetes 在 GCP* 中学习网络选项在现实世界中的部署。

```
// start minikube with cni option
# minikube start --network-plugin=cni
...
Kubectl is now configured to use the cluster.  
```

当我们指定`network-plugin`选项时，它会在启动时使用`--network-plugin-dir`中为插件指定的目录。在 CNI 插件中，默认的插件目录是`/opt/cni/net.d`。集群启动后，我们登录节点，通过`minikube ssh`查看里面的设置:

```
# minikube ssh
$ ifconfig 
...
mybridge  Link encap:Ethernet  HWaddr 0A:58:0A:01:00:01
 inet addr:10.1.0.1  Bcast:0.0.0.0  
          Mask:255.255.0.0
...  
```

我们会发现节点中有一个新的网桥，如果我们通过`5-1-1_pod.yml`再次创建示例 pod，我们会发现 pod 的 IP 地址变成了`10.1.0.x`，附属于`mybridge`而不是`docker0`。

```
# kubectl create -f 5-1-1_pod.yaml
pod "example" created
# kubectl describe po example
Name:       example
Namespace:  default
Node:       minikube/192.168.99.100
Start Time: Sun, 23 Jul 2017 14:24:24 -0400
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:         10.1.0.4  
```

为什么会这样？那是因为我们指定使用 CNI 作为网络插件，不使用`docker0`(也称**集装箱网络模型**或 **libnetwork** )。CNI 创建了一个虚拟接口，将其连接到底层网络，并设置 IP 地址和路由，最终将其映射到 pods 的命名空间。让我们来看看位于`/etc/cni/net.d/`的配置:

```
# cat /etc/cni/net.d/k8s.conf
{
 "name": "rkt.kubernetes.io",
 "type": "bridge",
 "bridge": "mybridge",
 "mtu": 1460,
 "addIf": "true",
 "isGateway": true,
 "ipMasq": true,
 "ipam": {
 "type": "host-local",
 "subnet": "10.1.0.0/16",
 "gateway": "10.1.0.1",
 "routes": [
      {
       "dst": "0.0.0.0/0"
      }
 ]
 }
}
```

在这个例子中，我们使用 CNI 桥插件来重用 pod 容器的 L2 桥。如果数据包来自`10.1.0.0/16`，目的地是任何地方，它会经过这个网关。就像我们之前看到的图一样，我们可以让另一个启用了`10.1.2.0/16`子网的 CNI 节点，这样 ARP 数据包就可以到达目标 pod 所在节点的物理接口。然后，它实现了跨节点的 pod 到 pod 通信。

让我们检查一下 iptables 中的规则:

```
// check the rules in iptables 
# sudo iptables -t nat -nL
... 
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
KUBE-POSTROUTING  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */
MASQUERADE  all  --  172.17.0.0/16        0.0.0.0/0
CNI-25df152800e33f7b16fc085a  all  --  10.1.0.0/16          0.0.0.0/0            /* name: "rkt.kubernetes.io" id: "328287949eb4d4483a3a8035d65cc326417ae7384270844e59c2f4e963d87e18" */
CNI-f1931fed74271104c4d10006  all  --  10.1.0.0/16          0.0.0.0/0            /* name: "rkt.kubernetes.io" id: "08c562ff4d67496fdae1c08facb2766ca30533552b8bd0682630f203b18f8c0a" */  
```

所有相关规则已切换至`10.1.0.0/16` CIDR。

# 吊舱对服务通信

Kubernetes 是动态的。豆荚总是被创建和删除。Kubernetes 服务是通过标签选择器定义一组荚的抽象。我们通常使用服务来访问 pod，而不是显式指定 pod。当我们创建一个服务时，将会创建一个`endpoint`对象，它描述了该服务中的标签选择器所选择的一组 pod IPs。

In some cases, `endpoint` object will not be created with service creation. For example, services without selectors will not create a corresponding `endpoint` object. For more information, refer to the service without selectors section in [Chapter 3](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started with Kubernetes*.

那么，流量是如何从 pod 到达服务背后的 pod 的呢？默认情况下，Kubernetes 通过`kube-proxy`使用 iptables 来执行魔法。下图对此进行了解释。

![](../images/00091.jpeg)

让我们重复一下[第三章](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)、*Kubernetes*的`3-2-3_rc1.yaml`和`3-2-3_nodeport.yaml`例子，观察默认行为:

```
// create two pods with nginx and one service to observe default networking. Users are free to use any other kind of solution.
# kubectl create -f 3-2-3_rc1.yaml
replicationcontroller "nginx-1.12" created
# kubectl create -f 3-2-3_nodeport.yaml
service "nginx-nodeport" created  
```

让我们遵守 iptable 规则，看看这是如何工作的。如下图，我们的服务 IP 是`10.0.0.167`，下面的两个豆荚 IP 地址是`10.1.0.4`和`10.1.0.5`。

```
// kubectl describe svc nginx-nodeport
Name:             nginx-nodeport
Namespace:        default
Selector:         project=chapter3,service=web
Type:             NodePort
IP:               10.0.0.167
Port:             <unset>     80/TCP
NodePort:         <unset>     32261/TCP
Endpoints:        10.1.0.4:80,10.1.0.5:80
...  
```

让我们通过`minikube ssh`进入 minikube 节点，检查它的可调度规则:

```
# sudo iptables -t nat -nL
...
Chain KUBE-SERVICES (2 references)
target     prot opt source               destination
KUBE-SVC-37ROJ3MK6RKFMQ2B  tcp  --  0.0.0.0/0            **10.0.0.167**           /* default/nginx-nodeport: cluster IP */ tcp dpt:80
KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL

Chain **KUBE-SVC-37ROJ3MK6RKFMQ2B** (2 references)
target     prot opt source               destination
**KUBE-SEP-SVVBOHTYP7PAP3J5**  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ statistic mode random probability 0.50000000000
**KUBE-SEP-AYS7I6ZPYFC6YNNF**  all  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */
Chain **KUBE-SEP-SVVBOHTYP7PAP3J5** (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.1.0.4             0.0.0.0/0            /* default/nginx-nodeport: */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ tcp to:10.1.0.4:80
Chain KUBE-SEP-AYS7I6ZPYFC6YNNF (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.1.0.5             0.0.0.0/0            /* default/nginx-nodeport: */
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* default/nginx-nodeport: */ tcp to:10.1.0.5:80
...  
```

这里的关键点是，该服务将集群 IP 暴露给来自目标`KUBE-SVC-37ROJ3MK6RKFMQ2B`的外部流量，该目标链接到两个自定义链`KUBE-SEP-SVVBOHTYP7PAP3J5`和`KUBE-SEP-AYS7I6ZPYFC6YNNF`，统计模式随机概率为 0.5。这意味着，iptables 将生成一个随机数，并根据概率分布 0.5 将其调整到目的地。这两个自定义链将`DNAT`目标设置为对应的 pod IP。`DNAT`目标负责更改数据包的目的 IP 地址。默认情况下，当流量进入时，conntrack 被启用以跟踪连接的目的地和源。所有这些都会导致路由行为。当流量到达服务时，iptables 会随机选择其中一个 pod 进行路由，并将目的 IP 从服务 IP 修改为真实的 pod IP，并取消 DNAT 以一直返回。

# 外部服务通信

为库本内斯的外部流量提供服务的能力至关重要。Kubernetes 提供了两个 API 对象来实现这一点:

*   **服务**:外部网络负载平衡器或节点端口(L4)
*   **入口:** HTTP(S)负载平衡器(L7)

对于入口，我们将在下一节中了解更多。我们将首先关注 L4。根据我们对跨节点的 pod 到 pod 通信的了解，数据包是如何在服务和 pod 之间进出的。下图显示了它的工作原理。假设我们有两个服务，一个服务 A 有三个 pod(pod A、pod b 和 pod c)，另一个服务 B 只有一个 pod (pod d)。当流量从负载平衡器进入时，数据包将被分派到其中一个节点。大多数云负载平衡器本身并不知道吊舱或容器。它只知道节点。如果节点通过了运行状况检查，那么它将成为目标的候选节点。假设我们要访问服务 B，它目前只有一个 pod 在一个节点上运行。然而，负载平衡器将数据包发送到另一个没有运行任何我们想要的 pod 的节点。交通路线如下所示:

![](../images/00092.jpeg)

数据包路由过程将是:

1.  负载平衡器将选择其中一个节点来转发数据包。在 GCE 中，它根据源 IP 和端口、目标 IP 和端口以及协议的散列来选择实例。在 AWS 中，它基于循环算法。
2.  这里，路由目的地将被更改为 pod d (DNAT)，并将其转发到另一个节点，类似于跨节点的 pod 到 pod 通信。
3.  然后，是服务到吊舱的通信。数据包到达 d 舱时会有相应的响应。
4.  Pod 到 service 的通信也是由 iptables 操纵的。
5.  数据包将被转发到原始节点。
6.  源和目的地将被卸载到负载平衡器和客户端，并一直发送回来。

In Kubernetes 1.7, there is a new attribute in service called **externalTrafficPolicy**. You can set its value to local, then after the traffic goes into a node, Kubernetes will route the pods on that node, if any.

# 进入

Kubernetes 中的 Pods 和服务都有自己的 IP；然而，它通常不是你提供给外部互联网的接口。虽然有配置了节点 IP 的服务，但是节点 IP 中的端口不能在服务之间重复。决定用哪个服务管理哪个端口是很麻烦的。此外，节点来了又走，向外部服务提供静态节点 IP 并不聪明。

入口定义了一组规则，允许入站连接访问 Kubernetes 集群服务。它将流量引入 L7 集群，在每个虚拟机上分配一个端口并将其转发到服务端口。如下图所示。我们定义了一组规则，并将它们作为源类型入口发布到应用编程接口服务器。当流量进入时，入口控制器将按照入口规则完成并路由入口。如下图所示，入口用于通过不同的 URL 将外部流量路由到 kubernetes 端点:

![](../images/00093.jpeg)

现在，我们将通过一个例子，看看这是如何工作的。在本例中，我们将创建两个名为`nginx`和`echoserver`的服务，并配置入口路径`/welcome`和`/echoserver`。我们可以在 minikube 运行这个。minikube 的旧版本默认情况下不支持入口；我们必须首先启用它:

```
// start over our minikube local
# minikube delete && minikube start

// enable ingress in minikube
# minikube addons enable ingress
ingress was successfully enabled 

// check current setting for addons in minikube
# minikube addons list
- registry: disabled
- registry-creds: disabled
- addon-manager: enabled
- dashboard: enabled
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: **enabled** 
```

在 minikube 中启用入口将创建一个 nginx 入口控制器和一个`ConfigMap`来存储 nginx 配置(参见[https://github . com/kubernetes/ingress/blob/master/controllers/nginx/readme . MD](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md)))，以及一个 RC 和服务作为处理未映射请求的默认 HTTP 后端。我们可以通过在`kubectl`命令中添加`--namespace=kube-system`来观察它们。接下来，让我们创建我们的后端资源。这是我们的 nginx `Deployment`和`Service`:

```
# cat 5-2-1_nginx.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
 name: nginx
spec:
 replicas: 2
 template:
 metadata:
 labels:
 project: chapter5
 service: nginx
 spec:
 containers:
 - name: nginx
 image: nginx
 ports:
 - containerPort: 80
---
kind: Service
apiVersion: v1
metadata:
 name: nginx
spec:
 type: NodePort
  selector:
 project: chapter5
 service: nginx
 ports:
 - protocol: TCP
 port: 80
 targetPort: 80
// create nginx RS and service
# kubectl create -f 5-2-1_nginx.yaml
deployment "nginx" created
service "nginx" created
```

然后，我们将使用 RS 创建另一项服务:

```
// another backend named echoserver
# cat 5-2-1_echoserver.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
 name: echoserver
spec:
 replicas: 1
 template:
 metadata:
 name: echoserver
 labels:
 project: chapter5
 service: echoserver
 spec:
 containers:
 - name: echoserver
 image: gcr.io/google_containers/echoserver:1.4
 ports:
 - containerPort: 8080
---

kind: Service
apiVersion: v1
metadata:
 name: echoserver
spec:
 type: NodePort
 selector:
 project: chapter5
 service: echoserver
 ports:
 - protocol: TCP
 port: 8080
 targetPort: 8080

// create RS and SVC by above configuration file
# kubectl create -f 5-2-1_echoserver.yaml
deployment "echoserver" created
service "echoserver" created  
```

接下来，我们将创建入口资源。有个注解叫`ingress.kubernetes.io/rewrite-target`。如果服务请求来自根 URL，这是必需的。如果没有重写注释，我们将得到 404 作为响应。请参考[https://github . com/kubernetes/ingress/blob/master/controllers/nginx/configuration . MD #注解](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations)了解 nginx 入口控制器中更多支持的注解:

```
# cat 5-2-1_ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
 name: ingress-example
 annotations:
 ingress.kubernetes.io/rewrite-target: /
spec:
 rules:
 - host: devops.k8s
 http:
 paths:
 - path: /welcome
 backend:
 serviceName: nginx
 servicePort: 80
 - path: /echoserver
 backend:
 serviceName: echoserver
 servicePort: 8080

// create ingress
# kubectl create -f 5-2-1_ingress.yaml
ingress "ingress-example" created
```

In some cloud providers, service LoadBalancer controller is supported. It could be integrated with ingress via the `status.loadBalancer.ingress` syntax in the configuration file. For more information, refer to [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).

由于我们的主机被设置为`devops.k8s`，只有当我们从该主机名访问它时，它才会返回。您可以在 DNS 服务器中配置 DNS 记录，或者在本地修改主机文件。为简单起见，我们将在宿主文件中添加一行`ip hostname`格式的内容:

```
// normally host file located in /etc/hosts in linux
# sudo sh -c "echo `minikube ip` devops.k8s >> /etc/hosts"  
```

那么我们应该能够通过以下网址直接访问我们的服务:

```
# curl http://devops.k8s/welcome
...
<title>Welcome to nginx!</title>
...
// check echoserver 
# curl http://devops.k8s/echoserver
CLIENT VALUES:
client_address=172.17.0.4
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://devops.k8s:8080/  
```

pod 入口控制器根据 URL 路径调度流量。路由路径类似于外部服务通信。数据包在节点和吊舱之间跳跃。Kubernetes 是可插拔的。许多第三方实现正在进行。我们在这里只触及表面，而 iptables 只是一个默认的和常见的实现。网络在每个版本中都有很大的发展。在撰写本文时，Kubernetes 刚刚发布了 1.7 版本。

# 网络策略

网络策略对 pods 来说就像一个软件防火墙。默认情况下，每个吊舱都可以没有任何边界地相互通信。网络策略是您可以应用于 pods 的隔离之一。它通过名称空间选择器和容器选择器来定义谁可以访问哪个端口中的哪个容器。命名空间中的网络策略是附加的，一旦 pod 启用了策略，它就会拒绝任何其他入口(也称为默认拒绝所有入口)。

目前有多家网络提供商支持网络政策，如 Calico([https://www . project Calico . org/Calico-network-policy-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/))、Romana([https://github.com/romana/romana](https://github.com/romana/romana)))、Weave Net([https://www.weave.works/docs/net/latest/kube-addon/#npc)](https://www.weave.works/docs/net/latest/kube-addon/#npc))、Contiv([http://contiv.github.io/documents/networking/policies.html)](http://contiv.github.io/documents/networking/policies.html))和 Trireme([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes))。用户可以自由选择任何选项。为了简单起见，我们将使用带 minikube 的 Calico。为此，我们必须使用`--network-plugin=cni`选项启动 minikube。在这一点上，网络政策在 Kubernetes 还是相当新的。我们正在运行带有 v.1.0.7 minikube ISO 的 Kubernetes v . 1 . 7 . 0 版本，通过自托管解决方案部署 Calico([http://docs . project Calico . org/v 1.5/入门/Kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/))。首先，我们需要下载一个`calico.yaml`([https://github . com/project calico/Calico/blob/master/v 2.4/入门/kubernetes/installation/hosted/Calico . YAML](https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml)))文件来创建 Calico 节点和策略控制器。`etcd_endpoints`需要配置。要找到 etcd 的 IP，我们需要访问 localkube 资源。

```
// find out etcd ip
# minikube ssh -- "sudo /usr/local/bin/localkube --host-ip"
2017-07-27 04:10:58.941493 I | proto: duplicate proto type registered: google.protobuf.Any
2017-07-27 04:10:58.941822 I | proto: duplicate proto type registered: google.protobuf.Duration
2017-07-27 04:10:58.942028 I | proto: duplicate proto type registered: google.protobuf.Timestamp
localkube host ip:  10.0.2.15  
```

etcd 的默认端口是`2379`。在这种情况下，我们将`calico.yaml`中的`etcd_endpoint`从`http://127.0.0.1:2379`修改为`http://10.0.2.15:2379`:

```
// launch calico
# kubectl apply -f calico.yaml
configmap "calico-config" created
secret "calico-etcd-secrets" created
daemonset "calico-node" created
deployment "calico-policy-controller" created
job "configure-calico" created

// list the pods in kube-system
# kubectl get pods --namespace=kube-system
NAME                                        READY     STATUS    RESTARTS   AGE
calico-node-ss243                           2/2       Running   0          1m
calico-policy-controller-2249040168-r2270   1/1       Running   0          1m  
```

我们再以`5-2-1_nginx.yaml`为例:

```
# kubectl create -f 5-2-1_nginx.yaml
replicaset "nginx" created
service "nginx" created
// list the services
# kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes   10.0.0.1     <none>        443/TCP        47m
nginx        10.0.0.42    <nodes>       80:31071/TCP   5m
```

我们会发现我们的 nginx 服务有 IP `10.0.0.42`。让我们启动一个简单的 bash 并使用`wget`来看看我们是否可以访问我们的 nginx:

```
# kubectl run busybox -i -t --image=busybox /bin/sh
If you don't see a command prompt, try pressing enter.
/ # wget --spider 10.0.0.42 
Connecting to 10.0.0.42 (10.0.0.42:80)  
```

`--spider`参数用于检查网址是否存在。在这种情况下，busybox 可以成功访问 nginx。接下来，让我们将`NetworkPolicy`应用于我们的 nginx 吊舱:

```
// declare a network policy
# cat 5-3-1_networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
 name: nginx-networkpolicy
spec:
 podSelector:
 matchLabels:
 service: nginx
 ingress:
 - from:
 - podSelector:
 matchLabels:
 project: chapter5  
```

我们可以在这里看到一些重要的语法。`podSelector`用于选择吊舱，应与目标吊舱的标签相匹配。另一个是`ingress[].from[].podSelector`，用于定义谁可以访问这些豆荚。在这种情况下，所有带有`project=chapter5`标签的豆荚都有资格访问带有`server=nginx`标签的豆荚。如果我们回到 busybox pod，我们将无法再联系 nginx，因为现在 nginx pod 已经打开了 NetworkPolicy。默认情况下，它是全部拒绝，因此 busybox 将无法与 nginx 对话:

```
// in busybox pod, or you could use `kubectl attach <pod_name> -c busybox -i -t` to re-attach to the pod 
# wget --spider --timeout=1 10.0.0.42
Connecting to 10.0.0.42 (10.0.0.42:80)
wget: download timed out  
```

我们可以使用`kubectl edit deployment busybox`将标签`project=chaper5`添加到 busybox 豆荚中。

Refer to the labels and selectors section in [Chapter 3](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started with Kubernetes* if you forget how to do so.

之后，我们可以再次联系 nginx 吊舱:

```
// inside busybox pod
/ # wget --spider 10.0.0.42 
Connecting to 10.0.0.42 (10.0.0.42:80)  
```

借助前面的例子，我们有了一个如何应用网络策略的想法。我们也可以应用一些默认策略来拒绝所有或允许所有，通过调整选择器来选择没有人或所有人。例如，拒绝所有行为可以通过以下方式实现:

```
# cat 5-3-1_np_denyall.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: default-deny
spec:
 podSelector:  
```

这样，所有不匹配标签的豆荚将拒绝所有其他流量。或者，我们可以创建一个`NetworkPolicy`，它的入口从任何地方都被列出。那么在这个命名空间中运行的 pods 可以被任何其他人访问。

```
# cat 5-3-1_np_allowall.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: allow-all
spec:
 podSelector:
 ingress:
 - {}  
```

# 摘要

在这一章中，我们学习了容器之间如何进行必要的通信，并介绍了容器之间的通信是如何工作的。服务是一个抽象，如果标签选择器匹配，它将流量路由到下面的任何一个单元。我们学习了 iptables magic 如何使用 pod 进行服务。我们知道了数据包是如何从外部路由到吊舱的，以及 DNAT，un-DAT 技巧。我们还学习了新的应用编程接口对象，如*入口*，它允许我们使用网址路径路由到后端的不同服务。最后引入了另一个对象`NetworkPolicy`。它提供了第二层安全性，充当软件防火墙规则。有了网络策略，我们可以让特定的豆荚只与特定的豆荚通信。例如，只有数据检索服务可以与数据库容器对话。所有这些都使得 Kubernetes 更加灵活、安全和强大。

到目前为止，我们已经了解了 Kubernetes 的基本概念。接下来，我们将通过监控集群指标和分析 Kubernetes 的应用程序和系统日志，更清楚地了解集群内部发生的事情。监控和日志工具对于每个 DevOps 都是必不可少的，它们在动态集群(如 Kubernetes)中也发挥着极其重要的作用。因此，我们将深入了解集群的活动，例如调度、部署、扩展和服务发现。下一章将帮助您更好地理解在现实世界中操作 Kubernetes 的行为。